{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ExternalDNS \u00b6 ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. What It Does \u00b6 Inspired by Kubernetes DNS , Kubernetes\u2019 cluster-internal DNS server, ExternalDNS makes Kubernetes resources discoverable via public DNS servers. Like KubeDNS, it retrieves a list of resources (Services, Ingresses, etc.) from the Kubernetes API to determine a desired list of DNS records. Unlike KubeDNS, however, it\u2019s not a DNS server itself, but merely configures other DNS providers accordingly\u2014e.g. AWS Route 53 or Google Cloud DNS . In a broader sense, ExternalDNS allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way. The FAQ contains additional information and addresses several questions about key concepts of ExternalDNS. To see ExternalDNS in action, have a look at this video or read this blogpost . The Latest Release \u00b6 ExternalDNS allows you to keep selected zones (via --domain-filter ) synchronized with Ingresses and Services of type=LoadBalancer and nodes in various DNS providers: * Google Cloud DNS * AWS Route 53 * AWS Cloud Map * AzureDNS * BlueCat * Civo * CloudFlare * RcodeZero * DigitalOcean * DNSimple * Infoblox * Dyn * OpenStack Designate * PowerDNS * CoreDNS * Exoscale * Oracle Cloud Infrastructure DNS * Linode DNS * RFC2136 * NS1 * TransIP * VinylDNS * Vultr * OVH * Scaleway * Akamai Edge DNS * GoDaddy * Gandi * ANS Group SafeDNS * IBM Cloud DNS * TencentCloud PrivateDNS * TencentCloud DNSPod * Plural * Pi-hole From this release, ExternalDNS can become aware of the records it is managing (enabled via --registry=txt ), therefore ExternalDNS can safely manage non-empty hosted zones. We strongly encourage you to use v0.5 (or greater) with --registry=txt enabled and --txt-owner-id set to a unique value that doesn\u2019t change for the lifetime of your cluster. You might also want to run ExternalDNS in a dry run mode ( --dry-run flag) to see the changes to be submitted to your DNS Provider API. Note that all flags can be replaced with environment variables; for instance, --dry-run could be replaced with EXTERNAL_DNS_DRY_RUN=1 , or --registry txt could be replaced with EXTERNAL_DNS_REGISTRY=txt . Status of providers \u00b6 ExternalDNS supports multiple DNS providers which have been implemented by the ExternalDNS contributors . Maintaining all of those in a central repository is a challenge and we have limited resources to test changes. This means that it is very hard to test all providers for possible regressions and, as written in the Contributing section, we encourage contributors to step in as maintainers for the individual providers and help by testing the integrations. End-to-end testing of ExternalDNS is currently performed in the separate kubernetes-on-aws repository. We define the following stability levels for providers: Stable : Used for smoke tests before a release, used in production and maintainers are active. Beta : Community supported, well tested, but maintainers have no access to resources to execute integration tests on the real platform and/or are not using it in production. Alpha : Community provided with no support from the maintainers apart from reviewing PRs. The following table clarifies the current status of the providers according to the aforementioned stability levels: Provider Status Maintainers Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta Akamai Edge DNS Beta AzureDNS Beta BlueCat Alpha @seanmalloy @vinny-sabatini Civo Alpha @alejandrojnm CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha @saileshgiri Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha OVH Alpha Scaleway DNS Alpha @Sh4d1 Vultr Alpha UltraDNS Alpha GoDaddy Alpha Gandi Alpha @packi SafeDNS Alpha @assureddt IBMCloud Alpha @hughhuangzh TencentCloud Alpha @Hyzhou Plural Alpha @michaeljguarino Pi-hole Alpha @tinyzimmer Kubernetes version compatibility \u00b6 A breaking change was added in external-dns v0.10.0. ExternalDNS <= 0.9.x >= 0.10.0 Kubernetes <= 1.18 Kubernetes >= 1.19 and <= 1.21 Kubernetes >= 1.22 Running ExternalDNS: \u00b6 The are two ways of running ExternalDNS: Deploying to a Cluster Running Locally Deploying to a Cluster \u00b6 The following tutorials are provided: Akamai Edge DNS Alibaba Cloud AWS AWS Load Balancer Controller Route53 Same domain for public and private Route53 zones Cloud Map Kube Ingress AWS Controller Azure DNS Azure Private DNS Civo Cloudflare BlueCat CoreDNS DigitalOcean DNSimple Dyn Exoscale ExternalName Services Google Kubernetes Engine Using Google\u2019s Default Ingress Controller Using the Nginx Ingress Controller Headless Services Infoblox Istio Gateway Source Kubernetes Security Context Linode Nginx Ingress Controller NS1 NS Record Creation with CRD Source OpenStack Designate Oracle Cloud Infrastructure (OCI) DNS PowerDNS RcodeZero RancherDNS (RDNS) RFC2136 TransIP VinylDNS OVH Scaleway Vultr UltraDNS GoDaddy Gandi SafeDNS IBM Cloud Nodes as source TencentCloud Plural Pi-hole Running Locally \u00b6 See the contributor guide for details on compiling from source. Setup Steps \u00b6 Next, run an application and expose it via a Kubernetes Service: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --target-port=80 --type=LoadBalancer Annotate the Service with your desired external DNS name. Make sure to change example.org to your domain. kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.example.org.\" Optionally, you can customize the TTL value of the resulting DNS record by using the external-dns.alpha.kubernetes.io/ttl annotation: kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/ttl=10\" For more details on configuring TTL, see here . Use the internal-hostname annotation to create DNS records with ClusterIP as the target. kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/internal-hostname=nginx.internal.example.org.\" If the service is not of type Loadbalancer you need the \u2013publish-internal-services flag. Locally run a single sync loop of ExternalDNS. external-dns --registry txt --txt-owner-id my-cluster-id --provider google --google-project example-project --source service --once --dry-run This should output the DNS records it will modify to match the managed zone with the DNS records you desire. It also assumes you are running in the default namespace. See the FAQ for more information regarding namespaces. Note: TXT records will have my-cluster-id value embedded. Those are used to ensure that ExternalDNS is aware of the records it manages. Once you\u2019re satisfied with the result, you can run ExternalDNS like you would run it in your cluster: as a control loop, and not in dry-run mode: external-dns --registry txt --txt-owner-id my-cluster-id --provider google --google-project example-project --source service Check that ExternalDNS has created the desired DNS record for your Service and that it points to its load balancer\u2019s IP. Then try to resolve it: dig +short nginx.example.org. 104.155.60.49 Now you can experiment and watch how ExternalDNS makes sure that your DNS records are configured as desired. Here are a couple of things you can try out: * Change the desired hostname by modifying the Service\u2019s annotation. * Recreate the Service and see that the DNS record will be updated to point to the new load balancer IP. * Add another Service to create more DNS records. * Remove Services to clean up your managed zone. The tutorials section contains examples, including Ingress resources, and shows you how to set up ExternalDNS in different environments such as other cloud providers and alternative Ingress controllers. Note \u00b6 If using a txt registry and attempting to use a CNAME the --txt-prefix must be set to avoid conflicts. Changing --txt-prefix will result in lost ownership over previously created records. If externalIPs list is defined for a LoadBalancer service, this list will be used instead of an assigned load balancer IP to create a DNS record. It\u2019s useful when you run bare metal Kubernetes clusters behind NAT or in a similar setup, where a load balancer IP differs from a public IP (e.g. with MetalLB ). Roadmap \u00b6 ExternalDNS was built with extensibility in mind. Adding and experimenting with new DNS providers and sources of desired DNS records should be as easy as possible. It should also be possible to modify how ExternalDNS behaves\u2014e.g. whether it should add records but never delete them. Here\u2019s a rough outline on what is to come (subject to change): v0.1 \u00b6 Support for Google CloudDNS Support for Kubernetes Services v0.2 \u00b6 Support for AWS Route 53 Support for Kubernetes Ingresses v0.3 \u00b6 Support for AWS Route 53 via ALIAS Support for multiple zones Ownership System v0.4 \u00b6 Support for AzureDNS Support for CloudFlare Support for DigitalOcean Multiple DNS names per Service v0.5 \u00b6 Support for creating DNS records to multiple targets (for Google and AWS) Support for OpenStack Designate Support for PowerDNS Support for Linode Support for RcodeZero Support for NS1 Support for TransIP Support for Azure Private DNS v0.6 \u00b6 Ability to replace kOps\u2019 DNS Controller (This could also directly become v1.0 ) Support for OVH v1.0 \u00b6 Ability to replace kOps\u2019 DNS Controller Add support for pod source Add support for DNS Controller annotations for pod and service sources Add support for kOps gossip provider Ability to replace Zalando\u2019s Mate Ability to replace Molecule Software\u2019s route53-kubernetes Yet to be defined \u00b6 Support for CoreDNS Support for record weights Support for different behavioral policies Support for Services with type=NodePort Support for CRDs Support for more advanced DNS record configurations Have a look at the milestones to get an idea of where we currently stand. Contributing \u00b6 Are you interested in contributing to external-dns? We, the maintainers and community, would love your suggestions, contributions, and help! Also, the maintainers can be contacted at any time to learn more about how to get involved. We also encourage ALL active community participants to act as if they are maintainers, even if you don\u2019t have \u201cofficial\u201d write permissions. This is a community effort, we are here to serve the Kubernetes community. If you have an active interest and you want to get involved, you have real power! Don\u2019t assume that the only people who can get things done around here are the \u201cmaintainers\u201d. We also would love to add more \u201cofficial\u201d maintainers, so show us what you can do! The external-dns project is currently in need of maintainers for specific DNS providers. Ideally each provider would have at least two maintainers. It would be nice if the maintainers run the provider in production, but it is not strictly required. Provider listed here that do not have a maintainer listed are in need of assistance. Read the contributing guidelines and have a look at the contributing docs to learn about building the project, the project structure, and the purpose of each package. For an overview on how to write new Sources and Providers check out Sources and Providers . Heritage \u00b6 ExternalDNS is an effort to unify the following similar projects in order to bring the Kubernetes community an easy and predictable way of managing DNS records across cloud providers based on their Kubernetes resources: Kops\u2019 DNS Controller Zalando\u2019s Mate Molecule Software\u2019s route53-kubernetes User Demo How-To Blogs and Examples \u00b6 A full demo on GKE Kubernetes. See How-to Kubernetes with DNS management (ssl-manager pre-req) Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns medium.com/@jpantjsoha/how-to-kubernetes-with-dns-management-for-gitops-31239ea75d8d) Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns","title":"Home"},{"location":"#externaldns","text":"ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers.","title":"ExternalDNS"},{"location":"#what-it-does","text":"Inspired by Kubernetes DNS , Kubernetes\u2019 cluster-internal DNS server, ExternalDNS makes Kubernetes resources discoverable via public DNS servers. Like KubeDNS, it retrieves a list of resources (Services, Ingresses, etc.) from the Kubernetes API to determine a desired list of DNS records. Unlike KubeDNS, however, it\u2019s not a DNS server itself, but merely configures other DNS providers accordingly\u2014e.g. AWS Route 53 or Google Cloud DNS . In a broader sense, ExternalDNS allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way. The FAQ contains additional information and addresses several questions about key concepts of ExternalDNS. To see ExternalDNS in action, have a look at this video or read this blogpost .","title":"What It Does"},{"location":"#the-latest-release","text":"ExternalDNS allows you to keep selected zones (via --domain-filter ) synchronized with Ingresses and Services of type=LoadBalancer and nodes in various DNS providers: * Google Cloud DNS * AWS Route 53 * AWS Cloud Map * AzureDNS * BlueCat * Civo * CloudFlare * RcodeZero * DigitalOcean * DNSimple * Infoblox * Dyn * OpenStack Designate * PowerDNS * CoreDNS * Exoscale * Oracle Cloud Infrastructure DNS * Linode DNS * RFC2136 * NS1 * TransIP * VinylDNS * Vultr * OVH * Scaleway * Akamai Edge DNS * GoDaddy * Gandi * ANS Group SafeDNS * IBM Cloud DNS * TencentCloud PrivateDNS * TencentCloud DNSPod * Plural * Pi-hole From this release, ExternalDNS can become aware of the records it is managing (enabled via --registry=txt ), therefore ExternalDNS can safely manage non-empty hosted zones. We strongly encourage you to use v0.5 (or greater) with --registry=txt enabled and --txt-owner-id set to a unique value that doesn\u2019t change for the lifetime of your cluster. You might also want to run ExternalDNS in a dry run mode ( --dry-run flag) to see the changes to be submitted to your DNS Provider API. Note that all flags can be replaced with environment variables; for instance, --dry-run could be replaced with EXTERNAL_DNS_DRY_RUN=1 , or --registry txt could be replaced with EXTERNAL_DNS_REGISTRY=txt .","title":"The Latest Release"},{"location":"#status-of-providers","text":"ExternalDNS supports multiple DNS providers which have been implemented by the ExternalDNS contributors . Maintaining all of those in a central repository is a challenge and we have limited resources to test changes. This means that it is very hard to test all providers for possible regressions and, as written in the Contributing section, we encourage contributors to step in as maintainers for the individual providers and help by testing the integrations. End-to-end testing of ExternalDNS is currently performed in the separate kubernetes-on-aws repository. We define the following stability levels for providers: Stable : Used for smoke tests before a release, used in production and maintainers are active. Beta : Community supported, well tested, but maintainers have no access to resources to execute integration tests on the real platform and/or are not using it in production. Alpha : Community provided with no support from the maintainers apart from reviewing PRs. The following table clarifies the current status of the providers according to the aforementioned stability levels: Provider Status Maintainers Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta Akamai Edge DNS Beta AzureDNS Beta BlueCat Alpha @seanmalloy @vinny-sabatini Civo Alpha @alejandrojnm CloudFlare Beta RcodeZero Alpha DigitalOcean Alpha DNSimple Alpha Infoblox Alpha @saileshgiri Dyn Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha VinylDNS Alpha RancherDNS Alpha OVH Alpha Scaleway DNS Alpha @Sh4d1 Vultr Alpha UltraDNS Alpha GoDaddy Alpha Gandi Alpha @packi SafeDNS Alpha @assureddt IBMCloud Alpha @hughhuangzh TencentCloud Alpha @Hyzhou Plural Alpha @michaeljguarino Pi-hole Alpha @tinyzimmer","title":"Status of providers"},{"location":"#kubernetes-version-compatibility","text":"A breaking change was added in external-dns v0.10.0. ExternalDNS <= 0.9.x >= 0.10.0 Kubernetes <= 1.18 Kubernetes >= 1.19 and <= 1.21 Kubernetes >= 1.22","title":"Kubernetes version compatibility"},{"location":"#running-externaldns","text":"The are two ways of running ExternalDNS: Deploying to a Cluster Running Locally","title":"Running ExternalDNS:"},{"location":"#deploying-to-a-cluster","text":"The following tutorials are provided: Akamai Edge DNS Alibaba Cloud AWS AWS Load Balancer Controller Route53 Same domain for public and private Route53 zones Cloud Map Kube Ingress AWS Controller Azure DNS Azure Private DNS Civo Cloudflare BlueCat CoreDNS DigitalOcean DNSimple Dyn Exoscale ExternalName Services Google Kubernetes Engine Using Google\u2019s Default Ingress Controller Using the Nginx Ingress Controller Headless Services Infoblox Istio Gateway Source Kubernetes Security Context Linode Nginx Ingress Controller NS1 NS Record Creation with CRD Source OpenStack Designate Oracle Cloud Infrastructure (OCI) DNS PowerDNS RcodeZero RancherDNS (RDNS) RFC2136 TransIP VinylDNS OVH Scaleway Vultr UltraDNS GoDaddy Gandi SafeDNS IBM Cloud Nodes as source TencentCloud Plural Pi-hole","title":"Deploying to a Cluster"},{"location":"#running-locally","text":"See the contributor guide for details on compiling from source.","title":"Running Locally"},{"location":"#setup-steps","text":"Next, run an application and expose it via a Kubernetes Service: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --target-port=80 --type=LoadBalancer Annotate the Service with your desired external DNS name. Make sure to change example.org to your domain. kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.example.org.\" Optionally, you can customize the TTL value of the resulting DNS record by using the external-dns.alpha.kubernetes.io/ttl annotation: kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/ttl=10\" For more details on configuring TTL, see here . Use the internal-hostname annotation to create DNS records with ClusterIP as the target. kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/internal-hostname=nginx.internal.example.org.\" If the service is not of type Loadbalancer you need the \u2013publish-internal-services flag. Locally run a single sync loop of ExternalDNS. external-dns --registry txt --txt-owner-id my-cluster-id --provider google --google-project example-project --source service --once --dry-run This should output the DNS records it will modify to match the managed zone with the DNS records you desire. It also assumes you are running in the default namespace. See the FAQ for more information regarding namespaces. Note: TXT records will have my-cluster-id value embedded. Those are used to ensure that ExternalDNS is aware of the records it manages. Once you\u2019re satisfied with the result, you can run ExternalDNS like you would run it in your cluster: as a control loop, and not in dry-run mode: external-dns --registry txt --txt-owner-id my-cluster-id --provider google --google-project example-project --source service Check that ExternalDNS has created the desired DNS record for your Service and that it points to its load balancer\u2019s IP. Then try to resolve it: dig +short nginx.example.org. 104.155.60.49 Now you can experiment and watch how ExternalDNS makes sure that your DNS records are configured as desired. Here are a couple of things you can try out: * Change the desired hostname by modifying the Service\u2019s annotation. * Recreate the Service and see that the DNS record will be updated to point to the new load balancer IP. * Add another Service to create more DNS records. * Remove Services to clean up your managed zone. The tutorials section contains examples, including Ingress resources, and shows you how to set up ExternalDNS in different environments such as other cloud providers and alternative Ingress controllers.","title":"Setup Steps"},{"location":"#note","text":"If using a txt registry and attempting to use a CNAME the --txt-prefix must be set to avoid conflicts. Changing --txt-prefix will result in lost ownership over previously created records. If externalIPs list is defined for a LoadBalancer service, this list will be used instead of an assigned load balancer IP to create a DNS record. It\u2019s useful when you run bare metal Kubernetes clusters behind NAT or in a similar setup, where a load balancer IP differs from a public IP (e.g. with MetalLB ).","title":"Note"},{"location":"#roadmap","text":"ExternalDNS was built with extensibility in mind. Adding and experimenting with new DNS providers and sources of desired DNS records should be as easy as possible. It should also be possible to modify how ExternalDNS behaves\u2014e.g. whether it should add records but never delete them. Here\u2019s a rough outline on what is to come (subject to change):","title":"Roadmap"},{"location":"#v01","text":"Support for Google CloudDNS Support for Kubernetes Services","title":"v0.1"},{"location":"#v02","text":"Support for AWS Route 53 Support for Kubernetes Ingresses","title":"v0.2"},{"location":"#v03","text":"Support for AWS Route 53 via ALIAS Support for multiple zones Ownership System","title":"v0.3"},{"location":"#v04","text":"Support for AzureDNS Support for CloudFlare Support for DigitalOcean Multiple DNS names per Service","title":"v0.4"},{"location":"#v05","text":"Support for creating DNS records to multiple targets (for Google and AWS) Support for OpenStack Designate Support for PowerDNS Support for Linode Support for RcodeZero Support for NS1 Support for TransIP Support for Azure Private DNS","title":"v0.5"},{"location":"#v06","text":"Ability to replace kOps\u2019 DNS Controller (This could also directly become v1.0 ) Support for OVH","title":"v0.6"},{"location":"#v10","text":"Ability to replace kOps\u2019 DNS Controller Add support for pod source Add support for DNS Controller annotations for pod and service sources Add support for kOps gossip provider Ability to replace Zalando\u2019s Mate Ability to replace Molecule Software\u2019s route53-kubernetes","title":"v1.0"},{"location":"#yet-to-be-defined","text":"Support for CoreDNS Support for record weights Support for different behavioral policies Support for Services with type=NodePort Support for CRDs Support for more advanced DNS record configurations Have a look at the milestones to get an idea of where we currently stand.","title":"Yet to be defined"},{"location":"#contributing","text":"Are you interested in contributing to external-dns? We, the maintainers and community, would love your suggestions, contributions, and help! Also, the maintainers can be contacted at any time to learn more about how to get involved. We also encourage ALL active community participants to act as if they are maintainers, even if you don\u2019t have \u201cofficial\u201d write permissions. This is a community effort, we are here to serve the Kubernetes community. If you have an active interest and you want to get involved, you have real power! Don\u2019t assume that the only people who can get things done around here are the \u201cmaintainers\u201d. We also would love to add more \u201cofficial\u201d maintainers, so show us what you can do! The external-dns project is currently in need of maintainers for specific DNS providers. Ideally each provider would have at least two maintainers. It would be nice if the maintainers run the provider in production, but it is not strictly required. Provider listed here that do not have a maintainer listed are in need of assistance. Read the contributing guidelines and have a look at the contributing docs to learn about building the project, the project structure, and the purpose of each package. For an overview on how to write new Sources and Providers check out Sources and Providers .","title":"Contributing"},{"location":"#heritage","text":"ExternalDNS is an effort to unify the following similar projects in order to bring the Kubernetes community an easy and predictable way of managing DNS records across cloud providers based on their Kubernetes resources: Kops\u2019 DNS Controller Zalando\u2019s Mate Molecule Software\u2019s route53-kubernetes","title":"Heritage"},{"location":"#user-demo-how-to-blogs-and-examples","text":"A full demo on GKE Kubernetes. See How-to Kubernetes with DNS management (ssl-manager pre-req) Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns medium.com/@jpantjsoha/how-to-kubernetes-with-dns-management-for-gitops-31239ea75d8d) Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns","title":"User Demo How-To Blogs and Examples"},{"location":"20190708-external-dns-incubator/","text":"Move ExternalDNS out of Kubernetes incubator \u00b6 Move ExternalDNS out of Kubernetes incubator Summary Motivation Goals Proposal Details Graduation Criteria Maintainers Release process, artifacts Risks and Mitigations Summary \u00b6 ExternalDNS is a project that synchronizes Kubernetes\u2019 Services, Ingresses and other Kubernetes resources to DNS backends for several DNS providers. The projects was started as a Kubernetes Incubator project in February 2017 and being the Kubernetes incubation initiative officially over, the maintainers want to propose the project to be moved to the kubernetes GitHub organization or to kubernetes-sigs, under the sponsorship of sig-network. Motivation \u00b6 ExternalDNS started as a community project with the goal of unifying several existing projects that were trying to solve the same problem: create DNS records for Kubernetes resources on several DNS backends. When the project was proposed (see the original discussion ), there were at least 3 existing implementations of the same functionality: Mate - https://github.com/linki/mate DNS-controller from kops - https://github.com/kubernetes/kops/tree/HEAD/dns-controller Route53-kubernetes - https://github.com/wearemolecule/route53-kubernetes ExternalDNS\u2019 goal from the beginning was to provide an officially supported solution to those problems. After two years of development, the project is still in the kubernetes-sigs. The incubation has been officially discontinued and to quote @thockin \u201cIncubator projects should either become real projects in Kubernetes, shut themselves down, or move elsewhere\u201d (see original thread here ). This KEP proposes to move ExternalDNS to the main Kubernetes organization or kubernetes-sigs. The \u201cProposal\u201d section details the reasons behind it. Goals \u00b6 The only goal of this KEP is to establish consensus regarding the future of the ExternalDNS project and determine where it belongs. Proposal \u00b6 This KEP is about moving External DNS out of the Kubernetes incubator. This section will cover the reasons why External DNS is useful and what the community would miss in case the project would be discontinued or moved under another organization. External DNS\u2026 Is the de facto solution to create DNS records for several Kubernetes resources. Is a vital component to achieve an experience close to a PaaS that many Kubernetes users try to replicate on top of Kubernetes, by allowing to automatically create DNS records for web applications. Supports already 18 different DNS providers including all major public clouds (AWS, Azure, GCP). Given that the kubernetes-sigs organization will eventually be shut down, the possible alternatives to moving to be an official Kubernetes project are the following: Shut down the project Move the project elsewhere We believe that those alternatives would result in a worse outcome for the community compared to moving the project to the any of the other official Kubernetes organizations. In fact, shutting down ExternalDNS can cause: The community to rebuild the same solution as already happened multiple times before the project was launched. Currently ExternalDNS is easy to be found, referenced in many articles/tutorials and for that reason not exposed to that risk. Existing users of the projects to be left without a future proof working solution. Moving the ExternalDNS project outside of Kubernetes projects would cause: Problems (re-)establishing user trust which could eventually lead to fragmentation and duplication. It would be hard to establish in which organization the project should be moved to. The most natural would be Zalando\u2019s organization, being the company that put most of the work on the project. While it is possible to assume Zalando\u2019s commitment to open-source, that would be a strategic mistake for the project community and for the Kubernetes ecosystem due to the obvious lack of neutrality. Lack of resources to test, lack of issue management via automation. For those reasons, we propose to move ExternalDNS out of the Kubernetes incubator, to live either under the kubernetes or kubernetes-sigs organization to keep being a vital part of the Kubernetes ecosystem. Details \u00b6 Graduation Criteria \u00b6 ExternalDNS is a two years old project widely used in production by many companies. The implementation for the three major cloud providers (AWS, Azure, GCP) is stable, not changing its logic and the project is being used in production by many company using Kubernetes. We have evidence that many companies are using ExternalDNS in production, but it is out of scope for this proposal to collect a comprehensive list of companies. The project was quoted by a number of tutorials on the web, including the official tutorials from AWS . ExternalDNS can\u2019t be consider to be \u201cdone\u201d: while the core functionality has been implemented, there is lack of integration testing and structural changes that are needed. Those are identified in the project roadmap, which is roughly made of the following items: Decoupling of the providers Implementation proposal Development Bug fixing and performance optimization (i.e. rate limiting on cloud providers) Integration testing suite, to be implemented at least for the \u201cstable\u201d providers For those reasons, we consider ExternalDNS to be in Beta state as a project. We believe that once the items mentioned above will be implemented, the project can reach a declared GA status. There are a number of other factors that need to be covered to fully describe the state of the project, including who are the maintainers, the way we release and manage the project and so on. Maintainers \u00b6 The project has the following maintainers: hjacobs Raffo linki njuettner The list of maintainers shrunk over time as people moved out of the original development team (all the team members were working at Zalando at the time of project creation) and the project required less work. The high number of providers contributed to the project pose a maintainability challenge: it is hard to bring the providers forward in terms of functionalities or even test them. The maintainers believe that the plan to transform the current Provider interface from a Go interface to an API will allow for enough decoupling and to hand over the maintenance of those plugins to the contributors themselves, see the risk and mitigations section for further details. Release process, artifacts \u00b6 The project uses the free quota of TravisCI to run tests for the project. The release pipeline for the project is currently fully owned by Zalando. It runs on the internal system of the company (closed source) which external maintainers/users can\u2019t access and that pushes images to the publicly accessible docker registry available at the URL registry.opensource.zalan.do . The docker registry service is provided as best effort with no sort of SLA and the maintainers team openly suggests the users to build and maintain their own docker image based on the provided Dockerfiles. Providing a vanity URL for the docker images was consider a non goal till now, but the community seems to be wanting official images from a GCR domain, similarly to what is available for other parts of official Kubernetes projects. ExternalDNS does not follow a specific release cycle. Releases are made often when there are major contributions (i.e. new providers) or important bug fixes. That said, the default branch is considered stable and can be used as well to build images. Risks and Mitigations \u00b6 The following are risks that were identified: Low number of maintainers: we are currently facing issues keeping up with the number of pull requests and issues giving the low number of maintainers. The list of maintainers already shrunk from 8 maintainers to 4. Issues maintaining community contributed providers: we often lack access to external providers (i.e. InfoBlox, etc.) and this means that we cannot verify the implementations and/or run regression tests that go beyond unit testing. Somewhat low quality of releases due to lack of integration testing. We think that the following actions will constitute appropriate mitigations: Decoupling the providers via an API will allow us to resolve the problem of the providers. Being the project already more than 2 years old and given that there are 18 providers implemented, we possess enough information to define an API that we can be stable in a short timeframe. Once this is stable, the problem of testing the providers can be deferred to be a provider\u2019s responsibility. This will also reduce the scope of External DNS core code, which means that there will be no need for a further increase of the maintaining team. We added integration testing for the main cloud providers to the roadmap for the 1.0 release to make sure that we cover the mostly used ones. We believe that this item should be tackled independently from the decoupling of providers as it would be capable of generating value independently from the result of the decoupling efforts. With the move to the Kubernetes incubation, we hope that we will be able to access the testing resources of the Kubernetes project. In this way, we hope to decouple the project from the dependency on Zalando\u2019s internal CI tool. This will help open up the possibility to increase the visibility on the project from external contributors, which currently would be blocked by the lack of access to the software used for the whole release pipeline.","title":"Out of Incubator"},{"location":"20190708-external-dns-incubator/#move-externaldns-out-of-kubernetes-incubator","text":"Move ExternalDNS out of Kubernetes incubator Summary Motivation Goals Proposal Details Graduation Criteria Maintainers Release process, artifacts Risks and Mitigations","title":"Move ExternalDNS out of Kubernetes incubator"},{"location":"20190708-external-dns-incubator/#summary","text":"ExternalDNS is a project that synchronizes Kubernetes\u2019 Services, Ingresses and other Kubernetes resources to DNS backends for several DNS providers. The projects was started as a Kubernetes Incubator project in February 2017 and being the Kubernetes incubation initiative officially over, the maintainers want to propose the project to be moved to the kubernetes GitHub organization or to kubernetes-sigs, under the sponsorship of sig-network.","title":"Summary"},{"location":"20190708-external-dns-incubator/#motivation","text":"ExternalDNS started as a community project with the goal of unifying several existing projects that were trying to solve the same problem: create DNS records for Kubernetes resources on several DNS backends. When the project was proposed (see the original discussion ), there were at least 3 existing implementations of the same functionality: Mate - https://github.com/linki/mate DNS-controller from kops - https://github.com/kubernetes/kops/tree/HEAD/dns-controller Route53-kubernetes - https://github.com/wearemolecule/route53-kubernetes ExternalDNS\u2019 goal from the beginning was to provide an officially supported solution to those problems. After two years of development, the project is still in the kubernetes-sigs. The incubation has been officially discontinued and to quote @thockin \u201cIncubator projects should either become real projects in Kubernetes, shut themselves down, or move elsewhere\u201d (see original thread here ). This KEP proposes to move ExternalDNS to the main Kubernetes organization or kubernetes-sigs. The \u201cProposal\u201d section details the reasons behind it.","title":"Motivation"},{"location":"20190708-external-dns-incubator/#goals","text":"The only goal of this KEP is to establish consensus regarding the future of the ExternalDNS project and determine where it belongs.","title":"Goals"},{"location":"20190708-external-dns-incubator/#proposal","text":"This KEP is about moving External DNS out of the Kubernetes incubator. This section will cover the reasons why External DNS is useful and what the community would miss in case the project would be discontinued or moved under another organization. External DNS\u2026 Is the de facto solution to create DNS records for several Kubernetes resources. Is a vital component to achieve an experience close to a PaaS that many Kubernetes users try to replicate on top of Kubernetes, by allowing to automatically create DNS records for web applications. Supports already 18 different DNS providers including all major public clouds (AWS, Azure, GCP). Given that the kubernetes-sigs organization will eventually be shut down, the possible alternatives to moving to be an official Kubernetes project are the following: Shut down the project Move the project elsewhere We believe that those alternatives would result in a worse outcome for the community compared to moving the project to the any of the other official Kubernetes organizations. In fact, shutting down ExternalDNS can cause: The community to rebuild the same solution as already happened multiple times before the project was launched. Currently ExternalDNS is easy to be found, referenced in many articles/tutorials and for that reason not exposed to that risk. Existing users of the projects to be left without a future proof working solution. Moving the ExternalDNS project outside of Kubernetes projects would cause: Problems (re-)establishing user trust which could eventually lead to fragmentation and duplication. It would be hard to establish in which organization the project should be moved to. The most natural would be Zalando\u2019s organization, being the company that put most of the work on the project. While it is possible to assume Zalando\u2019s commitment to open-source, that would be a strategic mistake for the project community and for the Kubernetes ecosystem due to the obvious lack of neutrality. Lack of resources to test, lack of issue management via automation. For those reasons, we propose to move ExternalDNS out of the Kubernetes incubator, to live either under the kubernetes or kubernetes-sigs organization to keep being a vital part of the Kubernetes ecosystem.","title":"Proposal"},{"location":"20190708-external-dns-incubator/#details","text":"","title":"Details"},{"location":"20190708-external-dns-incubator/#graduation-criteria","text":"ExternalDNS is a two years old project widely used in production by many companies. The implementation for the three major cloud providers (AWS, Azure, GCP) is stable, not changing its logic and the project is being used in production by many company using Kubernetes. We have evidence that many companies are using ExternalDNS in production, but it is out of scope for this proposal to collect a comprehensive list of companies. The project was quoted by a number of tutorials on the web, including the official tutorials from AWS . ExternalDNS can\u2019t be consider to be \u201cdone\u201d: while the core functionality has been implemented, there is lack of integration testing and structural changes that are needed. Those are identified in the project roadmap, which is roughly made of the following items: Decoupling of the providers Implementation proposal Development Bug fixing and performance optimization (i.e. rate limiting on cloud providers) Integration testing suite, to be implemented at least for the \u201cstable\u201d providers For those reasons, we consider ExternalDNS to be in Beta state as a project. We believe that once the items mentioned above will be implemented, the project can reach a declared GA status. There are a number of other factors that need to be covered to fully describe the state of the project, including who are the maintainers, the way we release and manage the project and so on.","title":"Graduation Criteria"},{"location":"20190708-external-dns-incubator/#maintainers","text":"The project has the following maintainers: hjacobs Raffo linki njuettner The list of maintainers shrunk over time as people moved out of the original development team (all the team members were working at Zalando at the time of project creation) and the project required less work. The high number of providers contributed to the project pose a maintainability challenge: it is hard to bring the providers forward in terms of functionalities or even test them. The maintainers believe that the plan to transform the current Provider interface from a Go interface to an API will allow for enough decoupling and to hand over the maintenance of those plugins to the contributors themselves, see the risk and mitigations section for further details.","title":"Maintainers"},{"location":"20190708-external-dns-incubator/#release-process-artifacts","text":"The project uses the free quota of TravisCI to run tests for the project. The release pipeline for the project is currently fully owned by Zalando. It runs on the internal system of the company (closed source) which external maintainers/users can\u2019t access and that pushes images to the publicly accessible docker registry available at the URL registry.opensource.zalan.do . The docker registry service is provided as best effort with no sort of SLA and the maintainers team openly suggests the users to build and maintain their own docker image based on the provided Dockerfiles. Providing a vanity URL for the docker images was consider a non goal till now, but the community seems to be wanting official images from a GCR domain, similarly to what is available for other parts of official Kubernetes projects. ExternalDNS does not follow a specific release cycle. Releases are made often when there are major contributions (i.e. new providers) or important bug fixes. That said, the default branch is considered stable and can be used as well to build images.","title":"Release process, artifacts"},{"location":"20190708-external-dns-incubator/#risks-and-mitigations","text":"The following are risks that were identified: Low number of maintainers: we are currently facing issues keeping up with the number of pull requests and issues giving the low number of maintainers. The list of maintainers already shrunk from 8 maintainers to 4. Issues maintaining community contributed providers: we often lack access to external providers (i.e. InfoBlox, etc.) and this means that we cannot verify the implementations and/or run regression tests that go beyond unit testing. Somewhat low quality of releases due to lack of integration testing. We think that the following actions will constitute appropriate mitigations: Decoupling the providers via an API will allow us to resolve the problem of the providers. Being the project already more than 2 years old and given that there are 18 providers implemented, we possess enough information to define an API that we can be stable in a short timeframe. Once this is stable, the problem of testing the providers can be deferred to be a provider\u2019s responsibility. This will also reduce the scope of External DNS core code, which means that there will be no need for a further increase of the maintaining team. We added integration testing for the main cloud providers to the roadmap for the 1.0 release to make sure that we cover the mostly used ones. We believe that this item should be tackled independently from the decoupling of providers as it would be capable of generating value independently from the result of the decoupling efforts. With the move to the Kubernetes incubation, we hope that we will be able to access the testing resources of the Kubernetes project. In this way, we hope to decouple the project from the dependency on Zalando\u2019s internal CI tool. This will help open up the possibility to increase the visibility on the project from external contributors, which currently would be blocked by the lack of access to the software used for the whole release pipeline.","title":"Risks and Mitigations"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines \u00b6 Welcome to Kubernetes. We are excited about the prospect of you joining our community ! The Kubernetes community abides by the CNCF code of conduct . Here is an excerpt: As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. Getting Started \u00b6 We have full documentation on how to get started contributing here: Contributor License Agreement Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests Kubernetes Contributor Guide - Main contributor documentation, or you can just jump directly to the contributing section Contributor Cheat Sheet - Common resources for existing developers Mentorship \u00b6 Mentoring Initiatives - We have a diverse set of mentorship programs available that are always looking for volunteers! Contact Information \u00b6 Slack channel Mailing list","title":"Kubernetes Contributions"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Welcome to Kubernetes. We are excited about the prospect of you joining our community ! The Kubernetes community abides by the CNCF code of conduct . Here is an excerpt: As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#getting-started","text":"We have full documentation on how to get started contributing here: Contributor License Agreement Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests Kubernetes Contributor Guide - Main contributor documentation, or you can just jump directly to the contributing section Contributor Cheat Sheet - Common resources for existing developers","title":"Getting Started"},{"location":"CONTRIBUTING/#mentorship","text":"Mentoring Initiatives - We have a diverse set of mentorship programs available that are always looking for volunteers!","title":"Mentorship"},{"location":"CONTRIBUTING/#contact-information","text":"Slack channel Mailing list","title":"Contact Information"},{"location":"LICENSE/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \u201cLicense\u201d shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \u201cLicensor\u201d shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \u201cLegal Entity\u201d shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \u201ccontrol\u201d means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \u201cYou\u201d (or \u201cYour\u201d) shall mean an individual or Legal Entity exercising permissions granted by this License. \u201cSource\u201d form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \u201cObject\u201d form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \u201cWork\u201d shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \u201cDerivative Works\u201d shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \u201cContribution\u201d shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \u201csubmitted\u201d means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \u201cNot a Contribution.\u201d \u201cContributor\u201d shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and \u00a9 You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \u201cNOTICE\u201d text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"{}\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright {yyyy} {name of copyright owner} Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"code-of-conduct/","text":"Kubernetes Community Code of Conduct \u00b6 Please refer to our Kubernetes Community Code of Conduct","title":"Code of Conduct"},{"location":"code-of-conduct/#kubernetes-community-code-of-conduct","text":"Please refer to our Kubernetes Community Code of Conduct","title":"Kubernetes Community Code of Conduct"},{"location":"faq/","text":"Frequently asked questions \u00b6 How is ExternalDNS useful to me? \u00b6 You\u2019ve probably created many deployments. Typically, you expose your deployment to the Internet by creating a Service with type=LoadBalancer . Depending on your environment, this usually assigns a random publicly available endpoint to your service that you can access from anywhere in the world. On Google Kubernetes Engine, this is a public IP address: $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx 10.3.249.226 35.187.104.85 80:32281/TCP 1m But dealing with IPs for service discovery isn\u2019t nice, so you register this IP with your DNS provider under a better name\u2014most likely, one that corresponds to your service name. If the IP changes, you update the DNS record accordingly. Those times are over! ExternalDNS takes care of that last step for you by keeping your DNS records synchronized with your external entry points. ExternalDNS\u2019 usefulness also becomes clear when you use Ingresses to allow external traffic into your cluster. Via Ingress, you can tell Kubernetes to route traffic to different services based on certain HTTP request attributes, e.g. the Host header: $ kubectl get ing NAME HOSTS ADDRESS PORTS AGE entrypoint frontend.example.org,backend.example.org 35.186.250.78 80 1m But there\u2019s nothing that actually makes clients resolve those hostnames to the Ingress\u2019 IP address. Again, you normally have to register each entry with your DNS provider. Only if you\u2019re lucky can you use a wildcard, like in the example above. ExternalDNS can solve this for you as well. Which DNS providers are supported? \u00b6 Please check the provider status table for the list of supported providers and their status. As stated in the README, we are currently looking for stable maintainers for those providers, to ensure that bugfixes and new features will be available for all of those. Which Kubernetes objects are supported? \u00b6 Services exposed via type=LoadBalancer , type=ExternalName , type=NodePort , and for the hostnames defined in Ingress objects as well as headless hostPort services. How do I specify a DNS name for my Kubernetes objects? \u00b6 There are three sources of information for ExternalDNS to decide on DNS name. ExternalDNS will pick one in order as listed below: For ingress objects ExternalDNS will create a DNS record based on the hosts specified for the ingress object, as well as the external-dns.alpha.kubernetes.io/hostname annotation. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the loadbalancer IP, it also will look for the annotation external-dns.alpha.kubernetes.io/internal-hostname on the service and use the service IP. For ingresses, you can optionally force ExternalDNS to create records based on either the hosts specified or the external-dns.alpha.kubernetes.io/hostname annotation. This behavior is controlled by setting the external-dns.alpha.kubernetes.io/ingress-hostname-source annotation on that ingress to either defined-hosts-only or annotation-only . If compatibility mode is enabled (e.g. --compatibility={mate,molecule} flag), External DNS will parse annotations used by Zalando/Mate, wearemolecule/route53-kubernetes. Compatibility mode with Kops DNS Controller is planned to be added in the future. If --fqdn-template flag is specified, e.g. --fqdn-template={{.Name}}.my-org.com , ExternalDNS will use service/ingress specifications for the provided template to generate DNS name. Can I specify multiple global FQDN templates? \u00b6 Yes, you can. Pass in a comma separated list to --fqdn-template . Beaware this will double (triple, etc) the amount of DNS entries based on how many services, ingresses and so on you have and will get you faster towards the API request limit of your DNS provider. Which Service and Ingress controllers are supported? \u00b6 Regarding Services, we\u2019ll support the OSI Layer 4 load balancers that Kubernetes creates on AWS and Google Kubernetes Engine, and possibly other clusters running on Google Compute Engine. Regarding Ingress, we\u2019ll support: * Google\u2019s Ingress Controller on GKE that integrates with their Layer 7 load balancers (GLBC) * nginx-ingress-controller v0.9.x with a fronting Service * Zalando\u2019s AWS Ingress controller , based on AWS ALBs and Skipper * Traefik * version 1.7, when kubernetes.ingressEndpoint is configured ( kubernetes.ingressEndpoint.useDefaultPublishedService in the Helm chart ) * versions >=2.0, when providers.kubernetesIngress.ingressEndpoint is configured ( providers.kubernetesIngress.publishedService.enabled is set to true in the new Helm chart ) Are other Ingress Controllers supported? \u00b6 For Ingress objects, ExternalDNS will attempt to discover the target hostname of the relevant Ingress Controller automatically. If you are using an Ingress Controller that is not listed above you may have issues with ExternalDNS not discovering Endpoints and consequently not creating any DNS records. As a workaround, it is possible to force create an Endpoint by manually specifying a target host/IP for the records to be created by setting the annotation external-dns.alpha.kubernetes.io/target in the Ingress object. Another reason you may want to override the ingress hostname or IP address is if you have an external mechanism for handling failover across ingress endpoints. Possible scenarios for this would include using keepalived-vip to manage failover faster than DNS TTLs might expire. Note that if you set the target to a hostname, then a CNAME record will be created. In this case, the hostname specified in the Ingress object\u2019s annotation must already exist. (i.e. you have a Service resource for your Ingress Controller with the external-dns.alpha.kubernetes.io/hostname annotation set to the same value.) What about other projects similar to ExternalDNS? \u00b6 ExternalDNS is a joint effort to unify different projects accomplishing the same goals, namely: Kops\u2019 DNS Controller Zalando\u2019s Mate Molecule Software\u2019s route53-kubernetes We strive to make the migration from these implementations a smooth experience. This means that, for some time, we\u2019ll support their annotation semantics in ExternalDNS and allow both implementations to run side-by-side. This enables you to migrate incrementally and slowly phase out the other implementation. How does it work with other implementations and legacy records? \u00b6 ExternalDNS will allow you to opt into any Services and Ingresses that you want it to consider, by an annotation. This way, it can co-exist with other implementations running in the same cluster if they also support this pattern. However, we\u2019ll most likely declare ExternalDNS to be the default implementation. This means that ExternalDNS will consider Services and Ingresses that don\u2019t specifically declare which controller they want to be processed by; this is similar to the ingress.class annotation on GKE. I\u2019m afraid you will mess up my DNS records! \u00b6 Since v0.3, ExternalDNS can be configured to use an ownership registry. When this option is enabled, ExternalDNS will keep track of which records it has control over, and will never modify any records over which it doesn\u2019t have control. This is a fundamental requirement to operate ExternalDNS safely when there might be other actors creating DNS records in the same target space. For now ExternalDNS uses TXT records to label owned records, and there might be other alternatives coming in the future releases. Does anyone use ExternalDNS in production? \u00b6 Yes, multiple companies are using ExternalDNS in production. Zalando, as an example, has been using it in production since its v0.3 release, mostly using the AWS provider. How can we start using ExternalDNS? \u00b6 Check out the following descriptive tutorials on how to run ExternalDNS in GKE and AWS or any other supported provider. Why is ExternalDNS only adding a single IP address in Route 53 on AWS when using the nginx-ingress-controller ? How do I get it to use the FQDN of the ELB assigned to my nginx-ingress-controller Service instead? \u00b6 By default the nginx-ingress-controller assigns a single IP address to an Ingress resource when it\u2019s created. ExternalDNS uses what\u2019s assigned to the Ingress resource, so it too will use this single IP address when adding the record in Route 53. In most AWS deployments, you\u2019ll instead want the Route 53 entry to be the FQDN of the ELB that is assigned to the nginx-ingress-controller Service. To accomplish this, when you create the nginx-ingress-controller Deployment, you need to provide the --publish-service option to the /nginx-ingress-controller executable under args . Once this is deployed new Ingress resources will get the ELB\u2019s FQDN and ExternalDNS will use the same when creating records in Route 53. According to the nginx-ingress-controller docs the value you need to provide --publish-service is: Service fronting the ingress controllers. Takes the form namespace/name. The controller will set the endpoint records on the ingress objects to reflect those on the service. For example if your nginx-ingress-controller Service\u2019s name is nginx-ingress-controller-svc and it\u2019s in the default namespace the start of your resource YAML might look like the following. Note the second to last line. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress-controller spec: replicas: 1 selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress spec: hostNetwork: false containers: - name: nginx-ingress-controller image: \"gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.11\" imagePullPolicy: \"IfNotPresent\" args: - /nginx-ingress-controller - --default-backend-service={your-backend-service} - --publish-service=default/nginx-ingress-controller-svc - --configmap={your-configmap} I have a Service/Ingress but it\u2019s ignored by ExternalDNS. Why? \u00b6 ExternalDNS can be configured to only use Services or Ingresses as source. In case Services or Ingresses seem to be ignored in your setup, consider checking how the flag --source was configured when deployed. For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/267. I\u2019m using an ELB with TXT registry but the CNAME record clashes with the TXT record. How to avoid this? \u00b6 CNAMEs cannot co-exist with other records, therefore you can use the --txt-prefix flag which makes sure to create a TXT record with a name following the pattern prefix.<CNAME record> . For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/262. Can I force ExternalDNS to create CNAME records for ELB/ALB? \u00b6 The default logic is: when a target looks like an ELB/ALB, ExternalDNS will create ALIAS records for it. Under certain circumstances you want to force ExternalDNS to create CNAME records instead. If you want to do that, start ExternalDNS with the --aws-prefer-cname flag. Why should I want to force ExternalDNS to create CNAME records for ELB/ALB? Some motivations of users were: \u201cOur hosted zones records are synchronized with our enterprise DNS. The record type ALIAS is an AWS proprietary record type and AWS allows you to set a DNS record directly on AWS resources. Since this is not a DNS RfC standard and therefore can not be transferred and created in our enterprise DNS. So we need to force CNAME creation instead.\u201d or \u201cIn case of ALIAS if we do nslookup with domain name, it will return only IPs of ELB. So it is always difficult for us to locate ELB in AWS console to which domain is pointing. If we configure it with CNAME it will return exact ELB CNAME, which is more helpful.!\u201d Which permissions do I need when running ExternalDNS on a GCE or GKE node. \u00b6 You need to add either https://www.googleapis.com/auth/ndev.clouddns.readwrite or https://www.googleapis.com/auth/cloud-platform on your instance group\u2019s scope. What metrics can I get from ExternalDNS and what do they mean? \u00b6 ExternalDNS exposes 2 types of metrics: Sources and Registry errors. Source s are mostly Kubernetes API objects. Examples of source errors may be connection errors to the Kubernetes API server itself or missing RBAC permissions. It can also stem from incompatible configuration in the objects itself like invalid characters, processing a broken fqdnTemplate, etc. Registry errors are mostly Provider errors, unless there\u2019s some coding flaw in the registry package. Provider errors often arise due to accessing their APIs due to network or missing cloud-provider permissions when reading records. When applying a changeset, errors will arise if the changeset applied is incompatible with the current state. In case of an increased error count, you could correlate them with the http_request_duration_seconds{handler=\"instrumented_http\"} metric which should show increased numbers for status codes 4xx (permissions, configuration, invalid changeset) or 5xx (apiserver down). You can use the host label in the metric to figure out if the request was against the Kubernetes API server (Source errors) or the DNS provider API (Registry/Provider errors). Here is the full list of available metrics provided by ExternalDNS: Name Description Type external_dns_controller_last_sync_timestamp_seconds Timestamp of last successful sync with the DNS provider Gauge external_dns_registry_endpoints_total Number of Endpoints in all sources Gauge external_dns_registry_errors_total Number of Registry errors Counter external_dns_source_endpoints_total Number of Endpoints in the registry Gauge external_dns_source_errors_total Number of Source errors Counter external_dns_controller_verified_records Number of DNS A-records that exists both in Gauge source & registry external_dns_registry_a_records Number of A records in registry Gauge external_dns_source_a_records Number of A records in source Gauge How can I run ExternalDNS under a specific GCP Service Account, e.g. to access DNS records in other projects? \u00b6 Have a look at https://github.com/linki/mate/blob/v0.6.2/examples/google/README.md#permissions How do I configure multiple Sources via environment variables? (also applies to domain filters) \u00b6 Separate the individual values via a line break. The equivalent of --source=service --source=ingress would be service\\ningress . However, it can be tricky do define that depending on your environment. The following examples work (zsh): Via docker: $ docker run \\ -e EXTERNAL_DNS_SOURCE = $'service\\ningress' \\ -e EXTERNAL_DNS_PROVIDER = google \\ -e EXTERNAL_DNS_DOMAIN_FILTER = $'foo.com\\nbar.com' \\ registry.k8s.io/external-dns/external-dns:v0.13.1 time=\"2017-08-08T14:10:26Z\" level=info msg=\"config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... Locally: $ export EXTERNAL_DNS_SOURCE = $'service\\ningress' $ external-dns --provider = google INFO[0000] config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... $ EXTERNAL_DNS_SOURCE=$'service\\ningress' external-dns --provider=google INFO[0000] config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... In a Kubernetes manifest: spec : containers : - name : external-dns args : - --provider=google env : - name : EXTERNAL_DNS_SOURCE value : \"service\\ningress\" Or preferably: spec : containers : - name : external-dns args : - --provider=google env : - name : EXTERNAL_DNS_SOURCE value : |- service ingress Running an internal and external dns service \u00b6 Sometimes you need to run an internal and an external dns service. The internal one should provision hostnames used on the internal network (perhaps inside a VPC), and the external one to expose DNS to the internet. To do this with ExternalDNS you can use the --annotation-filter to specifically tie an instance of ExternalDNS to an instance of an ingress controller. Let\u2019s assume you have two ingress controllers nginx-internal and nginx-external then you can start two ExternalDNS providers one with --annotation-filter=kubernetes.io/ingress.class in (nginx-internal) and one with --annotation-filter=kubernetes.io/ingress.class in (nginx-external) . If you need to search for multiple values of said annotation, you can provide a comma separated list, like so: --annotation-filter=kubernetes.io/ingress.class in (nginx-internal, alb-ingress-internal) . Beware when using multiple sources, e.g. --source=service --source=ingress , --annotation-filter will filter every given source objects. If you need to filter only one specific source you have to run a separated external dns service containing only the wanted --source and --annotation-filter . Note: Filtering based on annotation means that the external-dns controller will receive all resources of that kind and then filter on the client-side. In larger clusters with many resources which change frequently this can cause performance issues. If only some resources need to be managed by an instance of external-dns then label filtering can be used instead of annotation filtering. This means that only those resources which match the selector specified in --label-filter will be passed to the controller. How do I specify that I want the DNS record to point to either the Node\u2019s public or private IP when it has both? \u00b6 If your Nodes have both public and private IP addresses, you might want to write DNS records with one or the other. For example, you may want to write a DNS record in a private zone that resolves to your Nodes\u2019 private IPs so that traffic never leaves your private network. To accomplish this, set this annotation on your service: external-dns.alpha.kubernetes.io/access=private Conversely, to force the public IP: external-dns.alpha.kubernetes.io/access=public If this annotation is not set, and the node has both public and private IP addresses, then the public IP will be used by default. Some loadbalancer implementations assign multiple IP addresses as external addresses. You can filter the generated targets by their networks using --target-net-filter=10.0.0.0/8 or --exclude-target-net=10.0.0.0/8 . Can external-dns manage(add/remove) records in a hosted zone which is setup in different AWS account? \u00b6 Yes, give it the correct cross-account/assume-role permissions and use the --aws-assume-role flag https://github.com/kubernetes-sigs/external-dns/pull/524#issue-181256561 How do I provide multiple values to the annotation external-dns.alpha.kubernetes.io/hostname ? \u00b6 Separate them by , . Are there official Docker images provided? \u00b6 When we tag a new release, we push a container image to the Kubernetes projects official container registry with the following name: registry.k8s.io/external-dns/external-dns As tags, you use the external-dns release of choice(i.e. v0.7.6 ). A latest tag is not provided in the container registry. If you wish to build your own image, you can use the provided Dockerfile as a starting point. Which architectures are supported? \u00b6 From v0.7.5 on we support amd64 , arm32v7 and arm64v8 . This means that you can run ExternalDNS on a Kubernetes cluster backed by Rasperry Pis or on ARM instances in the cloud as well as more traditional machines backed by amd64 compatible CPUs. Which operating systems are supported? \u00b6 At the time of writing we only support GNU/linux and we have no plans of supporting Windows or other operating systems. Why am I seeing time out errors even though I have connectivity to my cluster? \u00b6 If you\u2019re seeing an error such as this: FATA[0060] failed to sync cache: timed out waiting for the condition You may not have the correct permissions required to query all the necessary resources in your kubernetes cluster. Specifically, you may be running in a namespace that you don\u2019t have these permissions in. By default, commands are run against the default namespace. Try changing this to your particular namespace to see if that fixes the issue.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"faq/#how-is-externaldns-useful-to-me","text":"You\u2019ve probably created many deployments. Typically, you expose your deployment to the Internet by creating a Service with type=LoadBalancer . Depending on your environment, this usually assigns a random publicly available endpoint to your service that you can access from anywhere in the world. On Google Kubernetes Engine, this is a public IP address: $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx 10.3.249.226 35.187.104.85 80:32281/TCP 1m But dealing with IPs for service discovery isn\u2019t nice, so you register this IP with your DNS provider under a better name\u2014most likely, one that corresponds to your service name. If the IP changes, you update the DNS record accordingly. Those times are over! ExternalDNS takes care of that last step for you by keeping your DNS records synchronized with your external entry points. ExternalDNS\u2019 usefulness also becomes clear when you use Ingresses to allow external traffic into your cluster. Via Ingress, you can tell Kubernetes to route traffic to different services based on certain HTTP request attributes, e.g. the Host header: $ kubectl get ing NAME HOSTS ADDRESS PORTS AGE entrypoint frontend.example.org,backend.example.org 35.186.250.78 80 1m But there\u2019s nothing that actually makes clients resolve those hostnames to the Ingress\u2019 IP address. Again, you normally have to register each entry with your DNS provider. Only if you\u2019re lucky can you use a wildcard, like in the example above. ExternalDNS can solve this for you as well.","title":"How is ExternalDNS useful to me?"},{"location":"faq/#which-dns-providers-are-supported","text":"Please check the provider status table for the list of supported providers and their status. As stated in the README, we are currently looking for stable maintainers for those providers, to ensure that bugfixes and new features will be available for all of those.","title":"Which DNS providers are supported?"},{"location":"faq/#which-kubernetes-objects-are-supported","text":"Services exposed via type=LoadBalancer , type=ExternalName , type=NodePort , and for the hostnames defined in Ingress objects as well as headless hostPort services.","title":"Which Kubernetes objects are supported?"},{"location":"faq/#how-do-i-specify-a-dns-name-for-my-kubernetes-objects","text":"There are three sources of information for ExternalDNS to decide on DNS name. ExternalDNS will pick one in order as listed below: For ingress objects ExternalDNS will create a DNS record based on the hosts specified for the ingress object, as well as the external-dns.alpha.kubernetes.io/hostname annotation. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the loadbalancer IP, it also will look for the annotation external-dns.alpha.kubernetes.io/internal-hostname on the service and use the service IP. For ingresses, you can optionally force ExternalDNS to create records based on either the hosts specified or the external-dns.alpha.kubernetes.io/hostname annotation. This behavior is controlled by setting the external-dns.alpha.kubernetes.io/ingress-hostname-source annotation on that ingress to either defined-hosts-only or annotation-only . If compatibility mode is enabled (e.g. --compatibility={mate,molecule} flag), External DNS will parse annotations used by Zalando/Mate, wearemolecule/route53-kubernetes. Compatibility mode with Kops DNS Controller is planned to be added in the future. If --fqdn-template flag is specified, e.g. --fqdn-template={{.Name}}.my-org.com , ExternalDNS will use service/ingress specifications for the provided template to generate DNS name.","title":"How do I specify a DNS name for my Kubernetes objects?"},{"location":"faq/#can-i-specify-multiple-global-fqdn-templates","text":"Yes, you can. Pass in a comma separated list to --fqdn-template . Beaware this will double (triple, etc) the amount of DNS entries based on how many services, ingresses and so on you have and will get you faster towards the API request limit of your DNS provider.","title":"Can I specify multiple global FQDN templates?"},{"location":"faq/#which-service-and-ingress-controllers-are-supported","text":"Regarding Services, we\u2019ll support the OSI Layer 4 load balancers that Kubernetes creates on AWS and Google Kubernetes Engine, and possibly other clusters running on Google Compute Engine. Regarding Ingress, we\u2019ll support: * Google\u2019s Ingress Controller on GKE that integrates with their Layer 7 load balancers (GLBC) * nginx-ingress-controller v0.9.x with a fronting Service * Zalando\u2019s AWS Ingress controller , based on AWS ALBs and Skipper * Traefik * version 1.7, when kubernetes.ingressEndpoint is configured ( kubernetes.ingressEndpoint.useDefaultPublishedService in the Helm chart ) * versions >=2.0, when providers.kubernetesIngress.ingressEndpoint is configured ( providers.kubernetesIngress.publishedService.enabled is set to true in the new Helm chart )","title":"Which Service and Ingress controllers are supported?"},{"location":"faq/#are-other-ingress-controllers-supported","text":"For Ingress objects, ExternalDNS will attempt to discover the target hostname of the relevant Ingress Controller automatically. If you are using an Ingress Controller that is not listed above you may have issues with ExternalDNS not discovering Endpoints and consequently not creating any DNS records. As a workaround, it is possible to force create an Endpoint by manually specifying a target host/IP for the records to be created by setting the annotation external-dns.alpha.kubernetes.io/target in the Ingress object. Another reason you may want to override the ingress hostname or IP address is if you have an external mechanism for handling failover across ingress endpoints. Possible scenarios for this would include using keepalived-vip to manage failover faster than DNS TTLs might expire. Note that if you set the target to a hostname, then a CNAME record will be created. In this case, the hostname specified in the Ingress object\u2019s annotation must already exist. (i.e. you have a Service resource for your Ingress Controller with the external-dns.alpha.kubernetes.io/hostname annotation set to the same value.)","title":"Are other Ingress Controllers supported?"},{"location":"faq/#what-about-other-projects-similar-to-externaldns","text":"ExternalDNS is a joint effort to unify different projects accomplishing the same goals, namely: Kops\u2019 DNS Controller Zalando\u2019s Mate Molecule Software\u2019s route53-kubernetes We strive to make the migration from these implementations a smooth experience. This means that, for some time, we\u2019ll support their annotation semantics in ExternalDNS and allow both implementations to run side-by-side. This enables you to migrate incrementally and slowly phase out the other implementation.","title":"What about other projects similar to ExternalDNS?"},{"location":"faq/#how-does-it-work-with-other-implementations-and-legacy-records","text":"ExternalDNS will allow you to opt into any Services and Ingresses that you want it to consider, by an annotation. This way, it can co-exist with other implementations running in the same cluster if they also support this pattern. However, we\u2019ll most likely declare ExternalDNS to be the default implementation. This means that ExternalDNS will consider Services and Ingresses that don\u2019t specifically declare which controller they want to be processed by; this is similar to the ingress.class annotation on GKE.","title":"How does it work with other implementations and legacy records?"},{"location":"faq/#im-afraid-you-will-mess-up-my-dns-records","text":"Since v0.3, ExternalDNS can be configured to use an ownership registry. When this option is enabled, ExternalDNS will keep track of which records it has control over, and will never modify any records over which it doesn\u2019t have control. This is a fundamental requirement to operate ExternalDNS safely when there might be other actors creating DNS records in the same target space. For now ExternalDNS uses TXT records to label owned records, and there might be other alternatives coming in the future releases.","title":"I'm afraid you will mess up my DNS records!"},{"location":"faq/#does-anyone-use-externaldns-in-production","text":"Yes, multiple companies are using ExternalDNS in production. Zalando, as an example, has been using it in production since its v0.3 release, mostly using the AWS provider.","title":"Does anyone use ExternalDNS in production?"},{"location":"faq/#how-can-we-start-using-externaldns","text":"Check out the following descriptive tutorials on how to run ExternalDNS in GKE and AWS or any other supported provider.","title":"How can we start using ExternalDNS?"},{"location":"faq/#why-is-externaldns-only-adding-a-single-ip-address-in-route-53-on-aws-when-using-the-nginx-ingress-controller-how-do-i-get-it-to-use-the-fqdn-of-the-elb-assigned-to-my-nginx-ingress-controller-service-instead","text":"By default the nginx-ingress-controller assigns a single IP address to an Ingress resource when it\u2019s created. ExternalDNS uses what\u2019s assigned to the Ingress resource, so it too will use this single IP address when adding the record in Route 53. In most AWS deployments, you\u2019ll instead want the Route 53 entry to be the FQDN of the ELB that is assigned to the nginx-ingress-controller Service. To accomplish this, when you create the nginx-ingress-controller Deployment, you need to provide the --publish-service option to the /nginx-ingress-controller executable under args . Once this is deployed new Ingress resources will get the ELB\u2019s FQDN and ExternalDNS will use the same when creating records in Route 53. According to the nginx-ingress-controller docs the value you need to provide --publish-service is: Service fronting the ingress controllers. Takes the form namespace/name. The controller will set the endpoint records on the ingress objects to reflect those on the service. For example if your nginx-ingress-controller Service\u2019s name is nginx-ingress-controller-svc and it\u2019s in the default namespace the start of your resource YAML might look like the following. Note the second to last line. apiVersion: apps/v1 kind: Deployment metadata: name: nginx-ingress-controller spec: replicas: 1 selector: matchLabels: app: nginx-ingress template: metadata: labels: app: nginx-ingress spec: hostNetwork: false containers: - name: nginx-ingress-controller image: \"gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.11\" imagePullPolicy: \"IfNotPresent\" args: - /nginx-ingress-controller - --default-backend-service={your-backend-service} - --publish-service=default/nginx-ingress-controller-svc - --configmap={your-configmap}","title":"Why is ExternalDNS only adding a single IP address in Route 53 on AWS when using the nginx-ingress-controller? How do I get it to use the FQDN of the ELB assigned to my nginx-ingress-controller Service instead?"},{"location":"faq/#i-have-a-serviceingress-but-its-ignored-by-externaldns-why","text":"ExternalDNS can be configured to only use Services or Ingresses as source. In case Services or Ingresses seem to be ignored in your setup, consider checking how the flag --source was configured when deployed. For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/267.","title":"I have a Service/Ingress but it's ignored by ExternalDNS. Why?"},{"location":"faq/#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this","text":"CNAMEs cannot co-exist with other records, therefore you can use the --txt-prefix flag which makes sure to create a TXT record with a name following the pattern prefix.<CNAME record> . For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/262.","title":"I'm using an ELB with TXT registry but the CNAME record clashes with the TXT record. How to avoid this?"},{"location":"faq/#can-i-force-externaldns-to-create-cname-records-for-elbalb","text":"The default logic is: when a target looks like an ELB/ALB, ExternalDNS will create ALIAS records for it. Under certain circumstances you want to force ExternalDNS to create CNAME records instead. If you want to do that, start ExternalDNS with the --aws-prefer-cname flag. Why should I want to force ExternalDNS to create CNAME records for ELB/ALB? Some motivations of users were: \u201cOur hosted zones records are synchronized with our enterprise DNS. The record type ALIAS is an AWS proprietary record type and AWS allows you to set a DNS record directly on AWS resources. Since this is not a DNS RfC standard and therefore can not be transferred and created in our enterprise DNS. So we need to force CNAME creation instead.\u201d or \u201cIn case of ALIAS if we do nslookup with domain name, it will return only IPs of ELB. So it is always difficult for us to locate ELB in AWS console to which domain is pointing. If we configure it with CNAME it will return exact ELB CNAME, which is more helpful.!\u201d","title":"Can I force ExternalDNS to create CNAME records for ELB/ALB?"},{"location":"faq/#which-permissions-do-i-need-when-running-externaldns-on-a-gce-or-gke-node","text":"You need to add either https://www.googleapis.com/auth/ndev.clouddns.readwrite or https://www.googleapis.com/auth/cloud-platform on your instance group\u2019s scope.","title":"Which permissions do I need when running ExternalDNS on a GCE or GKE node."},{"location":"faq/#what-metrics-can-i-get-from-externaldns-and-what-do-they-mean","text":"ExternalDNS exposes 2 types of metrics: Sources and Registry errors. Source s are mostly Kubernetes API objects. Examples of source errors may be connection errors to the Kubernetes API server itself or missing RBAC permissions. It can also stem from incompatible configuration in the objects itself like invalid characters, processing a broken fqdnTemplate, etc. Registry errors are mostly Provider errors, unless there\u2019s some coding flaw in the registry package. Provider errors often arise due to accessing their APIs due to network or missing cloud-provider permissions when reading records. When applying a changeset, errors will arise if the changeset applied is incompatible with the current state. In case of an increased error count, you could correlate them with the http_request_duration_seconds{handler=\"instrumented_http\"} metric which should show increased numbers for status codes 4xx (permissions, configuration, invalid changeset) or 5xx (apiserver down). You can use the host label in the metric to figure out if the request was against the Kubernetes API server (Source errors) or the DNS provider API (Registry/Provider errors). Here is the full list of available metrics provided by ExternalDNS: Name Description Type external_dns_controller_last_sync_timestamp_seconds Timestamp of last successful sync with the DNS provider Gauge external_dns_registry_endpoints_total Number of Endpoints in all sources Gauge external_dns_registry_errors_total Number of Registry errors Counter external_dns_source_endpoints_total Number of Endpoints in the registry Gauge external_dns_source_errors_total Number of Source errors Counter external_dns_controller_verified_records Number of DNS A-records that exists both in Gauge source & registry external_dns_registry_a_records Number of A records in registry Gauge external_dns_source_a_records Number of A records in source Gauge","title":"What metrics can I get from ExternalDNS and what do they mean?"},{"location":"faq/#how-can-i-run-externaldns-under-a-specific-gcp-service-account-eg-to-access-dns-records-in-other-projects","text":"Have a look at https://github.com/linki/mate/blob/v0.6.2/examples/google/README.md#permissions","title":"How can I run ExternalDNS under a specific GCP Service Account, e.g. to access DNS records in other projects?"},{"location":"faq/#how-do-i-configure-multiple-sources-via-environment-variables-also-applies-to-domain-filters","text":"Separate the individual values via a line break. The equivalent of --source=service --source=ingress would be service\\ningress . However, it can be tricky do define that depending on your environment. The following examples work (zsh): Via docker: $ docker run \\ -e EXTERNAL_DNS_SOURCE = $'service\\ningress' \\ -e EXTERNAL_DNS_PROVIDER = google \\ -e EXTERNAL_DNS_DOMAIN_FILTER = $'foo.com\\nbar.com' \\ registry.k8s.io/external-dns/external-dns:v0.13.1 time=\"2017-08-08T14:10:26Z\" level=info msg=\"config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... Locally: $ export EXTERNAL_DNS_SOURCE = $'service\\ningress' $ external-dns --provider = google INFO[0000] config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... $ EXTERNAL_DNS_SOURCE=$'service\\ningress' external-dns --provider=google INFO[0000] config: &{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ... In a Kubernetes manifest: spec : containers : - name : external-dns args : - --provider=google env : - name : EXTERNAL_DNS_SOURCE value : \"service\\ningress\" Or preferably: spec : containers : - name : external-dns args : - --provider=google env : - name : EXTERNAL_DNS_SOURCE value : |- service ingress","title":"How do I configure multiple Sources via environment variables? (also applies to domain filters)"},{"location":"faq/#running-an-internal-and-external-dns-service","text":"Sometimes you need to run an internal and an external dns service. The internal one should provision hostnames used on the internal network (perhaps inside a VPC), and the external one to expose DNS to the internet. To do this with ExternalDNS you can use the --annotation-filter to specifically tie an instance of ExternalDNS to an instance of an ingress controller. Let\u2019s assume you have two ingress controllers nginx-internal and nginx-external then you can start two ExternalDNS providers one with --annotation-filter=kubernetes.io/ingress.class in (nginx-internal) and one with --annotation-filter=kubernetes.io/ingress.class in (nginx-external) . If you need to search for multiple values of said annotation, you can provide a comma separated list, like so: --annotation-filter=kubernetes.io/ingress.class in (nginx-internal, alb-ingress-internal) . Beware when using multiple sources, e.g. --source=service --source=ingress , --annotation-filter will filter every given source objects. If you need to filter only one specific source you have to run a separated external dns service containing only the wanted --source and --annotation-filter . Note: Filtering based on annotation means that the external-dns controller will receive all resources of that kind and then filter on the client-side. In larger clusters with many resources which change frequently this can cause performance issues. If only some resources need to be managed by an instance of external-dns then label filtering can be used instead of annotation filtering. This means that only those resources which match the selector specified in --label-filter will be passed to the controller.","title":"Running an internal and external dns service"},{"location":"faq/#how-do-i-specify-that-i-want-the-dns-record-to-point-to-either-the-nodes-public-or-private-ip-when-it-has-both","text":"If your Nodes have both public and private IP addresses, you might want to write DNS records with one or the other. For example, you may want to write a DNS record in a private zone that resolves to your Nodes\u2019 private IPs so that traffic never leaves your private network. To accomplish this, set this annotation on your service: external-dns.alpha.kubernetes.io/access=private Conversely, to force the public IP: external-dns.alpha.kubernetes.io/access=public If this annotation is not set, and the node has both public and private IP addresses, then the public IP will be used by default. Some loadbalancer implementations assign multiple IP addresses as external addresses. You can filter the generated targets by their networks using --target-net-filter=10.0.0.0/8 or --exclude-target-net=10.0.0.0/8 .","title":"How do I specify that I want the DNS record to point to either the Node's public or private IP when it has both?"},{"location":"faq/#can-external-dns-manageaddremove-records-in-a-hosted-zone-which-is-setup-in-different-aws-account","text":"Yes, give it the correct cross-account/assume-role permissions and use the --aws-assume-role flag https://github.com/kubernetes-sigs/external-dns/pull/524#issue-181256561","title":"Can external-dns manage(add/remove) records in a hosted zone which is setup in different AWS account?"},{"location":"faq/#how-do-i-provide-multiple-values-to-the-annotation-external-dnsalphakubernetesiohostname","text":"Separate them by , .","title":"How do I provide multiple values to the annotation external-dns.alpha.kubernetes.io/hostname?"},{"location":"faq/#are-there-official-docker-images-provided","text":"When we tag a new release, we push a container image to the Kubernetes projects official container registry with the following name: registry.k8s.io/external-dns/external-dns As tags, you use the external-dns release of choice(i.e. v0.7.6 ). A latest tag is not provided in the container registry. If you wish to build your own image, you can use the provided Dockerfile as a starting point.","title":"Are there official Docker images provided?"},{"location":"faq/#which-architectures-are-supported","text":"From v0.7.5 on we support amd64 , arm32v7 and arm64v8 . This means that you can run ExternalDNS on a Kubernetes cluster backed by Rasperry Pis or on ARM instances in the cloud as well as more traditional machines backed by amd64 compatible CPUs.","title":"Which architectures are supported?"},{"location":"faq/#which-operating-systems-are-supported","text":"At the time of writing we only support GNU/linux and we have no plans of supporting Windows or other operating systems.","title":"Which operating systems are supported?"},{"location":"faq/#why-am-i-seeing-time-out-errors-even-though-i-have-connectivity-to-my-cluster","text":"If you\u2019re seeing an error such as this: FATA[0060] failed to sync cache: timed out waiting for the condition You may not have the correct permissions required to query all the necessary resources in your kubernetes cluster. Specifically, you may be running in a namespace that you don\u2019t have these permissions in. By default, commands are run against the default namespace. Try changing this to your particular namespace to see if that fixes the issue.","title":"Why am I seeing time out errors even though I have connectivity to my cluster?"},{"location":"initial-design/","text":"Proposal: Design of External DNS \u00b6 Background \u00b6 Project proposal Initial discussion This document describes the initial design proposal. External DNS is purposed to fill the existing gap of creating DNS records for Kubernetes resources. While there exist alternative solutions, this project is meant to be a standard way of managing DNS records for Kubernetes. The current project is a fusion of the following projects and driven by its maintainers: Kops DNS Controller Mate wearemolecule/route53-kubernetes Example use case: \u00b6 User runs kubectl create -f ingress.yaml , this will create an ingress as normal. Typically the user would then have to manually create a DNS record pointing the ingress endpoint If the external-dns controller is running on the cluster, it could automatically configure the DNS records instead, by observing the host attribute in the ingress object. Goals \u00b6 Support AWS Route53 and Google Cloud DNS providers DNS for Kubernetes services(type=Loadbalancer) and Ingress Create/update/remove records as according to Kubernetes resources state It should address main requirements and support main features of the projects mentioned above Design \u00b6 Extensibility \u00b6 New cloud providers should be easily pluggable. Initially only AWS/Google platforms are supported. However, in the future we are planning to incorporate CoreDNS and Azure DNS as possible DNS providers Configuration \u00b6 DNS records will be automatically created in multiple situations: 1. Setting spec.rules.host on an ingress object. 2. Setting spec.tls.hosts on an ingress object. 3. Adding the annotation external-dns.alpha.kubernetes.io/hostname on an ingress object. 4. Adding the annotation external-dns.alpha.kubernetes.io/hostname on a type=LoadBalancer service object. Annotations \u00b6 Record configuration should occur via resource annotations. Supported annotations: Annotations Tag external-dns.alpha.kubernetes.io/controller Description Tells a DNS controller to process this service. This is useful when running different DNS controllers at the same time (or different versions of the same controller). The v1 implementation of dns-controller would look for service annotations dns-controller and dns-controller/v1 but not for mate/v1 or dns-controller/v2 Default dns-controller Example dns-controller/v1 Required false \u2014 \u2014 Tag external-dns.alpha.kubernetes.io/hostname Description Fully qualified name of the desired record Default none Example foo.example.org Required Only for services. Ingress hostname is retrieved from spec.rules.host meta data on ingress Compatibility \u00b6 External DNS should be compatible with annotations used by three above mentioned projects. The idea is that resources created and tagged with annotations for other projects should continue to be valid and now managed by External DNS. Mate Mate does not require services/ingress to be tagged. Therefore, it is not safe to run both Mate and External-DNS simultaneously. The idea is that initial release (?) of External DNS will support Mate annotations, which indicates the hostname to be created. Therefore the switch should be simple. Annotations Tag zalando.org/dnsname Description Hostname to be registered Default Empty(falls back to template based approach) Example foo.example.org Required false route53-kubernetes It should be safe to run both route53-kubernetes and external-dns simultaneously. Since route53-kubernetes only looks at services with the label dns=route53 and does not support ingress there should be no collisions between annotations. If users desire to switch to external-dns they can run both controllers and migrate services over as they are able. Ownership \u00b6 External DNS should be responsible for the created records. Which means that the records should be tagged and only tagged records are viable for future deletion/update. It should not mess with pre-existing records created via other means. Ownership via TXT records \u00b6 Each record managed by External DNS is accompanied with a TXT record with a specific value to indicate that corresponding DNS record is managed by External DNS and it can be updated/deleted respectively. TXT records are limited to lifetimes of service/ingress objects and are created/deleted once k8s resources are created/deleted.","title":"Initial Design"},{"location":"initial-design/#proposal-design-of-external-dns","text":"","title":"Proposal: Design of External DNS"},{"location":"initial-design/#background","text":"Project proposal Initial discussion This document describes the initial design proposal. External DNS is purposed to fill the existing gap of creating DNS records for Kubernetes resources. While there exist alternative solutions, this project is meant to be a standard way of managing DNS records for Kubernetes. The current project is a fusion of the following projects and driven by its maintainers: Kops DNS Controller Mate wearemolecule/route53-kubernetes","title":"Background"},{"location":"initial-design/#example-use-case","text":"User runs kubectl create -f ingress.yaml , this will create an ingress as normal. Typically the user would then have to manually create a DNS record pointing the ingress endpoint If the external-dns controller is running on the cluster, it could automatically configure the DNS records instead, by observing the host attribute in the ingress object.","title":"Example use case:"},{"location":"initial-design/#goals","text":"Support AWS Route53 and Google Cloud DNS providers DNS for Kubernetes services(type=Loadbalancer) and Ingress Create/update/remove records as according to Kubernetes resources state It should address main requirements and support main features of the projects mentioned above","title":"Goals"},{"location":"initial-design/#design","text":"","title":"Design"},{"location":"initial-design/#extensibility","text":"New cloud providers should be easily pluggable. Initially only AWS/Google platforms are supported. However, in the future we are planning to incorporate CoreDNS and Azure DNS as possible DNS providers","title":"Extensibility"},{"location":"initial-design/#configuration","text":"DNS records will be automatically created in multiple situations: 1. Setting spec.rules.host on an ingress object. 2. Setting spec.tls.hosts on an ingress object. 3. Adding the annotation external-dns.alpha.kubernetes.io/hostname on an ingress object. 4. Adding the annotation external-dns.alpha.kubernetes.io/hostname on a type=LoadBalancer service object.","title":"Configuration"},{"location":"initial-design/#annotations","text":"Record configuration should occur via resource annotations. Supported annotations: Annotations Tag external-dns.alpha.kubernetes.io/controller Description Tells a DNS controller to process this service. This is useful when running different DNS controllers at the same time (or different versions of the same controller). The v1 implementation of dns-controller would look for service annotations dns-controller and dns-controller/v1 but not for mate/v1 or dns-controller/v2 Default dns-controller Example dns-controller/v1 Required false \u2014 \u2014 Tag external-dns.alpha.kubernetes.io/hostname Description Fully qualified name of the desired record Default none Example foo.example.org Required Only for services. Ingress hostname is retrieved from spec.rules.host meta data on ingress","title":"Annotations"},{"location":"initial-design/#compatibility","text":"External DNS should be compatible with annotations used by three above mentioned projects. The idea is that resources created and tagged with annotations for other projects should continue to be valid and now managed by External DNS. Mate Mate does not require services/ingress to be tagged. Therefore, it is not safe to run both Mate and External-DNS simultaneously. The idea is that initial release (?) of External DNS will support Mate annotations, which indicates the hostname to be created. Therefore the switch should be simple. Annotations Tag zalando.org/dnsname Description Hostname to be registered Default Empty(falls back to template based approach) Example foo.example.org Required false route53-kubernetes It should be safe to run both route53-kubernetes and external-dns simultaneously. Since route53-kubernetes only looks at services with the label dns=route53 and does not support ingress there should be no collisions between annotations. If users desire to switch to external-dns they can run both controllers and migrate services over as they are able.","title":"Compatibility"},{"location":"initial-design/#ownership","text":"External DNS should be responsible for the created records. Which means that the records should be tagged and only tagged records are viable for future deletion/update. It should not mess with pre-existing records created via other means.","title":"Ownership"},{"location":"initial-design/#ownership-via-txt-records","text":"Each record managed by External DNS is accompanied with a TXT record with a specific value to indicate that corresponding DNS record is managed by External DNS and it can be updated/deleted respectively. TXT records are limited to lifetimes of service/ingress objects and are created/deleted once k8s resources are created/deleted.","title":"Ownership via TXT records"},{"location":"registry/","text":"TXT Registry migration to a new format \u00b6 In order to support more record types and be able to track ownership without TXT record name clash, a new TXT record is introduced. It contains record type it manages, e.g.: * A record foo.example.com will be tracked with classic foo.example.com TXT record * At the same time a new TXT record will be created a-foo.example.com Prefix and suffix are extended with %{record_type} template where the user can control how prefixed/suffixed records should look like. In order to maintain compatibility, both records will be maintained for some time, in order to have downgrade possibility. The controller will try to create the \u201cnew format\u201d TXT records if they are not present to ease the migration from the versions < 0.12.0. Later on, the old format will be dropped and only the new format will be kept ( - ). Cleanup will be done by controller itself.","title":"Registry"},{"location":"registry/#txt-registry-migration-to-a-new-format","text":"In order to support more record types and be able to track ownership without TXT record name clash, a new TXT record is introduced. It contains record type it manages, e.g.: * A record foo.example.com will be tracked with classic foo.example.com TXT record * At the same time a new TXT record will be created a-foo.example.com Prefix and suffix are extended with %{record_type} template where the user can control how prefixed/suffixed records should look like. In order to maintain compatibility, both records will be maintained for some time, in order to have downgrade possibility. The controller will try to create the \u201cnew format\u201d TXT records if they are not present to ease the migration from the versions < 0.12.0. Later on, the old format will be dropped and only the new format will be kept ( - ). Cleanup will be done by controller itself.","title":"TXT Registry migration to a new format"},{"location":"release/","text":"Release \u00b6 Release cycle \u00b6 Currently we don\u2019t release regularly. Whenever we think it makes sense to release a new version we do it, but we aim to do a new release every month. You might want to ask in our Slack channel external-dns when the next release will come out. Versioning convention \u00b6 These are the conventions that we will be using for releases following 0.7.6 : Patch version should be updated if we need to merge bugfixes, e.g. provider a does need a fix in order make updates working again. I would see updating or improving documentation here. Minor version should be updated if new features are implemented in existing providers or new provider get introduced. Major version should be upgraded if we introduce breaking changes. How to release a new image \u00b6 Prerequisite \u00b6 We use https://github.com/cli/cli to automate the release process. Please install it according to the official documentation . You must be an official maintainer of the project to be able to do a release. Steps \u00b6 Run scripts/releaser.sh to create a new GitHub release. Alternatively you can create a release in the GitHub UI making sure to click on the autogenerate release node feature. The step above will trigger the Kubernetes based CI/CD system Prow . Verify that a new image was built and uploaded to gcr.io/k8s-staging-external-dns/external-dns . Create a PR in the k8s.io repo (see https://github.com/kubernetes/k8s.io/pull/540 for reference) by taking the current staging image using the sha256 digest. Once the PR is merged, the image will be live with the corresponding tag specified in the PR. Verify that the image is pullable with the given tag (i.e. v0.7.5 ). Branch out from the default branch and run scripts/kustomize-version-updater.sh to update the image tag used in the kustomization.yaml. Create an issue to release the corresponding Helm chart via the chart release process (below) assigned to a chart maintainer Create a PR with the kustomize change. Once the PR is merged, all is done :-) How to release a new chart version \u00b6 The chart needs to be released in response to an ExternalDNS image release or on an as-needed basis; this should be triggered by an issue to release the chart. Steps \u00b6 Create a PR to update Chart.yaml with the ExternalDNS version in appVersion , agreed on chart release version in version and annotations showing the changes Validate that the chart linting is successful Merge the PR to trigger a GitHub action to release the chart","title":"Release"},{"location":"release/#release","text":"","title":"Release"},{"location":"release/#release-cycle","text":"Currently we don\u2019t release regularly. Whenever we think it makes sense to release a new version we do it, but we aim to do a new release every month. You might want to ask in our Slack channel external-dns when the next release will come out.","title":"Release cycle"},{"location":"release/#versioning-convention","text":"These are the conventions that we will be using for releases following 0.7.6 : Patch version should be updated if we need to merge bugfixes, e.g. provider a does need a fix in order make updates working again. I would see updating or improving documentation here. Minor version should be updated if new features are implemented in existing providers or new provider get introduced. Major version should be upgraded if we introduce breaking changes.","title":"Versioning convention"},{"location":"release/#how-to-release-a-new-image","text":"","title":"How to release a new image"},{"location":"release/#prerequisite","text":"We use https://github.com/cli/cli to automate the release process. Please install it according to the official documentation . You must be an official maintainer of the project to be able to do a release.","title":"Prerequisite"},{"location":"release/#steps","text":"Run scripts/releaser.sh to create a new GitHub release. Alternatively you can create a release in the GitHub UI making sure to click on the autogenerate release node feature. The step above will trigger the Kubernetes based CI/CD system Prow . Verify that a new image was built and uploaded to gcr.io/k8s-staging-external-dns/external-dns . Create a PR in the k8s.io repo (see https://github.com/kubernetes/k8s.io/pull/540 for reference) by taking the current staging image using the sha256 digest. Once the PR is merged, the image will be live with the corresponding tag specified in the PR. Verify that the image is pullable with the given tag (i.e. v0.7.5 ). Branch out from the default branch and run scripts/kustomize-version-updater.sh to update the image tag used in the kustomization.yaml. Create an issue to release the corresponding Helm chart via the chart release process (below) assigned to a chart maintainer Create a PR with the kustomize change. Once the PR is merged, all is done :-)","title":"Steps"},{"location":"release/#how-to-release-a-new-chart-version","text":"The chart needs to be released in response to an ExternalDNS image release or on an as-needed basis; this should be triggered by an issue to release the chart.","title":"How to release a new chart version"},{"location":"release/#steps_1","text":"Create a PR to update Chart.yaml with the ExternalDNS version in appVersion , agreed on chart release version in version and annotations showing the changes Validate that the chart linting is successful Merge the PR to trigger a GitHub action to release the chart","title":"Steps"},{"location":"ttl/","text":"Configure DNS record TTL (Time-To-Live) \u00b6 An optional annotation external-dns.alpha.kubernetes.io/ttl is available to customize the TTL value of a DNS record. TTL is specified as an integer encoded as string representing seconds. To configure it, simply annotate a service/ingress, e.g.: apiVersion : v1 kind : Service metadata : annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com. external-dns.alpha.kubernetes.io/ttl : \"60\" ... TTL can also be specified as a duration value parsable by Golang time.ParseDuration : apiVersion : v1 kind : Service metadata : annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com. external-dns.alpha.kubernetes.io/ttl : \"1m\" ... Both examples result in the same value of 60 seconds TTL. TTL must be a positive value. Providers \u00b6 AWS (Route53) Azure Cloudflare DigitalOcean DNSimple Google InMemory Linode TransIP RFC2136 Vultr UltraDNS PRs welcome! Notes \u00b6 When the external-dns.alpha.kubernetes.io/ttl annotation is not provided, the TTL will default to 0 seconds and endpoint.TTL.isConfigured() will be false. AWS Provider \u00b6 The AWS Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code. Azure \u00b6 TTL value should be between 1 and 2,147,483,647 seconds. By default it will be 300s. CloudFlare Provider \u00b6 CloudFlare overrides the value to \u201cauto\u201d when the TTL is 0. DigitalOcean Provider \u00b6 The DigitalOcean Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code. DNSimple Provider \u00b6 The DNSimple Provider default TTL is used when the TTL is 0. The default TTL is 3600s. Google Provider \u00b6 Previously with the Google Provider, TTL\u2019s were hard-coded to 300s. For safety, the Google Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code. For the moment, it is impossible to use a TTL value of 0 with the AWS, DigitalOcean, or Google Providers. This behavior may change in the future. Linode Provider \u00b6 The Linode Provider default TTL is used when the TTL is 0. The default is 24 hours TransIP Provider \u00b6 The TransIP Provider minimal TTL is used when the TTL is 0. The minimal TTL is 60s. Vultr Provider \u00b6 The Vultr provider minimal TTL is used when the TTL is 0. The default is 1 hour. UltraDNS \u00b6 The UltraDNS provider minimal TTL is used when the TTL is not provided. The default TTL is account level default TTL, if defined, otherwise 24 hours.","title":"TTL"},{"location":"ttl/#configure-dns-record-ttl-time-to-live","text":"An optional annotation external-dns.alpha.kubernetes.io/ttl is available to customize the TTL value of a DNS record. TTL is specified as an integer encoded as string representing seconds. To configure it, simply annotate a service/ingress, e.g.: apiVersion : v1 kind : Service metadata : annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com. external-dns.alpha.kubernetes.io/ttl : \"60\" ... TTL can also be specified as a duration value parsable by Golang time.ParseDuration : apiVersion : v1 kind : Service metadata : annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com. external-dns.alpha.kubernetes.io/ttl : \"1m\" ... Both examples result in the same value of 60 seconds TTL. TTL must be a positive value.","title":"Configure DNS record TTL (Time-To-Live)"},{"location":"ttl/#providers","text":"AWS (Route53) Azure Cloudflare DigitalOcean DNSimple Google InMemory Linode TransIP RFC2136 Vultr UltraDNS PRs welcome!","title":"Providers"},{"location":"ttl/#notes","text":"When the external-dns.alpha.kubernetes.io/ttl annotation is not provided, the TTL will default to 0 seconds and endpoint.TTL.isConfigured() will be false.","title":"Notes"},{"location":"ttl/#aws-provider","text":"The AWS Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code.","title":"AWS Provider"},{"location":"ttl/#azure","text":"TTL value should be between 1 and 2,147,483,647 seconds. By default it will be 300s.","title":"Azure"},{"location":"ttl/#cloudflare-provider","text":"CloudFlare overrides the value to \u201cauto\u201d when the TTL is 0.","title":"CloudFlare Provider"},{"location":"ttl/#digitalocean-provider","text":"The DigitalOcean Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code.","title":"DigitalOcean Provider"},{"location":"ttl/#dnsimple-provider","text":"The DNSimple Provider default TTL is used when the TTL is 0. The default TTL is 3600s.","title":"DNSimple Provider"},{"location":"ttl/#google-provider","text":"Previously with the Google Provider, TTL\u2019s were hard-coded to 300s. For safety, the Google Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code. For the moment, it is impossible to use a TTL value of 0 with the AWS, DigitalOcean, or Google Providers. This behavior may change in the future.","title":"Google Provider"},{"location":"ttl/#linode-provider","text":"The Linode Provider default TTL is used when the TTL is 0. The default is 24 hours","title":"Linode Provider"},{"location":"ttl/#transip-provider","text":"The TransIP Provider minimal TTL is used when the TTL is 0. The minimal TTL is 60s.","title":"TransIP Provider"},{"location":"ttl/#vultr-provider","text":"The Vultr provider minimal TTL is used when the TTL is 0. The default is 1 hour.","title":"Vultr Provider"},{"location":"ttl/#ultradns","text":"The UltraDNS provider minimal TTL is used when the TTL is not provided. The default TTL is account level default TTL, if defined, otherwise 24 hours.","title":"UltraDNS"},{"location":"contributing/chart/","text":"Helm Chart \u00b6 Chart Changes \u00b6 When contributing chart changes please follow the same process as when contributing other content but also please DON\u2019T modify Chart.yaml in the PR as this would result in a chart release when merged and will mean that your PR will need modifying before it can be accepted. The chart version will be updated as part of the PR to release the chart. Please DO add your changes to the CHANGELOG.md file in the chart directory under the ## [UNRELEASED] section, if there isn\u2019t an uncommented ## [UNRELEASED] section please copy the commented out template and use that.","title":"Helm Chart"},{"location":"contributing/chart/#helm-chart","text":"","title":"Helm Chart"},{"location":"contributing/chart/#chart-changes","text":"When contributing chart changes please follow the same process as when contributing other content but also please DON\u2019T modify Chart.yaml in the PR as this would result in a chart release when merged and will mean that your PR will need modifying before it can be accepted. The chart version will be updated as part of the PR to release the chart. Please DO add your changes to the CHANGELOG.md file in the chart directory under the ## [UNRELEASED] section, if there isn\u2019t an uncommented ## [UNRELEASED] section please copy the commented out template and use that.","title":"Chart Changes"},{"location":"contributing/crd-source/","text":"CRD Source \u00b6 CRD source provides a generic mechanism to manage DNS records in your favourite DNS provider supported by external-dns. Details \u00b6 CRD source watches for a user specified CRD to extract Endpoints from its Spec . So users need to create such a CRD and register it to the kubernetes cluster and then create new object(s) of the CRD specifying the Endpoints. Registering CRD \u00b6 Here is typical example of CRD API type which provides Endpoints to CRD source : type TTL int64 type Targets [] string type ProviderSpecificProperty struct { Name string Value string } type ProviderSpecific [] ProviderSpecificProperty type Endpoint struct { // The hostname of the DNS record DNSName string `json:\"dnsName,omitempty\"` // The targets the DNS record points to Targets Targets `json:\"targets,omitempty\"` // RecordType type of record, e.g. CNAME, A, SRV, TXT etc RecordType string `json:\"recordType,omitempty\"` // TTL for the record RecordTTL TTL `json:\"recordTTL,omitempty\"` // Labels stores labels defined for the Endpoint // +optional Labels Labels `json:\"labels,omitempty\"` // ProviderSpecific stores provider specific config // +optional ProviderSpecific ProviderSpecific `json:\"providerSpecific,omitempty\"` } type DNSEndpointSpec struct { Endpoints [] * Endpoint `json:\"endpoints,omitempty\"` } type DNSEndpointStatus struct { // The generation observed by the external-dns controller. // +optional ObservedGeneration int64 `json:\"observedGeneration,omitempty\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DNSEndpoint is the CRD wrapper for Endpoint // +k8s:openapi-gen=true // +kubebuilder:resource:path=dnsendpoints // +kubebuilder:subresource:status type DNSEndpoint struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec DNSEndpointSpec `json:\"spec,omitempty\"` Status DNSEndpointStatus `json:\"status,omitempty\"` } Refer to kubebuilder to create and register the CRD. Usage \u00b6 One can use CRD source by specifying --source flag with crd and specifying the ApiVersion and Kind of the CRD with --crd-source-apiversion and crd-source-kind respectively. for e.g: $ build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1 --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run Creating DNS Records \u00b6 Create the objects of CRD type by filling in the fields of CRD and DNS record would be created accordingly. Example \u00b6 Here is an example CRD manifest generated by kubebuilder. Apply this to register the CRD $ kubectl apply --validate=false -f docs/contributing/crd-source/crd-manifest.yaml customresourcedefinition.apiextensions.k8s.io \"dnsendpoints.externaldns.k8s.io\" created Then you can create the dns-endpoint yaml similar to dnsendpoint-example $ kubectl apply -f docs/contributing/crd-source/dnsendpoint-example.yaml dnsendpoint.externaldns.k8s.io \"examplednsrecord\" created Run external-dns in dry-mode to see whether external-dns picks up the DNS record from CRD. $ build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1 --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run INFO[0000] running in dry-run mode. No changes to DNS records will be made. INFO[0000] Connected to cluster at https://192.168.99.100:8443 INFO[0000] CREATE: foo.bar.com 180 IN A 192.168.99.216 INFO[0000] CREATE: foo.bar.com 0 IN TXT \"heritage=external-dns,external-dns/owner=default\" RBAC configuration \u00b6 If you use RBAC, extend the external-dns ClusterRole with: - apiGroups: [\"externaldns.k8s.io\"] resources: [\"dnsendpoints\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"externaldns.k8s.io\"] resources: [\"dnsendpoints/status\"] verbs: [\"*\"]","title":"CRD Source"},{"location":"contributing/crd-source/#crd-source","text":"CRD source provides a generic mechanism to manage DNS records in your favourite DNS provider supported by external-dns.","title":"CRD Source"},{"location":"contributing/crd-source/#details","text":"CRD source watches for a user specified CRD to extract Endpoints from its Spec . So users need to create such a CRD and register it to the kubernetes cluster and then create new object(s) of the CRD specifying the Endpoints.","title":"Details"},{"location":"contributing/crd-source/#registering-crd","text":"Here is typical example of CRD API type which provides Endpoints to CRD source : type TTL int64 type Targets [] string type ProviderSpecificProperty struct { Name string Value string } type ProviderSpecific [] ProviderSpecificProperty type Endpoint struct { // The hostname of the DNS record DNSName string `json:\"dnsName,omitempty\"` // The targets the DNS record points to Targets Targets `json:\"targets,omitempty\"` // RecordType type of record, e.g. CNAME, A, SRV, TXT etc RecordType string `json:\"recordType,omitempty\"` // TTL for the record RecordTTL TTL `json:\"recordTTL,omitempty\"` // Labels stores labels defined for the Endpoint // +optional Labels Labels `json:\"labels,omitempty\"` // ProviderSpecific stores provider specific config // +optional ProviderSpecific ProviderSpecific `json:\"providerSpecific,omitempty\"` } type DNSEndpointSpec struct { Endpoints [] * Endpoint `json:\"endpoints,omitempty\"` } type DNSEndpointStatus struct { // The generation observed by the external-dns controller. // +optional ObservedGeneration int64 `json:\"observedGeneration,omitempty\"` } // +genclient // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DNSEndpoint is the CRD wrapper for Endpoint // +k8s:openapi-gen=true // +kubebuilder:resource:path=dnsendpoints // +kubebuilder:subresource:status type DNSEndpoint struct { metav1 . TypeMeta `json:\",inline\"` metav1 . ObjectMeta `json:\"metadata,omitempty\"` Spec DNSEndpointSpec `json:\"spec,omitempty\"` Status DNSEndpointStatus `json:\"status,omitempty\"` } Refer to kubebuilder to create and register the CRD.","title":"Registering CRD"},{"location":"contributing/crd-source/#usage","text":"One can use CRD source by specifying --source flag with crd and specifying the ApiVersion and Kind of the CRD with --crd-source-apiversion and crd-source-kind respectively. for e.g: $ build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1 --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run","title":"Usage"},{"location":"contributing/crd-source/#creating-dns-records","text":"Create the objects of CRD type by filling in the fields of CRD and DNS record would be created accordingly.","title":"Creating DNS Records"},{"location":"contributing/crd-source/#example","text":"Here is an example CRD manifest generated by kubebuilder. Apply this to register the CRD $ kubectl apply --validate=false -f docs/contributing/crd-source/crd-manifest.yaml customresourcedefinition.apiextensions.k8s.io \"dnsendpoints.externaldns.k8s.io\" created Then you can create the dns-endpoint yaml similar to dnsendpoint-example $ kubectl apply -f docs/contributing/crd-source/dnsendpoint-example.yaml dnsendpoint.externaldns.k8s.io \"examplednsrecord\" created Run external-dns in dry-mode to see whether external-dns picks up the DNS record from CRD. $ build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1 --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run INFO[0000] running in dry-run mode. No changes to DNS records will be made. INFO[0000] Connected to cluster at https://192.168.99.100:8443 INFO[0000] CREATE: foo.bar.com 180 IN A 192.168.99.216 INFO[0000] CREATE: foo.bar.com 0 IN TXT \"heritage=external-dns,external-dns/owner=default\"","title":"Example"},{"location":"contributing/crd-source/#rbac-configuration","text":"If you use RBAC, extend the external-dns ClusterRole with: - apiGroups: [\"externaldns.k8s.io\"] resources: [\"dnsendpoints\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"externaldns.k8s.io\"] resources: [\"dnsendpoints/status\"] verbs: [\"*\"]","title":"RBAC configuration"},{"location":"contributing/getting-started/","text":"Quick Start \u00b6 Git Go 1.19+ Go modules golangci-lint Docker kubectl Compile and run locally against a remote k8s cluster. git clone https://github.com/kubernetes-sigs/external-dns.git && cd external-dns make build # login to remote k8s cluster ./build/external-dns --source = service --provider = inmemory --once Run linting, unit tests, and coverage report. make lint make test make cover-html Build container image. make build.docker Design \u00b6 ExternalDNS\u2019s sources of DNS records live in package source . They implement the Source interface that has a single method Endpoints which returns the represented source\u2019s objects converted to Endpoints . Endpoints are just a tuple of DNS name and target where target can be an IP or another hostname. For example, the ServiceSource returns all Services converted to Endpoints where the hostname is the value of the external-dns.alpha.kubernetes.io/hostname annotation and the target is the IP of the load balancer or where the hostname is the value of the external-dns.alpha.kubernetes.io/internal-hostname annotation and the target is the IP of the service ClusterIP. This list of endpoints is passed to the Plan which determines the difference between the current DNS records and the desired list of Endpoints . Once the difference has been figured out the list of intended changes is passed to a Registry which live in the registry package. The registry is a wrapper and access point to DNS provider. Registry implements the ownership concept by marking owned records and filtering out records not owned by ExternalDNS before passing them to DNS provider. The provider is the adapter to the DNS provider, e.g. Google Cloud DNS. It implements two methods: ApplyChanges to apply a set of changes filtered by Registry and Records to retrieve the current list of records from the DNS provider. The orchestration between the different components is controlled by the controller . You can pick which Source and Provider to use at runtime via the --source and --provider flags, respectively. Adding a DNS Provider \u00b6 A typical way to start on, e.g. a CoreDNS provider, would be to add a coredns.go to the providers package and implement the interface methods. Then you would have to register your provider under a name in main.go , e.g. coredns , and would be able to trigger it\u2019s functions via setting --provider=coredns . Note, how your provider doesn\u2019t need to know anything about where the DNS records come from, nor does it have to figure out the difference between the current and the desired state, it merely executes the actions calculated by the plan. Running GitHub Actions locally \u00b6 You can also extend the CI workflow which is currently implemented as GitHub Action within the workflow folder. In order to test your changes before committing you can leverage act to run the GitHub Action locally. Follow the installation instructions in the nektos/act README.md . Afterwards just run act within the root folder of the project. For further usage of act refer to its documentation.","title":"Quick Start"},{"location":"contributing/getting-started/#quick-start","text":"Git Go 1.19+ Go modules golangci-lint Docker kubectl Compile and run locally against a remote k8s cluster. git clone https://github.com/kubernetes-sigs/external-dns.git && cd external-dns make build # login to remote k8s cluster ./build/external-dns --source = service --provider = inmemory --once Run linting, unit tests, and coverage report. make lint make test make cover-html Build container image. make build.docker","title":"Quick Start"},{"location":"contributing/getting-started/#design","text":"ExternalDNS\u2019s sources of DNS records live in package source . They implement the Source interface that has a single method Endpoints which returns the represented source\u2019s objects converted to Endpoints . Endpoints are just a tuple of DNS name and target where target can be an IP or another hostname. For example, the ServiceSource returns all Services converted to Endpoints where the hostname is the value of the external-dns.alpha.kubernetes.io/hostname annotation and the target is the IP of the load balancer or where the hostname is the value of the external-dns.alpha.kubernetes.io/internal-hostname annotation and the target is the IP of the service ClusterIP. This list of endpoints is passed to the Plan which determines the difference between the current DNS records and the desired list of Endpoints . Once the difference has been figured out the list of intended changes is passed to a Registry which live in the registry package. The registry is a wrapper and access point to DNS provider. Registry implements the ownership concept by marking owned records and filtering out records not owned by ExternalDNS before passing them to DNS provider. The provider is the adapter to the DNS provider, e.g. Google Cloud DNS. It implements two methods: ApplyChanges to apply a set of changes filtered by Registry and Records to retrieve the current list of records from the DNS provider. The orchestration between the different components is controlled by the controller . You can pick which Source and Provider to use at runtime via the --source and --provider flags, respectively.","title":"Design"},{"location":"contributing/getting-started/#adding-a-dns-provider","text":"A typical way to start on, e.g. a CoreDNS provider, would be to add a coredns.go to the providers package and implement the interface methods. Then you would have to register your provider under a name in main.go , e.g. coredns , and would be able to trigger it\u2019s functions via setting --provider=coredns . Note, how your provider doesn\u2019t need to know anything about where the DNS records come from, nor does it have to figure out the difference between the current and the desired state, it merely executes the actions calculated by the plan.","title":"Adding a DNS Provider"},{"location":"contributing/getting-started/#running-github-actions-locally","text":"You can also extend the CI workflow which is currently implemented as GitHub Action within the workflow folder. In order to test your changes before committing you can leverage act to run the GitHub Action locally. Follow the installation instructions in the nektos/act README.md . Afterwards just run act within the root folder of the project. For further usage of act refer to its documentation.","title":"Running GitHub Actions locally"},{"location":"contributing/sources-and-providers/","text":"Sources and Providers \u00b6 ExternalDNS supports swapping out endpoint sources and DNS providers and both sides are pluggable. There currently exist three sources and four provider implementations. Sources \u00b6 Sources are an abstraction over any kind of source of desired Endpoints, e.g.: * a list of Service objects from Kubernetes * a random list for testing purposes * an aggregated list of multiple nested sources The Source interface has a single method called Endpoints that should return all desired Endpoint objects as a flat list. type Source interface { Endpoints () ([] * endpoint . Endpoint , error ) } All sources live in package source . ServiceSource : collects all Services that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to an annotation set on the Service or is compiled from the Service attributes via the FQDN Go template string. IngressSource : collects all Ingresses that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to the host rules defined in the Ingress object. IstioGatewaySource : collects all Istio Gateways and returns them as Endpoint objects. The desired DNS name corresponds to the hosts listed within the servers spec of each Gateway object. ContourIngressRouteSource : collects all Contour IngressRoutes and returns them as Endpoint objects. The desired DNS name corresponds to the virtualhost.fqdn listed within the spec of each IngressRoute object. FakeSource : returns a random list of Endpoints for the purpose of testing providers without having access to a Kubernetes cluster. ConnectorSource : returns a list of Endpoint objects which are served by a tcp server configured through connector-source-server flag. CRDSource : returns a list of Endpoint objects sourced from the spec of CRD objects. For more details refer to CRD source documentation. EmptySource : returns an empty list of Endpoint objects for the purpose of testing and cleaning out entries. Providers \u00b6 Providers are an abstraction over any kind of sink for desired Endpoints, e.g.: * storing them in Google Cloud DNS * printing them to stdout for testing purposes * fanning out to multiple nested providers The Provider interface has two methods: Records and ApplyChanges . Records should return all currently existing DNS records converted to Endpoint objects as a flat list. Upon receiving a change set (via an object of plan.Changes ), ApplyChanges should translate these to the provider specific actions in order to persist them in the provider\u2019s storage. type Provider interface { Records () ([] * endpoint . Endpoint , error ) ApplyChanges ( changes * plan . Changes ) error } The interface tries to be generic and assumes a flat list of records for both functions. However, many providers scope records into zones. Therefore, the provider implementation has to do some extra work to return that flat list. For instance, the AWS provider fetches the list of all hosted zones before it can return or apply the list of records. If the provider has no concept of zones or if it makes sense to cache the list of hosted zones it is happily allowed to do so. Furthermore, the provider should respect the --domain-filter flag to limit the affected records by a domain suffix. For instance, the AWS provider filters out all hosted zones that doesn\u2019t match that domain filter. All providers live in package provider . GoogleProvider : returns and creates DNS records in Google Cloud DNS AWSProvider : returns and creates DNS records in AWS Route 53 AzureProvider : returns and creates DNS records in Azure DNS InMemoryProvider : Keeps a list of records in local memory Usage \u00b6 You can choose any combination of sources and providers on the command line. Given a cluster on AWS you would most likely want to use the Service and Ingress Source in combination with the AWS provider. Service + InMemory is useful for testing your service collecting functionality, whereas Fake + Google is useful for testing that the Google provider behaves correctly, etc.","title":"Sources and Providers"},{"location":"contributing/sources-and-providers/#sources-and-providers","text":"ExternalDNS supports swapping out endpoint sources and DNS providers and both sides are pluggable. There currently exist three sources and four provider implementations.","title":"Sources and Providers"},{"location":"contributing/sources-and-providers/#sources","text":"Sources are an abstraction over any kind of source of desired Endpoints, e.g.: * a list of Service objects from Kubernetes * a random list for testing purposes * an aggregated list of multiple nested sources The Source interface has a single method called Endpoints that should return all desired Endpoint objects as a flat list. type Source interface { Endpoints () ([] * endpoint . Endpoint , error ) } All sources live in package source . ServiceSource : collects all Services that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to an annotation set on the Service or is compiled from the Service attributes via the FQDN Go template string. IngressSource : collects all Ingresses that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to the host rules defined in the Ingress object. IstioGatewaySource : collects all Istio Gateways and returns them as Endpoint objects. The desired DNS name corresponds to the hosts listed within the servers spec of each Gateway object. ContourIngressRouteSource : collects all Contour IngressRoutes and returns them as Endpoint objects. The desired DNS name corresponds to the virtualhost.fqdn listed within the spec of each IngressRoute object. FakeSource : returns a random list of Endpoints for the purpose of testing providers without having access to a Kubernetes cluster. ConnectorSource : returns a list of Endpoint objects which are served by a tcp server configured through connector-source-server flag. CRDSource : returns a list of Endpoint objects sourced from the spec of CRD objects. For more details refer to CRD source documentation. EmptySource : returns an empty list of Endpoint objects for the purpose of testing and cleaning out entries.","title":"Sources"},{"location":"contributing/sources-and-providers/#providers","text":"Providers are an abstraction over any kind of sink for desired Endpoints, e.g.: * storing them in Google Cloud DNS * printing them to stdout for testing purposes * fanning out to multiple nested providers The Provider interface has two methods: Records and ApplyChanges . Records should return all currently existing DNS records converted to Endpoint objects as a flat list. Upon receiving a change set (via an object of plan.Changes ), ApplyChanges should translate these to the provider specific actions in order to persist them in the provider\u2019s storage. type Provider interface { Records () ([] * endpoint . Endpoint , error ) ApplyChanges ( changes * plan . Changes ) error } The interface tries to be generic and assumes a flat list of records for both functions. However, many providers scope records into zones. Therefore, the provider implementation has to do some extra work to return that flat list. For instance, the AWS provider fetches the list of all hosted zones before it can return or apply the list of records. If the provider has no concept of zones or if it makes sense to cache the list of hosted zones it is happily allowed to do so. Furthermore, the provider should respect the --domain-filter flag to limit the affected records by a domain suffix. For instance, the AWS provider filters out all hosted zones that doesn\u2019t match that domain filter. All providers live in package provider . GoogleProvider : returns and creates DNS records in Google Cloud DNS AWSProvider : returns and creates DNS records in AWS Route 53 AzureProvider : returns and creates DNS records in Azure DNS InMemoryProvider : Keeps a list of records in local memory","title":"Providers"},{"location":"contributing/sources-and-providers/#usage","text":"You can choose any combination of sources and providers on the command line. Given a cluster on AWS you would most likely want to use the Service and Ingress Source in combination with the AWS provider. Service + InMemory is useful for testing your service collecting functionality, whereas Fake + Google is useful for testing that the Google provider behaves correctly, etc.","title":"Usage"},{"location":"proposal/multi-target/","text":"Multiple Targets per hostname \u00b6 (November 2017) Purpose \u00b6 One should be able to define multiple targets (IPs/Hostnames) in the same Kubernetes resource object and expect ExternalDNS create DNS record(s) with a specified hostname and all targets. So far the connection between k8s resources (ingress/services) and DNS records were not streamlined. This proposal aims to make the connection explicit, making k8s resources acquire or release certain DNS names. As long as the resource ingress/service owns the record it can have multiple targets enable iff they are specified in the same resource. Use cases \u00b6 See https://github.com/kubernetes-sigs/external-dns/issues/239 Current behaviour \u00b6 (as of the moment of writing) Central piece of enabling multi-target is having consistent and correct behaviour in plan component in regards to how endpoints generated from kubernetes resources are mapped to dns records. Current implementation of the plan has inconsistent behaviour in the following scenarios, all of which must be resolved before multi-target support can be enabled in the provider implementations: No records registered so far. Two different ingresses request same hostname but different targets, e.g. Ingress A: example.com -> 1.1.1.1 and Ingress B: example.com -> 2.2.2.2 Current Behaviour : both are added to the \u201cCreate\u201d (records to be created) list and passed to Provider Expected Behaviour : only one (random/ or according to predefined strategy) should be chosen and passed to Provider NOTE : while this seems to go against multi-target support, this is done so no other resource can \u201chijack\u201d already created DNS record. Multi targets are supported only on per single resource basis Now let\u2019s say Ingress A was chosen and successfully created, but both ingress A and B are still there. So on next iteration ExternalDNS would see both again in the Desired list. Current Behaviour : DNS record target will change to that of Ingress B. Expected Behaviour : Ingress A should stay unchanged. Ingress B record is not created DNS record for Ingress A was created but its target has changed. Ingress B is still there Current Behaviour : Undetermined behaviour based on which ingress will be parsed last. Expected Behaviour : DNS record should point to the new target specified in A. Ingress B should still be ignored. NOTE : both 2. and 3. can be resolved if External DNS is aware which resource has already acquired DNS record Ingress C has multiple targets: 1.1.1.1 and 2.2.2.2 Current Behaviour : Both targets are split into different endpoints and we end up in one of the cases above Expected Behaviour : Endpoint should contain list of targets and treated as one ingress object. Requirements and assumptions \u00b6 For this feature to work we have to make sure that: DNS records are now owned by certain ingress/service resources. For External DNS it would mean that TXT records now should store back-reference for the resource this record was created for, i.e. \"heritage=external-dns,external-dns/resource=ingress/default/my-ingress-object-name\" DNS records are updated only: If owning resource target list has changed If owning resource record is not found in the desired list (meaning it was deleted), therefore it will now be owned by another record. So its target list will be updated Changes related to other record properties (e.g. TTL) All of the issues described in Current Behaviour sections are resolved Once Create/Update/Delete lists are calculated correctly (this is where conflicts based on requested DNS names are resolved) they are passed to provider , where provider specific implementation will decide how to convert the structures into required formats. If DNS provider does not (or partially) support multi targets then it is up to the provider to make sure that the change list of records passed to the DNS provider API is valid. TODO : explain best strategy. Additionally see https://github.com/kubernetes-sigs/external-dns/issues/258 Implementation plan \u00b6 Brief summary of open PRs and what they are trying to address: PRs \u00b6 https://github.com/kubernetes-sigs/external-dns/pull/243 - first attempt to add support for multiple targets. It is lagging far behind from tip what it does : unfinished attempt to extend Endpoint struct, for it to allow multiple targets (essentially target string -> targets []string ) action : evaluate if rebasing makes sense, or we can just close it. https://github.com/kubernetes-sigs/external-dns/pull/261 - attempt to rework plan to make it work correctly with multiple targets. what it does : attempts to fix issues with plan described in Current Behaviour section above. Included tests reveal the current problem with plan action : rebase on default branch and make necessary changes to satisfy requirements listed in this document including back-reference to owning record https://github.com/kubernetes-sigs/external-dns/pull/326 - attempt to add multiple target support. what it does : for each pair DNS Name + Record Type it aggregates all targets from the cluster and passes them to Provider. It adds basic support for DO, Azure, Cloudflare, AWS, GCP, however those are not tested (?). (DNSSimple and Infoblox providers were not updated) action : the plan logic will probably needs to be reworked, however the rest concerning support in Providers and extending Endpoint struct can be reused. Rebase on default branch and add missing pieces. Depends on 2 . Related PRs: https://github.com/kubernetes-sigs/external-dns/pull/331/files, https://github.com/kubernetes-sigs/external-dns/pull/347/files - aiming at AWS Route53 weighted records. These PRs should be considered after common agreement about the way to address multi-target support is achieved. Related discussion: https://github.com/kubernetes-sigs/external-dns/issues/196 How to proceed from here \u00b6 The following steps are needed: 1. Make sure consensus regarding the approach is achieved via collaboration on the current document 2. Notify all PR (see above) authors about the agreed approach 3. Implementation: a. `Plan` is working as expected - either based on #261 above or from scratch. `Plan` should be working correctly regardless of multi-target support b. Extensive testing making sure new `plan` does not introduce any breaking changes c. Change Endpoint struct to support multiple targets - based on #326 - integrate it with new `plan` @sethpollack d. Make sure new endpoint format can still be used in providers which have only partial support for multi targets ~~**TODO**: how ?~~ . This is to be done by simply using first target in the targets list. e. Add support for multi target which are already addressed in #326. It goes alongside c. and can be based on the same PR @sethpollack. New providers added since then should maintain same functionality. Extensive testing on all providers before making new release Update all related documentation and explain how multi targets are supported on per provider basis Think of introducing weighted records (see PRs section above) and making them configurable. Open questions \u00b6 Handling cases when ingress/service targets include both hostnames and IPs - postpone this until use cases occurs \u201cWeighted records scope\u201d: https://github.com/kubernetes-sigs/external-dns/issues/196 - this should be considered once multi-target support is implemented","title":"Multiple Targets per hostname"},{"location":"proposal/multi-target/#multiple-targets-per-hostname","text":"(November 2017)","title":"Multiple Targets per hostname"},{"location":"proposal/multi-target/#purpose","text":"One should be able to define multiple targets (IPs/Hostnames) in the same Kubernetes resource object and expect ExternalDNS create DNS record(s) with a specified hostname and all targets. So far the connection between k8s resources (ingress/services) and DNS records were not streamlined. This proposal aims to make the connection explicit, making k8s resources acquire or release certain DNS names. As long as the resource ingress/service owns the record it can have multiple targets enable iff they are specified in the same resource.","title":"Purpose"},{"location":"proposal/multi-target/#use-cases","text":"See https://github.com/kubernetes-sigs/external-dns/issues/239","title":"Use cases"},{"location":"proposal/multi-target/#current-behaviour","text":"(as of the moment of writing) Central piece of enabling multi-target is having consistent and correct behaviour in plan component in regards to how endpoints generated from kubernetes resources are mapped to dns records. Current implementation of the plan has inconsistent behaviour in the following scenarios, all of which must be resolved before multi-target support can be enabled in the provider implementations: No records registered so far. Two different ingresses request same hostname but different targets, e.g. Ingress A: example.com -> 1.1.1.1 and Ingress B: example.com -> 2.2.2.2 Current Behaviour : both are added to the \u201cCreate\u201d (records to be created) list and passed to Provider Expected Behaviour : only one (random/ or according to predefined strategy) should be chosen and passed to Provider NOTE : while this seems to go against multi-target support, this is done so no other resource can \u201chijack\u201d already created DNS record. Multi targets are supported only on per single resource basis Now let\u2019s say Ingress A was chosen and successfully created, but both ingress A and B are still there. So on next iteration ExternalDNS would see both again in the Desired list. Current Behaviour : DNS record target will change to that of Ingress B. Expected Behaviour : Ingress A should stay unchanged. Ingress B record is not created DNS record for Ingress A was created but its target has changed. Ingress B is still there Current Behaviour : Undetermined behaviour based on which ingress will be parsed last. Expected Behaviour : DNS record should point to the new target specified in A. Ingress B should still be ignored. NOTE : both 2. and 3. can be resolved if External DNS is aware which resource has already acquired DNS record Ingress C has multiple targets: 1.1.1.1 and 2.2.2.2 Current Behaviour : Both targets are split into different endpoints and we end up in one of the cases above Expected Behaviour : Endpoint should contain list of targets and treated as one ingress object.","title":"Current behaviour"},{"location":"proposal/multi-target/#requirements-and-assumptions","text":"For this feature to work we have to make sure that: DNS records are now owned by certain ingress/service resources. For External DNS it would mean that TXT records now should store back-reference for the resource this record was created for, i.e. \"heritage=external-dns,external-dns/resource=ingress/default/my-ingress-object-name\" DNS records are updated only: If owning resource target list has changed If owning resource record is not found in the desired list (meaning it was deleted), therefore it will now be owned by another record. So its target list will be updated Changes related to other record properties (e.g. TTL) All of the issues described in Current Behaviour sections are resolved Once Create/Update/Delete lists are calculated correctly (this is where conflicts based on requested DNS names are resolved) they are passed to provider , where provider specific implementation will decide how to convert the structures into required formats. If DNS provider does not (or partially) support multi targets then it is up to the provider to make sure that the change list of records passed to the DNS provider API is valid. TODO : explain best strategy. Additionally see https://github.com/kubernetes-sigs/external-dns/issues/258","title":"Requirements and assumptions"},{"location":"proposal/multi-target/#implementation-plan","text":"Brief summary of open PRs and what they are trying to address:","title":"Implementation plan"},{"location":"proposal/multi-target/#prs","text":"https://github.com/kubernetes-sigs/external-dns/pull/243 - first attempt to add support for multiple targets. It is lagging far behind from tip what it does : unfinished attempt to extend Endpoint struct, for it to allow multiple targets (essentially target string -> targets []string ) action : evaluate if rebasing makes sense, or we can just close it. https://github.com/kubernetes-sigs/external-dns/pull/261 - attempt to rework plan to make it work correctly with multiple targets. what it does : attempts to fix issues with plan described in Current Behaviour section above. Included tests reveal the current problem with plan action : rebase on default branch and make necessary changes to satisfy requirements listed in this document including back-reference to owning record https://github.com/kubernetes-sigs/external-dns/pull/326 - attempt to add multiple target support. what it does : for each pair DNS Name + Record Type it aggregates all targets from the cluster and passes them to Provider. It adds basic support for DO, Azure, Cloudflare, AWS, GCP, however those are not tested (?). (DNSSimple and Infoblox providers were not updated) action : the plan logic will probably needs to be reworked, however the rest concerning support in Providers and extending Endpoint struct can be reused. Rebase on default branch and add missing pieces. Depends on 2 . Related PRs: https://github.com/kubernetes-sigs/external-dns/pull/331/files, https://github.com/kubernetes-sigs/external-dns/pull/347/files - aiming at AWS Route53 weighted records. These PRs should be considered after common agreement about the way to address multi-target support is achieved. Related discussion: https://github.com/kubernetes-sigs/external-dns/issues/196","title":"PRs"},{"location":"proposal/multi-target/#how-to-proceed-from-here","text":"The following steps are needed: 1. Make sure consensus regarding the approach is achieved via collaboration on the current document 2. Notify all PR (see above) authors about the agreed approach 3. Implementation: a. `Plan` is working as expected - either based on #261 above or from scratch. `Plan` should be working correctly regardless of multi-target support b. Extensive testing making sure new `plan` does not introduce any breaking changes c. Change Endpoint struct to support multiple targets - based on #326 - integrate it with new `plan` @sethpollack d. Make sure new endpoint format can still be used in providers which have only partial support for multi targets ~~**TODO**: how ?~~ . This is to be done by simply using first target in the targets list. e. Add support for multi target which are already addressed in #326. It goes alongside c. and can be based on the same PR @sethpollack. New providers added since then should maintain same functionality. Extensive testing on all providers before making new release Update all related documentation and explain how multi targets are supported on per provider basis Think of introducing weighted records (see PRs section above) and making them configurable.","title":"How to proceed from here"},{"location":"proposal/multi-target/#open-questions","text":"Handling cases when ingress/service targets include both hostnames and IPs - postpone this until use cases occurs \u201cWeighted records scope\u201d: https://github.com/kubernetes-sigs/external-dns/issues/196 - this should be considered once multi-target support is implemented","title":"Open questions"},{"location":"proposal/registry/","text":"Registry \u00b6 [Old name: storage] \u00b6 Initial discussion - https://github.com/kubernetes-sigs/external-dns/issues/44 Purpose \u00b6 One should not be afraid to use external-dns, because it can delete or overwrite the records preexisting in the DNS provider. Why we need it? DNS provider (AWS Route53, Google DNS, etc.) stores dns records which are created via various means. Integration of External-DNS should be safe and should not delete or overwrite the records which it is not responsible for. Moreover, it should certainly be possible for multiple kubernetes clusters to share the same hosted zone within the dns provider, additionally multiple external-dns instances inside the same cluster should be able to co-exist without messing with the same set of records. Registry provides a mechanism to ensure the safe management of DNS records This proposal introduces multiple possible implementation with the details depending on the setup. Requirements and assumptions \u00b6 Pre-existing records should not be modified by external-dns External-dns instance only creates/modifies/deletes records which are created by this instance It should be possible to transfer the ownership to another external-dns instance Any integrated DNS provider should provide at least a single way to implement the registry Noop registry can be used to disable ownership Lifetime of the records should not be limited to lifetime of external-dns External-dns should have its identifier for marking the managed records - owner-id Types of registry \u00b6 The following presents two ways to implement the registry, and we are planning to implement both for compatibility purposes. TXT records \u00b6 This implementation idea is borrowed from Mate Each record created by external-dns is accompanied by the TXT record, which internally stores the external-dns identifier. For example, if external dns with owner-id=\"external-dns-1\" record to be created with dns name foo.zone.org , external-dns will create a TXT record with the same dns name <record_type>-foo.zone.org and injected value of \"external-dns-1\" . The transfer of ownership can be done by modifying the value of the TXT record. If no TXT record exists for the record or the value does not match its own owner-id , then external-dns will simply ignore it. Goods \u00b6 Easy to guarantee cross-cluster ownership safety Data lifetime is not limited to cluster or external-dns lifetime Supported by major DNS providers TXT record are created alongside other records in a batch request. Hence eliminating possibility of having inconsistent ownership information and dns provider state Bads \u00b6 TXT record cannot co-exist with CNAME records (this can be solved by creating a TXT record with another domain name, e.g. foo.org->foo.txt.org ) Introduces complexity to the logic Difficult to do the transfer of ownership Too easy to mess up with manual modifications ConfigMap \u00b6 This implementation cannot be considered 100% error free , hence use with caution [see Possible failure scenario below] Store the state in the configmap. ConfigMap is created and managed by each external-dns individually, i.e. external-dns with owner-id=external-dns-1 will create and operate on extern-dns-1-registry ConfigMap. ConfigMap will store all the records present in the DNS provider as serialized JSON. For example: kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-03-09T19:14:38Z name: external-dns-1-storage namespace: same-as-external-dns-1 data: records: \"[{ \\\"dnsname\\\": \\\"foo.org\\\", \\\"owner\\\": \\\"external-dns-1\\\", \\\"target\\\": \\\"loadbalancer1.com\\\", \\\"type\\\": \\\"A\\\" }, { \\\"dnsname\\\": \\\"bar.org\\\", \\\"owner\\\": \\\"external-dns-2\\\", \\\"target\\\": \\\"loadbalancer2.com\\\", \\\"type\\\": \\\"A\\\" }, { \\\"dnsname\\\": \\\"unmanaged.org\\\", \\\"owner\\\": \\\"\\\", \\\"target\\\": \\\"loadbalancer2.com\\\", \\\"type\\\": \\\"CNAME\\\" }]\" ConfigMap will be periodically resynced with the dns provider by fetching the dns records and comparing it with the data currently stored and hence rebuilding the ownership information. Goods \u00b6 Not difficult to implement and easy to do the ownership transfer ConfigMap is a first class citizen in kubernetes world Does not create dependency/restriction on DNS provider Cannot be easily messed with by other parties Bads \u00b6 ConfigMap might be out of sync with dns provider state LifeTime is obviously limited to the cluster lifetime Not supported in older kubernetes clusters Bloated ConfigMap - cannot be paginated and will grow very big on DNS provider with thousands of records Failure scenario \u00b6 It is possible that the configmap will go out of sync with the dns provider state. In the implementation flow external-dns will first modify records on dns provider side to subsequently update configmap. And if ExternalDNS will crash in-between two operation created records will be left unmanaged and not viable for update/deletion by External DNS. Component integration \u00b6 Components: * Source - all endpoints ( collection of ingress, service[type=LoadBalancer] etc.) * Plan - object responsible for the create of change lists in external-dns * Provider - interface to access the DNS provider API Registry will serve as wrapper around Provider providing additional information regarding endpoint ownership. Ownership will further taken into account by Plan to filter out records to include only records managed by current ExternalDNS instance (having same owner-id value) A single loop iteration of external-dns operation: Get all endpoints ( collection ingress, service[type=LoadBalancer] etc.) into collection of endpoints Get registry Records() (makes the call to DNSProvider and also build ownership information) Pass Records (including ownership information) and list of endpoints to Plan to do the calculation Call registry ApplyChanges() method (which subsequently calls DNS Provider Apply method to update records) If ConfigMap implementation of Registry is used, then ConfigMap needs to be updated separately In case of configmap, Registry gets updated all the time via Poll . Plan does not call DNS provider directly. Good value of the Poll is to have simple rate limiting mechanism on DNS provider. Notes: \u00b6 DNS Provider should use batch operations DNS Provider should be called with CREATE operation (not UPSERT!) when the record does not yet exist!","title":"Registry"},{"location":"proposal/registry/#registry","text":"","title":"Registry"},{"location":"proposal/registry/#old-name-storage","text":"Initial discussion - https://github.com/kubernetes-sigs/external-dns/issues/44","title":"[Old name: storage]"},{"location":"proposal/registry/#purpose","text":"One should not be afraid to use external-dns, because it can delete or overwrite the records preexisting in the DNS provider. Why we need it? DNS provider (AWS Route53, Google DNS, etc.) stores dns records which are created via various means. Integration of External-DNS should be safe and should not delete or overwrite the records which it is not responsible for. Moreover, it should certainly be possible for multiple kubernetes clusters to share the same hosted zone within the dns provider, additionally multiple external-dns instances inside the same cluster should be able to co-exist without messing with the same set of records. Registry provides a mechanism to ensure the safe management of DNS records This proposal introduces multiple possible implementation with the details depending on the setup.","title":"Purpose"},{"location":"proposal/registry/#requirements-and-assumptions","text":"Pre-existing records should not be modified by external-dns External-dns instance only creates/modifies/deletes records which are created by this instance It should be possible to transfer the ownership to another external-dns instance Any integrated DNS provider should provide at least a single way to implement the registry Noop registry can be used to disable ownership Lifetime of the records should not be limited to lifetime of external-dns External-dns should have its identifier for marking the managed records - owner-id","title":"Requirements and assumptions"},{"location":"proposal/registry/#types-of-registry","text":"The following presents two ways to implement the registry, and we are planning to implement both for compatibility purposes.","title":"Types of registry"},{"location":"proposal/registry/#txt-records","text":"This implementation idea is borrowed from Mate Each record created by external-dns is accompanied by the TXT record, which internally stores the external-dns identifier. For example, if external dns with owner-id=\"external-dns-1\" record to be created with dns name foo.zone.org , external-dns will create a TXT record with the same dns name <record_type>-foo.zone.org and injected value of \"external-dns-1\" . The transfer of ownership can be done by modifying the value of the TXT record. If no TXT record exists for the record or the value does not match its own owner-id , then external-dns will simply ignore it.","title":"TXT records"},{"location":"proposal/registry/#goods","text":"Easy to guarantee cross-cluster ownership safety Data lifetime is not limited to cluster or external-dns lifetime Supported by major DNS providers TXT record are created alongside other records in a batch request. Hence eliminating possibility of having inconsistent ownership information and dns provider state","title":"Goods"},{"location":"proposal/registry/#bads","text":"TXT record cannot co-exist with CNAME records (this can be solved by creating a TXT record with another domain name, e.g. foo.org->foo.txt.org ) Introduces complexity to the logic Difficult to do the transfer of ownership Too easy to mess up with manual modifications","title":"Bads"},{"location":"proposal/registry/#configmap","text":"This implementation cannot be considered 100% error free , hence use with caution [see Possible failure scenario below] Store the state in the configmap. ConfigMap is created and managed by each external-dns individually, i.e. external-dns with owner-id=external-dns-1 will create and operate on extern-dns-1-registry ConfigMap. ConfigMap will store all the records present in the DNS provider as serialized JSON. For example: kind: ConfigMap apiVersion: v1 metadata: creationTimestamp: 2016-03-09T19:14:38Z name: external-dns-1-storage namespace: same-as-external-dns-1 data: records: \"[{ \\\"dnsname\\\": \\\"foo.org\\\", \\\"owner\\\": \\\"external-dns-1\\\", \\\"target\\\": \\\"loadbalancer1.com\\\", \\\"type\\\": \\\"A\\\" }, { \\\"dnsname\\\": \\\"bar.org\\\", \\\"owner\\\": \\\"external-dns-2\\\", \\\"target\\\": \\\"loadbalancer2.com\\\", \\\"type\\\": \\\"A\\\" }, { \\\"dnsname\\\": \\\"unmanaged.org\\\", \\\"owner\\\": \\\"\\\", \\\"target\\\": \\\"loadbalancer2.com\\\", \\\"type\\\": \\\"CNAME\\\" }]\" ConfigMap will be periodically resynced with the dns provider by fetching the dns records and comparing it with the data currently stored and hence rebuilding the ownership information.","title":"ConfigMap"},{"location":"proposal/registry/#goods_1","text":"Not difficult to implement and easy to do the ownership transfer ConfigMap is a first class citizen in kubernetes world Does not create dependency/restriction on DNS provider Cannot be easily messed with by other parties","title":"Goods"},{"location":"proposal/registry/#bads_1","text":"ConfigMap might be out of sync with dns provider state LifeTime is obviously limited to the cluster lifetime Not supported in older kubernetes clusters Bloated ConfigMap - cannot be paginated and will grow very big on DNS provider with thousands of records","title":"Bads"},{"location":"proposal/registry/#failure-scenario","text":"It is possible that the configmap will go out of sync with the dns provider state. In the implementation flow external-dns will first modify records on dns provider side to subsequently update configmap. And if ExternalDNS will crash in-between two operation created records will be left unmanaged and not viable for update/deletion by External DNS.","title":"Failure scenario"},{"location":"proposal/registry/#component-integration","text":"Components: * Source - all endpoints ( collection of ingress, service[type=LoadBalancer] etc.) * Plan - object responsible for the create of change lists in external-dns * Provider - interface to access the DNS provider API Registry will serve as wrapper around Provider providing additional information regarding endpoint ownership. Ownership will further taken into account by Plan to filter out records to include only records managed by current ExternalDNS instance (having same owner-id value) A single loop iteration of external-dns operation: Get all endpoints ( collection ingress, service[type=LoadBalancer] etc.) into collection of endpoints Get registry Records() (makes the call to DNSProvider and also build ownership information) Pass Records (including ownership information) and list of endpoints to Plan to do the calculation Call registry ApplyChanges() method (which subsequently calls DNS Provider Apply method to update records) If ConfigMap implementation of Registry is used, then ConfigMap needs to be updated separately In case of configmap, Registry gets updated all the time via Poll . Plan does not call DNS provider directly. Good value of the Poll is to have simple rate limiting mechanism on DNS provider.","title":"Component integration"},{"location":"proposal/registry/#notes","text":"DNS Provider should use batch operations DNS Provider should be called with CREATE operation (not UPSERT!) when the record does not yet exist!","title":"Notes:"},{"location":"tutorials/ANS_Group_SafeDNS/","text":"Setting up ExternalDNS for Services on ANS Group\u2019s SafeDNS \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using SafeDNS. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial. Managing DNS with SafeDNS \u00b6 If you want to learn about how to use the SafeDNS service read the following tutorials: To learn more about the use of SafeDNS in general, see the following page: ANS Group\u2019s SafeDNS documentation . Creating SafeDNS credentials \u00b6 Generate a fresh API token for use with ExternalDNS, following the instructions at the ANS Group developer Getting-Started page. You will need to grant read/write access to the SafeDNS API. No access to any other ANS Group service is required. The environment variable SAFEDNS_TOKEN must have a value of this token to run ExternalDNS with SafeDNS integration. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # You will need to check what the latest version is yourself: # https://github.com/kubernetes-sigs/external-dns/releases image : registry.k8s.io/external-dns/external-dns:vX.Y.Z args : - --source=service # ingress is also possible # (optional) limit to only example.com domains; change to match the # zone created above. - --domain-filter=example.com - --provider=safedns env : - name : SAFEDNS_TOKEN value : \"SAFEDNSTOKENSAFEDNSTOKEN\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible # (optional) limit to only example.com domains; change to match the # zone created above. - --domain-filter=example.com - --provider=safedns env : - name : SAFEDNS_TOKEN value : \"SAFEDNSTOKENSAFEDNSTOKEN\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use a hostname that matches the domain filter specified above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the SafeDNS records. Verifying SafeDNS records \u00b6 Check your SafeDNS UI and select the appropriate domain to view the records for your SafeDNS zone. This should show the external IP address of the service as the A record for your domain. Alternatively, you can perform a DNS lookup for the hostname specified: $ dig +short my-app.example.com an.ip.addr.ess Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage SafeDNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on ANS Group's SafeDNS"},{"location":"tutorials/ANS_Group_SafeDNS/#setting-up-externaldns-for-services-on-ans-groups-safedns","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using SafeDNS. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on ANS Group's SafeDNS"},{"location":"tutorials/ANS_Group_SafeDNS/#managing-dns-with-safedns","text":"If you want to learn about how to use the SafeDNS service read the following tutorials: To learn more about the use of SafeDNS in general, see the following page: ANS Group\u2019s SafeDNS documentation .","title":"Managing DNS with SafeDNS"},{"location":"tutorials/ANS_Group_SafeDNS/#creating-safedns-credentials","text":"Generate a fresh API token for use with ExternalDNS, following the instructions at the ANS Group developer Getting-Started page. You will need to grant read/write access to the SafeDNS API. No access to any other ANS Group service is required. The environment variable SAFEDNS_TOKEN must have a value of this token to run ExternalDNS with SafeDNS integration.","title":"Creating SafeDNS credentials"},{"location":"tutorials/ANS_Group_SafeDNS/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/ANS_Group_SafeDNS/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # You will need to check what the latest version is yourself: # https://github.com/kubernetes-sigs/external-dns/releases image : registry.k8s.io/external-dns/external-dns:vX.Y.Z args : - --source=service # ingress is also possible # (optional) limit to only example.com domains; change to match the # zone created above. - --domain-filter=example.com - --provider=safedns env : - name : SAFEDNS_TOKEN value : \"SAFEDNSTOKENSAFEDNSTOKEN\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/ANS_Group_SafeDNS/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible # (optional) limit to only example.com domains; change to match the # zone created above. - --domain-filter=example.com - --provider=safedns env : - name : SAFEDNS_TOKEN value : \"SAFEDNSTOKENSAFEDNSTOKEN\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/ANS_Group_SafeDNS/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use a hostname that matches the domain filter specified above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the SafeDNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/ANS_Group_SafeDNS/#verifying-safedns-records","text":"Check your SafeDNS UI and select the appropriate domain to view the records for your SafeDNS zone. This should show the external IP address of the service as the A record for your domain. Alternatively, you can perform a DNS lookup for the hostname specified: $ dig +short my-app.example.com an.ip.addr.ess","title":"Verifying SafeDNS records"},{"location":"tutorials/ANS_Group_SafeDNS/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage SafeDNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/akamai-edgedns/","text":"Setting up External-DNS for Services on Akamai Edge DNS \u00b6 Prerequisites \u00b6 External-DNS v0.8.0 or greater. Zones \u00b6 External-DNS manages service endpoints in existing DNS zones. The Akamai provider does not add, remove or configure new zones. The Akamai Control Center or Akamai DevOps Tools , Akamai CLI and Akamai Terraform Provider can create and manage Edge DNS zones. Akamai Edge DNS Authentication \u00b6 The Akamai Edge DNS provider requires valid Akamai Edgegrid API authentication credentials to access zones and manage DNS records. Either directly by key or indirectly via a file can set credentials for the provider. The Akamai credential keys and mappings to the Akamai provider utilizing different presentation methods are: Edgegrid Auth Key External-DNS Cmd Line Key Environment/ConfigMap Key Description host akamai-serviceconsumerdomain EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN Akamai Edgegrid API server access_token akamai-access-token EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Akamai Edgegrid API access token client_token akamai-client-token EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN Akamai Edgegrid API client token client-secret akamai-client-secret EXTERNAL_DNS_AKAMAI_CLIENT_SECRET Akamai Edgegrid API client secret In addition to specifying auth credentials individually, an Akamai Edgegrid .edgerc file convention can set credentials. External-DNS Cmd Line Environment/ConfigMap Description akamai-edgerc-path EXTERNAL_DNS_AKAMAI_EDGERC_PATH Accessible path to Edgegrid credentials file, e.g /home/test/.edgerc akamai-edgerc-section EXTERNAL_DNS_AKAMAI_EDGERC_SECTION Section in Edgegrid credentials file containing credentials Akamai API Authentication provides an overview and further information about authorization credentials for API base applications and tools. Deploy External-DNS \u00b6 An operational External-DNS deployment consists of an External-DNS container and service. The following sections demonstrate the ConfigMap objects that would make up an example functional external DNS kubernetes configuration utilizing NGINX as the service. Connect your kubectl client to the External-DNS cluster, and then apply one of the following manifest files: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=akamai - --domain-filter=example.com # zone-id-filter may be specified as well to filter on contract ID - --registry=txt - --txt-owner-id={{ owner-id-for-this-external-dns }} - --txt-prefix={{ prefix label for TXT record }}. env : - name : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN - name : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN - name : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET - name : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=akamai - --domain-filter=example.com # zone-id-filter may be specified as well to filter on contract ID - --registry=txt - --txt-owner-id={{ owner-id-for-this-external-dns }} - --txt-prefix={{ prefix label for TXT record }}. env : - name : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN - name : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN - name : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET - name : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Create the deployment for External-DNS: $ kubectl apply -f externaldns.yaml Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com external-dns.alpha.kubernetes.io/ttl : \"600\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Create the deployment and service object: $ kubectl apply -f nginx.yaml Verify Akamai Edge DNS Records \u00b6 Wait 3-5 minutes before validating the records to allow the record changes to propagate to all the Akamai name servers. Validate records using the Akamai Control Center or by executing a dig, nslookup or similar DNS command. Cleanup \u00b6 Once you successfully configure and verify record management via External-DNS, you can delete the tutorial\u2019s examples: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml Additional Information \u00b6 The Akamai provider allows the administrative user to filter zones by both name ( domain-filter ) and contract Id ( zone-id-filter ). The Edge DNS API will return a \u2018500 Internal Error\u2019 for invalid contract Ids. The provider will substitute quotes in TXT records with a ` (back tick) when writing records with the API.","title":"Setting up External-DNS for Services on Akamai Edge DNS"},{"location":"tutorials/akamai-edgedns/#setting-up-external-dns-for-services-on-akamai-edge-dns","text":"","title":"Setting up External-DNS for Services on Akamai Edge DNS"},{"location":"tutorials/akamai-edgedns/#prerequisites","text":"External-DNS v0.8.0 or greater.","title":"Prerequisites"},{"location":"tutorials/akamai-edgedns/#zones","text":"External-DNS manages service endpoints in existing DNS zones. The Akamai provider does not add, remove or configure new zones. The Akamai Control Center or Akamai DevOps Tools , Akamai CLI and Akamai Terraform Provider can create and manage Edge DNS zones.","title":"Zones"},{"location":"tutorials/akamai-edgedns/#akamai-edge-dns-authentication","text":"The Akamai Edge DNS provider requires valid Akamai Edgegrid API authentication credentials to access zones and manage DNS records. Either directly by key or indirectly via a file can set credentials for the provider. The Akamai credential keys and mappings to the Akamai provider utilizing different presentation methods are: Edgegrid Auth Key External-DNS Cmd Line Key Environment/ConfigMap Key Description host akamai-serviceconsumerdomain EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN Akamai Edgegrid API server access_token akamai-access-token EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Akamai Edgegrid API access token client_token akamai-client-token EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN Akamai Edgegrid API client token client-secret akamai-client-secret EXTERNAL_DNS_AKAMAI_CLIENT_SECRET Akamai Edgegrid API client secret In addition to specifying auth credentials individually, an Akamai Edgegrid .edgerc file convention can set credentials. External-DNS Cmd Line Environment/ConfigMap Description akamai-edgerc-path EXTERNAL_DNS_AKAMAI_EDGERC_PATH Accessible path to Edgegrid credentials file, e.g /home/test/.edgerc akamai-edgerc-section EXTERNAL_DNS_AKAMAI_EDGERC_SECTION Section in Edgegrid credentials file containing credentials Akamai API Authentication provides an overview and further information about authorization credentials for API base applications and tools.","title":"Akamai Edge DNS Authentication"},{"location":"tutorials/akamai-edgedns/#deploy-external-dns","text":"An operational External-DNS deployment consists of an External-DNS container and service. The following sections demonstrate the ConfigMap objects that would make up an example functional external DNS kubernetes configuration utilizing NGINX as the service. Connect your kubectl client to the External-DNS cluster, and then apply one of the following manifest files:","title":"Deploy External-DNS"},{"location":"tutorials/akamai-edgedns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=akamai - --domain-filter=example.com # zone-id-filter may be specified as well to filter on contract ID - --registry=txt - --txt-owner-id={{ owner-id-for-this-external-dns }} - --txt-prefix={{ prefix label for TXT record }}. env : - name : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN - name : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN - name : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET - name : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/akamai-edgedns/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=akamai - --domain-filter=example.com # zone-id-filter may be specified as well to filter on contract ID - --registry=txt - --txt-owner-id={{ owner-id-for-this-external-dns }} - --txt-prefix={{ prefix label for TXT record }}. env : - name : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN - name : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN - name : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_CLIENT_SECRET - name : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Create the deployment for External-DNS: $ kubectl apply -f externaldns.yaml","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/akamai-edgedns/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com external-dns.alpha.kubernetes.io/ttl : \"600\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Create the deployment and service object: $ kubectl apply -f nginx.yaml","title":"Deploying an Nginx Service"},{"location":"tutorials/akamai-edgedns/#verify-akamai-edge-dns-records","text":"Wait 3-5 minutes before validating the records to allow the record changes to propagate to all the Akamai name servers. Validate records using the Akamai Control Center or by executing a dig, nslookup or similar DNS command.","title":"Verify Akamai Edge DNS Records"},{"location":"tutorials/akamai-edgedns/#cleanup","text":"Once you successfully configure and verify record management via External-DNS, you can delete the tutorial\u2019s examples: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/akamai-edgedns/#additional-information","text":"The Akamai provider allows the administrative user to filter zones by both name ( domain-filter ) and contract Id ( zone-id-filter ). The Edge DNS API will return a \u2018500 Internal Error\u2019 for invalid contract Ids. The provider will substitute quotes in TXT records with a ` (back tick) when writing records with the API.","title":"Additional Information"},{"location":"tutorials/alibabacloud/","text":"Setting up ExternalDNS for Services on Alibaba Cloud \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on Alibaba Cloud. Make sure to use >=0.5.6 version of ExternalDNS for this tutorial RAM Permissions \u00b6 { \"Version\" : \"1\" , \"Statement\" : [ { \"Action\" : \"alidns:AddDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DeleteDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:UpdateDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DescribeDomainRecords\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DescribeDomains\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:AddZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DeleteZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:UpdateZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZoneRecords\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZones\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZoneInfo\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } When running on Alibaba Cloud, you need to make sure that your nodes (on which External DNS runs) have the RAM instance profile with the above RAM role assigned. Set up a Alibaba Cloud DNS service or Private Zone service \u00b6 Alibaba Cloud DNS Service is the domain name resolution and management service for public access. It routes access from end-users to the designated web app. Alibaba Cloud Private Zone is the domain name resolution and management service for VPC internal access. If you prefer to try-out ExternalDNS in one of the existing domain or zone you can skip this step Create a DNS domain which will contain the managed DNS records. For public DNS service, the domain name should be valid and owned by yourself. $ aliyun alidns AddDomain --DomainName \"external-dns-test.com\" Make a note of the ID of the hosted zone you just created. $ aliyun alidns DescribeDomains --KeyWord = \"external-dns-test.com\" | jq -r '.Domains.Domain[0].DomainId' Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=alibabacloud - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier volumeMounts : - mountPath : /usr/share/zoneinfo name : hostpath volumes : - name : hostpath hostPath : path : /usr/share/zoneinfo type : Directory Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=alibabacloud - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier - --alibaba-cloud-config-file= # enable sts token volumeMounts : - mountPath : /usr/share/zoneinfo name : hostpath volumes : - name : hostpath hostPath : path : /usr/share/zoneinfo type : Directory Arguments \u00b6 This list is not the full list, but a few arguments that where chosen. alibaba-cloud-zone-type \u00b6 alibaba-cloud-zone-type allows filtering for private and public zones If value is public , it will sync with records in Alibaba Cloud DNS Service If value is private , it will sync with records in Alibaba Cloud Private Zone Service Verify ExternalDNS works (Ingress example) \u00b6 Create an ingress resource manifest file. For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : foo annotations : kubernetes.io/ingress.class : \"nginx\" # use the one that corresponds to your ingress controller. spec : rules : - host : foo.external-dns-test.com http : paths : - backend : service : name : foo port : number : 80 pathType : Prefix Verify ExternalDNS works (Service example) \u00b6 Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com. spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http After roughly two minutes check that a corresponding DNS record for your service was created. $ aliyun alidns DescribeDomainRecords --DomainName = external-dns-test.com { \"PageNumber\": 1, \"TotalCount\": 1, \"PageSize\": 20, \"RequestId\": \"1DBEF426-F771-46C7-9802-4989E9C94EE8\", \"DomainRecords\": { \"Record\": [ { \"RR\": \"nginx\", \"Status\": \"ENABLE\", \"Value\": \"1.2.3.4\", \"Weight\": 1, \"RecordId\": \"3994015629411328\", \"Type\": \"A\", \"DomainName\": \"external-dns-test.com\", \"Locked\": false, \"Line\": \"default\", \"TTL\": 600 }\uff0c { \"RR\": \"nginx\", \"Status\": \"ENABLE\", \"Value\": \"heritage=external-dns;external-dns/owner=my-identifier\", \"Weight\": 1, \"RecordId\": \"3994015629411329\", \"Type\": \"TTL\", \"DomainName\": \"external-dns-test.com\", \"Locked\": false, \"Line\": \"default\", \"TTL\": 600 } ] } } Note created TXT record alongside ALIAS record. TXT record signifies that the corresponding ALIAS record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. $ dig nginx.external-dns-test.com. If you hooked up your DNS zone with its parent zone correctly you can use curl to access your site. $ curl nginx.external-dns-test.com. <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html> Custom TTL \u00b6 The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com external-dns.alpha.kubernetes.io/ttl : 60 spec : ... This will set the DNS record\u2019s TTL to 60 seconds. Clean up \u00b6 Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly. $ kubectl delete service nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose. $ aliyun alidns DeleteDomain --DomainName external-dns-test.com For more info about Alibaba Cloud external dns, please refer this docs","title":"Setting up ExternalDNS for Services on Alibaba Cloud"},{"location":"tutorials/alibabacloud/#setting-up-externaldns-for-services-on-alibaba-cloud","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on Alibaba Cloud. Make sure to use >=0.5.6 version of ExternalDNS for this tutorial","title":"Setting up ExternalDNS for Services on Alibaba Cloud"},{"location":"tutorials/alibabacloud/#ram-permissions","text":"{ \"Version\" : \"1\" , \"Statement\" : [ { \"Action\" : \"alidns:AddDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DeleteDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:UpdateDomainRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DescribeDomainRecords\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"alidns:DescribeDomains\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:AddZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DeleteZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:UpdateZoneRecord\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZoneRecords\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZones\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" }, { \"Action\" : \"pvtz:DescribeZoneInfo\" , \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } When running on Alibaba Cloud, you need to make sure that your nodes (on which External DNS runs) have the RAM instance profile with the above RAM role assigned.","title":"RAM Permissions"},{"location":"tutorials/alibabacloud/#set-up-a-alibaba-cloud-dns-service-or-private-zone-service","text":"Alibaba Cloud DNS Service is the domain name resolution and management service for public access. It routes access from end-users to the designated web app. Alibaba Cloud Private Zone is the domain name resolution and management service for VPC internal access. If you prefer to try-out ExternalDNS in one of the existing domain or zone you can skip this step Create a DNS domain which will contain the managed DNS records. For public DNS service, the domain name should be valid and owned by yourself. $ aliyun alidns AddDomain --DomainName \"external-dns-test.com\" Make a note of the ID of the hosted zone you just created. $ aliyun alidns DescribeDomains --KeyWord = \"external-dns-test.com\" | jq -r '.Domains.Domain[0].DomainId'","title":"Set up a Alibaba Cloud DNS service or Private Zone service"},{"location":"tutorials/alibabacloud/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/alibabacloud/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=alibabacloud - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier volumeMounts : - mountPath : /usr/share/zoneinfo name : hostpath volumes : - name : hostpath hostPath : path : /usr/share/zoneinfo type : Directory","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/alibabacloud/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=alibabacloud - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier - --alibaba-cloud-config-file= # enable sts token volumeMounts : - mountPath : /usr/share/zoneinfo name : hostpath volumes : - name : hostpath hostPath : path : /usr/share/zoneinfo type : Directory","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/alibabacloud/#arguments","text":"This list is not the full list, but a few arguments that where chosen.","title":"Arguments"},{"location":"tutorials/alibabacloud/#alibaba-cloud-zone-type","text":"alibaba-cloud-zone-type allows filtering for private and public zones If value is public , it will sync with records in Alibaba Cloud DNS Service If value is private , it will sync with records in Alibaba Cloud Private Zone Service","title":"alibaba-cloud-zone-type"},{"location":"tutorials/alibabacloud/#verify-externaldns-works-ingress-example","text":"Create an ingress resource manifest file. For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : foo annotations : kubernetes.io/ingress.class : \"nginx\" # use the one that corresponds to your ingress controller. spec : rules : - host : foo.external-dns-test.com http : paths : - backend : service : name : foo port : number : 80 pathType : Prefix","title":"Verify ExternalDNS works (Ingress example)"},{"location":"tutorials/alibabacloud/#verify-externaldns-works-service-example","text":"Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com. spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http After roughly two minutes check that a corresponding DNS record for your service was created. $ aliyun alidns DescribeDomainRecords --DomainName = external-dns-test.com { \"PageNumber\": 1, \"TotalCount\": 1, \"PageSize\": 20, \"RequestId\": \"1DBEF426-F771-46C7-9802-4989E9C94EE8\", \"DomainRecords\": { \"Record\": [ { \"RR\": \"nginx\", \"Status\": \"ENABLE\", \"Value\": \"1.2.3.4\", \"Weight\": 1, \"RecordId\": \"3994015629411328\", \"Type\": \"A\", \"DomainName\": \"external-dns-test.com\", \"Locked\": false, \"Line\": \"default\", \"TTL\": 600 }\uff0c { \"RR\": \"nginx\", \"Status\": \"ENABLE\", \"Value\": \"heritage=external-dns;external-dns/owner=my-identifier\", \"Weight\": 1, \"RecordId\": \"3994015629411329\", \"Type\": \"TTL\", \"DomainName\": \"external-dns-test.com\", \"Locked\": false, \"Line\": \"default\", \"TTL\": 600 } ] } } Note created TXT record alongside ALIAS record. TXT record signifies that the corresponding ALIAS record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. $ dig nginx.external-dns-test.com. If you hooked up your DNS zone with its parent zone correctly you can use curl to access your site. $ curl nginx.external-dns-test.com. <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html>","title":"Verify ExternalDNS works (Service example)"},{"location":"tutorials/alibabacloud/#custom-ttl","text":"The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com external-dns.alpha.kubernetes.io/ttl : 60 spec : ... This will set the DNS record\u2019s TTL to 60 seconds.","title":"Custom TTL"},{"location":"tutorials/alibabacloud/#clean-up","text":"Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly. $ kubectl delete service nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose. $ aliyun alidns DeleteDomain --DomainName external-dns-test.com For more info about Alibaba Cloud external dns, please refer this docs","title":"Clean up"},{"location":"tutorials/aws-load-balancer-controller/","text":"Using ExternalDNS with aws-load-balancer-controller \u00b6 This tutorial describes how to use ExternalDNS with the aws-load-balancer-controller . Setting up ExternalDNS and aws-load-balancer-controller \u00b6 Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the source=ingress argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the ingress-class argument, but this is not required. For help setting up the AWS Load Balancer Controller, follow the Setup Guide . Note that the AWS Load Balancer Controller uses the same tags for subnet auto-discovery as Kubernetes does with the AWS cloud provider. In the examples that follow, it is assumed that you configured the ALB Ingress Controller with the ingress-class=alb argument (not to be confused with the same argument to ExternalDNS) so that the controller will only respect Ingress objects with the kubernetes.io/ingress.class annotation set to \u201calb\u201d. Deploy an example application \u00b6 Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ALB ingress objects. apiVersion : apps/v1 kind : Deployment metadata : name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver template : metadata : labels : app : echoserver spec : containers : - image : gcr.io/google_containers/echoserver:1.4 imagePullPolicy : Always name : echoserver ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : echoserver spec : ports : - port : 80 targetPort : 8080 protocol : TCP type : NodePort selector : app : echoserver Note that the Service object is of type NodePort . We don\u2019t need a Service of type LoadBalancer here, since we will be using an Ingress to create an ALB. Ingress examples \u00b6 Create the following Ingress to expose the echoserver application to the Internet. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - host : echoserver.mycluster.example.org http : &echoserver_root paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix - host : echoserver.example.org http : *echoserver_root The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to the echoserver application. If the source=ingress argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records being created, echoserver.mycluster.example.org and echoserver.example.org , which both alias the ALB that is associated with the Ingress object. Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : echoserver.mycluster.example.org, echoserver.example.org kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix In the above example we create a default path that works for any hostname, and make use of the external-dns.alpha.kubernetes.io/hostname annotation to create multiple aliases for the resulting ALB. Dualstack ALBs \u00b6 AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs. The AWS Load Balancer Controller uses the alb.ingress.kubernetes.io/ip-address-type annotation (which defaults to ipv4 ) to determine this. If this annotation is set to dualstack then ExternalDNS will create two alias records (one A record and one AAAA record) for each hostname associated with the Ingress object. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing alb.ingress.kubernetes.io/ip-address-type : dualstack kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an ALB with a dualstack interface. ExternalDNS will create both an A echoserver.example.org record and an AAAA record of the same name, that each are aliases for the same ALB.","title":"Using ExternalDNS with aws-load-balancer-controller"},{"location":"tutorials/aws-load-balancer-controller/#using-externaldns-with-aws-load-balancer-controller","text":"This tutorial describes how to use ExternalDNS with the aws-load-balancer-controller .","title":"Using ExternalDNS with aws-load-balancer-controller"},{"location":"tutorials/aws-load-balancer-controller/#setting-up-externaldns-and-aws-load-balancer-controller","text":"Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the source=ingress argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the ingress-class argument, but this is not required. For help setting up the AWS Load Balancer Controller, follow the Setup Guide . Note that the AWS Load Balancer Controller uses the same tags for subnet auto-discovery as Kubernetes does with the AWS cloud provider. In the examples that follow, it is assumed that you configured the ALB Ingress Controller with the ingress-class=alb argument (not to be confused with the same argument to ExternalDNS) so that the controller will only respect Ingress objects with the kubernetes.io/ingress.class annotation set to \u201calb\u201d.","title":"Setting up ExternalDNS and aws-load-balancer-controller"},{"location":"tutorials/aws-load-balancer-controller/#deploy-an-example-application","text":"Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ALB ingress objects. apiVersion : apps/v1 kind : Deployment metadata : name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver template : metadata : labels : app : echoserver spec : containers : - image : gcr.io/google_containers/echoserver:1.4 imagePullPolicy : Always name : echoserver ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : echoserver spec : ports : - port : 80 targetPort : 8080 protocol : TCP type : NodePort selector : app : echoserver Note that the Service object is of type NodePort . We don\u2019t need a Service of type LoadBalancer here, since we will be using an Ingress to create an ALB.","title":"Deploy an example application"},{"location":"tutorials/aws-load-balancer-controller/#ingress-examples","text":"Create the following Ingress to expose the echoserver application to the Internet. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - host : echoserver.mycluster.example.org http : &echoserver_root paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix - host : echoserver.example.org http : *echoserver_root The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to the echoserver application. If the source=ingress argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records being created, echoserver.mycluster.example.org and echoserver.example.org , which both alias the ALB that is associated with the Ingress object. Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : echoserver.mycluster.example.org, echoserver.example.org kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix In the above example we create a default path that works for any hostname, and make use of the external-dns.alpha.kubernetes.io/hostname annotation to create multiple aliases for the resulting ALB.","title":"Ingress examples"},{"location":"tutorials/aws-load-balancer-controller/#dualstack-albs","text":"AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs. The AWS Load Balancer Controller uses the alb.ingress.kubernetes.io/ip-address-type annotation (which defaults to ipv4 ) to determine this. If this annotation is set to dualstack then ExternalDNS will create two alias records (one A record and one AAAA record) for each hostname associated with the Ingress object. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/scheme : internet-facing alb.ingress.kubernetes.io/ip-address-type : dualstack kubernetes.io/ingress.class : alb name : echoserver spec : ingressClassName : alb rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an ALB with a dualstack interface. ExternalDNS will create both an A echoserver.example.org record and an AAAA record of the same name, that each are aliases for the same ALB.","title":"Dualstack ALBs"},{"location":"tutorials/aws-sd/","text":"Setting up ExternalDNS using AWS Cloud Map API \u00b6 This tutorial describes how to set up ExternalDNS for usage within a Kubernetes cluster with AWS Cloud Map API . AWS Cloud Map API is an alternative approach to managing DNS records directly using the Route53 API. It is more suitable for a dynamic environment where service endpoints change frequently. It abstracts away technical details of the DNS protocol and offers a simplified model. AWS Cloud Map consists of three main API calls: CreatePublicDnsNamespace \u2013 automatically creates a DNS hosted zone CreateService \u2013 creates a new named service inside the specified namespace RegisterInstance/DeregisterInstance \u2013 can be called multiple times to create a DNS record for the specified Service Learn more about the API in the AWS Cloud Map API Reference . IAM Permissions \u00b6 To use the AWS Cloud Map API, a user must have permissions to create the DNS namespace. Additionally you need to make sure that your nodes (on which External DNS runs) have an IAM instance profile with the AWSCloudMapFullAccess managed policy attached, that provides following permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:GetHostedZone\", \"route53:ListHostedZonesByName\", \"route53:CreateHostedZone\", \"route53:DeleteHostedZone\", \"route53:ChangeResourceRecordSets\", \"route53:CreateHealthCheck\", \"route53:GetHealthCheck\", \"route53:DeleteHealthCheck\", \"route53:UpdateHealthCheck\", \"ec2:DescribeVpcs\", \"ec2:DescribeRegions\", \"servicediscovery:*\" ], \"Resource\": [ \"*\" ] } ] } Set up a namespace \u00b6 Create a DNS namespace using the AWS Cloud Map API: $ aws servicediscovery create-public-dns-namespace --name \"external-dns-test.my-org.com\" Verify that the namespace was truly created $ aws servicediscovery list-namespaces Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster that you want to test ExternalDNS with. Then apply the following manifest file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 env : - name : AWS_REGION value : us-east-1 # put your CloudMap NameSpace region args : - --source=service - --source=ingress - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces. - --provider=aws-sd - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both) - --txt-owner-id=my-identifier Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 env : - name : AWS_REGION value : us-east-1 # put your CloudMap NameSpace region args : - --source=service - --source=ingress - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces. - --provider=aws-sd - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both) - --txt-owner-id=my-identifier Verify that ExternalDNS works (Service example) \u00b6 Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http After one minute check that a corresponding DNS record for your service was created in your hosted zone. We recommended that you use the Amazon Route53 console for that purpose. Custom TTL \u00b6 The default DNS record TTL (time to live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . For example, modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com external-dns.alpha.kubernetes.io/ttl : 60 spec : ... This will set the TTL for the DNS record to 60 seconds. Clean up \u00b6 Delete all service objects before terminating the cluster so all load balancers get cleaned up correctly. $ kubectl delete service nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the remaining service and namespace. $ aws servicediscovery list-services { \"Services\": [ { \"Id\": \"srv-6dygt5ywvyzvi3an\", \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:service/srv-6dygt5ywvyzvi3an\", \"Name\": \"nginx\" } ] } $ aws servicediscovery delete-service --id srv-6dygt5ywvyzvi3an $ aws servicediscovery list-namespaces { \"Namespaces\": [ { \"Type\": \"DNS_PUBLIC\", \"Id\": \"ns-durf2oxu4gxcgo6z\", \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:namespace/ns-durf2oxu4gxcgo6z\", \"Name\": \"external-dns-test.my-org.com\" } ] } $ aws servicediscovery delete-namespace --id ns-durf2oxu4gxcgo6z","title":"Setting up ExternalDNS using AWS Cloud Map API"},{"location":"tutorials/aws-sd/#setting-up-externaldns-using-aws-cloud-map-api","text":"This tutorial describes how to set up ExternalDNS for usage within a Kubernetes cluster with AWS Cloud Map API . AWS Cloud Map API is an alternative approach to managing DNS records directly using the Route53 API. It is more suitable for a dynamic environment where service endpoints change frequently. It abstracts away technical details of the DNS protocol and offers a simplified model. AWS Cloud Map consists of three main API calls: CreatePublicDnsNamespace \u2013 automatically creates a DNS hosted zone CreateService \u2013 creates a new named service inside the specified namespace RegisterInstance/DeregisterInstance \u2013 can be called multiple times to create a DNS record for the specified Service Learn more about the API in the AWS Cloud Map API Reference .","title":"Setting up ExternalDNS using AWS Cloud Map API"},{"location":"tutorials/aws-sd/#iam-permissions","text":"To use the AWS Cloud Map API, a user must have permissions to create the DNS namespace. Additionally you need to make sure that your nodes (on which External DNS runs) have an IAM instance profile with the AWSCloudMapFullAccess managed policy attached, that provides following permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"route53:GetHostedZone\", \"route53:ListHostedZonesByName\", \"route53:CreateHostedZone\", \"route53:DeleteHostedZone\", \"route53:ChangeResourceRecordSets\", \"route53:CreateHealthCheck\", \"route53:GetHealthCheck\", \"route53:DeleteHealthCheck\", \"route53:UpdateHealthCheck\", \"ec2:DescribeVpcs\", \"ec2:DescribeRegions\", \"servicediscovery:*\" ], \"Resource\": [ \"*\" ] } ] }","title":"IAM Permissions"},{"location":"tutorials/aws-sd/#set-up-a-namespace","text":"Create a DNS namespace using the AWS Cloud Map API: $ aws servicediscovery create-public-dns-namespace --name \"external-dns-test.my-org.com\" Verify that the namespace was truly created $ aws servicediscovery list-namespaces","title":"Set up a namespace"},{"location":"tutorials/aws-sd/#deploy-externaldns","text":"Connect your kubectl client to the cluster that you want to test ExternalDNS with. Then apply the following manifest file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/aws-sd/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 env : - name : AWS_REGION value : us-east-1 # put your CloudMap NameSpace region args : - --source=service - --source=ingress - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces. - --provider=aws-sd - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both) - --txt-owner-id=my-identifier","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/aws-sd/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 env : - name : AWS_REGION value : us-east-1 # put your CloudMap NameSpace region args : - --source=service - --source=ingress - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces. - --provider=aws-sd - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both) - --txt-owner-id=my-identifier","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/aws-sd/#verify-that-externaldns-works-service-example","text":"Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http After one minute check that a corresponding DNS record for your service was created in your hosted zone. We recommended that you use the Amazon Route53 console for that purpose.","title":"Verify that ExternalDNS works (Service example)"},{"location":"tutorials/aws-sd/#custom-ttl","text":"The default DNS record TTL (time to live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . For example, modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.my-org.com external-dns.alpha.kubernetes.io/ttl : 60 spec : ... This will set the TTL for the DNS record to 60 seconds.","title":"Custom TTL"},{"location":"tutorials/aws-sd/#clean-up","text":"Delete all service objects before terminating the cluster so all load balancers get cleaned up correctly. $ kubectl delete service nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the remaining service and namespace. $ aws servicediscovery list-services { \"Services\": [ { \"Id\": \"srv-6dygt5ywvyzvi3an\", \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:service/srv-6dygt5ywvyzvi3an\", \"Name\": \"nginx\" } ] } $ aws servicediscovery delete-service --id srv-6dygt5ywvyzvi3an $ aws servicediscovery list-namespaces { \"Namespaces\": [ { \"Type\": \"DNS_PUBLIC\", \"Id\": \"ns-durf2oxu4gxcgo6z\", \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:namespace/ns-durf2oxu4gxcgo6z\", \"Name\": \"external-dns-test.my-org.com\" } ] } $ aws servicediscovery delete-namespace --id ns-durf2oxu4gxcgo6z","title":"Clean up"},{"location":"tutorials/aws/","text":"Setting up ExternalDNS for Services on AWS \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on AWS. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial IAM Policy \u00b6 The following IAM Policy document allows ExternalDNS to update Route53 Resource Record Sets and Hosted Zones. You\u2019ll want to create this Policy in IAM first. In our example, we\u2019ll call the policy AllowExternalDNSUpdates (but you can call it whatever you prefer). If you prefer, you may fine-tune the policy to permit updates only to explicit Hosted Zone IDs. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } If you are using the AWS CLI, you can run the following to install the above policy (saved as policy.json ). This can be use in subsequent steps to allow ExternalDNS to access Route53 zones. aws iam create-policy --policy-name \"AllowExternalDNSUpdates\" --policy-document file://policy.json # example: arn:aws:iam::XXXXXXXXXXXX:policy/AllowExternalDNSUpdates export POLICY_ARN = $( aws iam list-policies \\ --query 'Policies[?PolicyName==`AllowExternalDNSUpdates`].Arn' --output text ) Provisioning a Kubernetes cluster \u00b6 You can use eksctl to easily provision an Amazon Elastic Kubernetes Service ( EKS ) cluster that is suitable for this tutorial. See Getting started with Amazon EKS \u2013 eksctl . export EKS_CLUSTER_NAME = \"my-externaldns-cluster\" export EKS_CLUSTER_REGION = \"us-east-2\" export KUBECONFIG = \" $HOME /.kube/ ${ EKS_CLUSTER_NAME } - ${ EKS_CLUSTER_REGION } .yaml\" eksctl create cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION Feel free to use other provisioning tools or an existing cluster. If Terraform is used, vpc and eks modules are recommended for standing up an EKS cluster. Amazon has a workshop called Amazon EKS Terraform Workshop that may be useful for this process. Permissions to modify DNS zone \u00b6 You will need to use the above policy (represented by the POLICY_ARN environment variable) to allow ExternalDNS to update records in Route53 DNS zones. Here are three common ways this can be accomplished: Node IAM Role Static credentials IAM Roles for Service Accounts For this tutorial, ExternalDNS will use the environment variable EXTERNALDNS_NS to represent the namespace, defaulted to default . Feel free to change this to something else, such externaldns or kube-addons . Make sure to edit the subjects[0].namespace for the ClusterRoleBinding resource when deploying ExternalDNS with RBAC enabled. See Manifest (for clusters with RBAC enabled) for more information. Additionally, throughout this tutorial, the example domain of example.com is used. Change this to appropriate domain under your control. See Set up a hosted zone section. Node IAM Role \u00b6 In this method, you can attach a policy to the Node IAM Role. This will allow nodes in the Kubernetes cluster to access Route53 zones, which allows ExternalDNS to update DNS records. Given that this allows all containers to access Route53, not just ExternalDNS, running on the node with these privileges, this method is not recommended, and is only suitable for limited limited test environments. If you are using eksctl to provision a new cluster, you add the policy at creation time with: eksctl create cluster --external-dns-access \\ --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION \\ WARNING : This will assign allow read-write access to all nodes in the cluster, not just ExternalDNS. For this reason, this method is only suitable for limited test environments. If you already provisioned a cluster or use other provisioning tools like Terraform, you can use AWS CLI to attach the policy to the Node IAM Role. Get the Node IAM role name \u00b6 The role name of the role associated with the node(s) where ExternalDNS will run is needed. An easy way to get the role name is to use the AWS web console (https://console.aws.amazon.com/eks/), and find any instance in the target node group and copy the role name associated with that instance. Get role name with a single managed nodegroup \u00b6 From the command line, if you have a single managed node group, the default with eksctl create cluster , you can find the role name with the following: # get managed node group name (assuming there's only one node group) GROUP_NAME = $( aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME \\ --query nodegroups --out text ) # fetch role arn given node group name ROLE_ARN = $( aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name $GROUP_NAME --query nodegroup.nodeRole --out text ) # extract just the name part of role arn ROLE_NAME = ${ NODE_ROLE_ARN ##*/ } Get role name with other configurations \u00b6 If you have multiple node groups or any unmanaged node groups, the process gets more complex. The first step is to get the instance host name of the desired node to where ExternalDNS will be deployed or is already deployed: # node instance name of one of the external dns pods currently running INSTANCE_NAME = $( kubectl get pods --all-namespaces \\ --selector app.kubernetes.io/instance = external-dns \\ --output jsonpath = '{.items[0].spec.nodeName}' ) # instance name of one of the nodes (change if node group is different) INSTANCE_NAME = $( kubectl get nodes --output name | cut -d '/' -f2 | tail -1 ) With the instance host name, you can then get the instance id: get_instance_id () { INSTANCE_NAME = $1 # example: ip-192-168-74-34.us-east-2.compute.internal # get list of nodes # ip-192-168-74-34.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx # ip-192-168-86-105.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx NODES = $( kubectl get nodes \\ --output jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.providerID}{\"\\n\"}{end}' ) # print instance id from matching node grep $INSTANCE_NAME <<< \" $NODES \" | cut -d '/' -f5 } INSTANCE_ID = $( get_instance_id $INSTANCE_NAME ) With the instance id, you can get the associated role name: findRoleName () { INSTANCE_ID = $1 # get all of the roles ROLES =( $( aws iam list-roles --query Roles [ * ] .RoleName --out text ) ) for ROLE in ${ ROLES [*] } ; do # get instance profile arn PROFILE_ARN = $( aws iam list-instance-profiles-for-role \\ --role-name $ROLE --query InstanceProfiles [ 0 ] .Arn --output text ) # if there is an instance profile if [[ \" $PROFILE_ARN \" ! = \"None\" ]] ; then # get all the instances with this associated instance profile INSTANCES = $( aws ec2 describe-instances \\ --filters Name = iam-instance-profile.arn,Values = $PROFILE_ARN \\ --query Reservations [ * ] .Instances [ 0 ] .InstanceId --out text ) # find instances that match the instant profile for INSTANCE in ${ INSTANCES [*] } ; do # set role name value if there is a match if [[ \" $INSTANCE_ID \" == \" $INSTANCE \" ]] ; then ROLE_NAME = $ROLE ; fi done fi done echo $ROLE_NAME } NODE_ROLE_NAME = $( findRoleName $INSTANCE_ID ) Using the role name, you can associate the policy that was created earlier: # attach policy arn created earlier to node IAM role aws iam attach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN WARNING : This will assign allow read-write access to all pods running on the same node pool, not just the ExternalDNS pod(s). Deploy ExternalDNS with attached policy to Node IAM Role \u00b6 If ExternalDNS is not yet deployed, follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC. NOTE : Before deleting the cluster during, be sure to run aws iam detach-role-policy . Otherwise, there can be errors as the provisioning system, such as eksctl or terraform , will not be able to delete the roles with the attached policy. Static credentials \u00b6 In this method, the policy is attached to an IAM user, and the credentials secrets for the IAM user are then made available using a Kubernetes secret. This method is not the preferred method as the secrets in the credential file could be copied and used by an unauthorized threat actor. However, if the Kubernetes cluster is not hosted on AWS, it may be the only method available. Given this situation, it is important to limit the associated privileges to just minimal required privileges, i.e. read-write access to Route53, and not used a credentials file that has extra privileges beyond what is required. Create IAM user and attach the policy \u00b6 # create IAM user aws iam create-user --user-name \"externaldns\" # attach policy arn created earlier to IAM user aws iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN Create the static credentials \u00b6 SECRET_ACCESS_KEY = $( aws iam create-access-key --user-name \"externaldns\" ) cat <<-EOF > /local/path/to/credentials [default] aws_access_key_id = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId') aws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey') EOF Create Kubernetes secret from credentials \u00b6 kubectl create secret generic external-dns \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } --from-file /local/path/to/credentials Deploy ExternalDNS using static credentials \u00b6 Follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC. Make sure to uncomment the section that mounts volumes, so that the credentials can be mounted. IAM Roles for Service Accounts \u00b6 IRSA ( IAM roles for Service Accounts ) allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts. This essentially allows only ExternalDNS pods to access Route53 without exposing any static credentials. This is the preferred method as it implements PoLP ( Principal of Least Privilege ). IMPORTANT : This method requires using KSA (Kuberntes service account) and RBAC. This method requires deploying with RBAC. See Manifest (for clusters with RBAC enabled) when ready to deploy ExternalDNS. NOTE : Similar methods to IRSA on AWS are kiam , which is in maintenence mode, and has instructions for creating an IAM role, and also kube2iam . IRSA is the officially supported method for EKS clusters, and so for non-EKS clusters on AWS, these other tools could be an option. Verify OIDC is supported \u00b6 aws eks describe-cluster --name $EKS_CLUSTER_NAME \\ --query \"cluster.identity.oidc.issuer\" --output text Associate OIDC to cluster \u00b6 Configure the cluster with an OIDC provider and add support for IRSA ( IAM roles for Service Accounts ). If you used eksctl to provision the EKS cluster, you can update it with the following command: eksctl utils associate-iam-oidc-provider \\ --cluster $EKS_CLUSTER_NAME --approve If the cluster was provisioned with Terraform, you can use the iam_openid_connect_provider resource ( ref ) to associate to the OIDC provider. Create an IAM role bound to a service account \u00b6 For the next steps in this process, we will need to associate the external-dns service account and a role used to grant access to Route53. This requires the following steps: Create a role with a trust relationship to the cluster\u2019s OIDC provider Attach the AllowExternalDNSUpdates policy to the role Create the external-dns service account Add annotation to the service account with the role arn Use eksctl with eksctl created EKS cluster \u00b6 If eksctl was used to provision the EKS cluster, you can perform all of these steps with the following command: eksctl create iamserviceaccount \\ --cluster $EKS_CLUSTER_NAME \\ --name \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --attach-policy-arn $POLICY_ARN \\ --approve Use aws cli with any EKS cluster \u00b6 Otherwise, we can do the following steps using aws commands (also see Creating an IAM role and policy for your service account ): ACCOUNT_ID = $( aws sts get-caller-identity \\ --query \"Account\" --output text ) OIDC_PROVIDER = $( aws eks describe-cluster --name $EKS_CLUSTER_NAME \\ --query \"cluster.identity.oidc.issuer\" --output text | sed -e 's|^https://||' ) cat <<-EOF > trust.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$ACCOUNT_ID:oidc-provider/$OIDC_PROVIDER\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"$OIDC_PROVIDER:sub\": \"system:serviceaccount:${EXTERNALDNS_NS:-\"default\"}:external-dns\", \"$OIDC_PROVIDER:aud\": \"sts.amazonaws.com\" } } } ] } EOF IRSA_ROLE = \"external-dns-irsa-role\" aws iam create-role --role-name $IRSA_ROLE --assume-role-policy-document file://trust.json aws iam attach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN ROLE_ARN = $( aws iam get-role --role-name $IRSA_ROLE --query Role.Arn --output text ) # Create service account (skip is already created) kubectl create serviceaccount \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } # Add annotation referencing IRSA role kubectl patch serviceaccount \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } --patch \\ \"{\\\"metadata\\\": { \\\"annotations\\\": { \\\"eks.amazonaws.com/role-arn\\\": \\\" $ROLE_ARN \\\" }}}\" If any part of this step is misconfigured, such as the role with incorrect namespace configured in the trust relationship, annotation pointing the the wrong role, etc., you will see errors like WebIdentityErr: failed to retrieve credentials . Check the configuration and make corrections. When the service account annotations are updated, then the current running pods will have to be terminated, so that new pod(s) with proper configuration (environment variables) will be created automatically. When annotation is added to service account, the ExternalDNS pod(s) scheduled will have AWS_ROLE_ARN , AWS_STS_REGIONAL_ENDPOINTS , and AWS_WEB_IDENTITY_TOKEN_FILE environment variables injected automatically. Deploy ExternalDNS using IRSA \u00b6 Follow the steps under Manifest (for clusters with RBAC enabled) . Make sure to comment out the service account section if this has been created already. If you deployed ExternalDNS before adding the service account annotation and the corresponding role, you will likely see error with failed to list hosted zones: AccessDenied: User . You can delete the current running ExternalDNS pod(s) after updating the annotation, so that new pods scheduled will have appropriate configuration to access Route53. Set up a hosted zone \u00b6 If you prefer to try-out ExternalDNS in one of the existing hosted-zones you can skip this step Create a DNS zone which will contain the managed DNS records. This tutorial will use the fictional domain of example.com . aws route53 create-hosted-zone --name \"example.com.\" \\ --caller-reference \"external-dns-test- $( date +%s ) \" Make a note of the nameservers that were assigned to your new zone. ZONE_ID = $( aws route53 list-hosted-zones-by-name --output json \\ --dns-name \"example.com.\" --query HostedZones [ 0 ] .Id --out text ) aws route53 list-resource-record-sets --output text \\ --hosted-zone-id $ZONE_ID --query \\ \"ResourceRecordSets[?Type == 'NS'].ResourceRecords[*].Value | []\" | tr '\\t' '\\n' This should yield something similar this: ns-695.awsdns-22.net. ns-1313.awsdns-36.org. ns-350.awsdns-43.com. ns-1805.awsdns-33.co.uk. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the from the list above. Please consult your registrar\u2019s documentation on how to do that. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. You can check if your cluster has RBAC by kubectl api-versions | grep rbac.authorization.k8s.io . For clusters with RBAC enabled, be sure to choose the correct namespace . For this tutorial, the enviornment variable EXTERNALDNS_NS will refer to the namespace. You can set this to a value of your choice: export EXTERNALDNS_NS = \"default\" # externaldns, kube-addons, etc # create namespace if it does not yet exist kubectl get namespaces | grep -q $EXTERNALDNS_NS || \\ kubectl create namespace $EXTERNALDNS_NS Manifest (for clusters without RBAC enabled) \u00b6 Save the following below as externaldns-no-rbac.yaml . apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-hostedzone-identifier env : - name : AWS_DEFAULT_REGION value : us-east-1 # change to region where EKS is installed # # Uncomment below if using static credentials # - name: AWS_SHARED_CREDENTIALS_FILE # value: /.aws/credentials # volumeMounts: # - name: aws-credentials # mountPath: /.aws # readOnly: true # volumes: # - name: aws-credentials # secret: # secretName: external-dns When ready you can deploy: kubectl create --filename externaldns-no-rbac.yaml \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } Manifest (for clusters with RBAC enabled) \u00b6 Save the following below as externaldns-with-rbac.yaml . # comment out sa if it was previously created apiVersion : v1 kind : ServiceAccount metadata : name : external-dns labels : app.kubernetes.io/name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns labels : app.kubernetes.io/name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer labels : app.kubernetes.io/name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default # change to desired namespace: externaldns, kube-addons --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=external-dns env : - name : AWS_DEFAULT_REGION value : us-east-1 # change to region where EKS is installed # # Uncommend below if using static credentials # - name: AWS_SHARED_CREDENTIALS_FILE # value: /.aws/credentials # volumeMounts: # - name: aws-credentials # mountPath: /.aws # readOnly: true # volumes: # - name: aws-credentials # secret: # secretName: external-dns When ready deploy: kubectl create --filename externaldns-with-rbac.yaml \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } Arguments \u00b6 This list is not the full list, but a few arguments that where chosen. aws-zone-type \u00b6 aws-zone-type allows filtering for private and public zones Annotations \u00b6 Annotations which are specific to AWS. alias \u00b6 external-dns.alpha.kubernetes.io/alias if set to true on an ingress, it will create an ALIAS record when the target is an ALIAS as well. To make the target an alias, the ingress needs to be configured correctly as described in the docs . In particular, the argument --publish-service=default/nginx-ingress-controller has to be set on the nginx-ingress-controller container. If one uses the nginx-ingress Helm chart, this flag can be set with the controller.publishService.enabled configuration option. Verify ExternalDNS works (Service example) \u00b6 Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. If you want to give multiple names to service, you can set it to external-dns.alpha.kubernetes.io/hostname with a comma , separator. For this verification phase, you can use default or another namespace for the nginx demo, for example: NGINXDEMO_NS = \"nginx\" kubectl get namespaces | grep -q $NGINXDEMO_NS || kubectl create namespace $NGINXDEMO_NS Save the following manifest below as nginx.yaml : apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http Deploy the nginx deployment and service with: kubectl create --filename nginx.yaml --namespace ${ NGINXDEMO_NS :- \"default\" } Verify that the load balancer was allocated with: kubectl get service nginx --namespace ${ NGINXDEMO_NS :- \"default\" } This should show something like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx LoadBalancer 10 .100.47.41 ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. 80 :32749/TCP 12m After roughly two minutes check that a corresponding DNS record for your service that was created. aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'A']\" This should show something like: [ { \"Name\" : \"nginx.example.com.\" , \"Type\" : \"A\" , \"AliasTarget\" : { \"HostedZoneId\" : \"ZEWFWZ4R16P7IB\" , \"DNSName\" : \"ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.\" , \"EvaluateTargetHealth\" : true } } ] You can also fetch the corresponding text records: aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'TXT']\" This will show something like: [ { \"Name\" : \"nginx.example.com.\" , \"Type\" : \"TXT\" , \"TTL\" : 300 , \"ResourceRecords\" : [ { \"Value\" : \"\\\"heritage=external-dns,external-dns/owner=external-dns,external-dns/resource=service/default/nginx\\\"\" } ] } ] Note created TXT record alongside ALIAS record. TXT record signifies that the corresponding ALIAS record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. For more information about ALIAS record, see Choosing between alias and non-alias records . Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. dig +short @ns-5514.awsdns-53.org. nginx.example.com. This should return 1+ IP addresses that correspond to the ELB FQDN, i.e. ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. . Next try the public nameservers configured by DNS client on your system: dig +short nginx.example.com. If you hooked up your DNS zone with its parent zone correctly you can use curl to access your site. curl nginx.example.com. This should show something like: <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > ... </ head > < body > < h1 > Welcome to nginx! </ h1 > ... </ body > </ html > Verify ExternalDNS works (Ingress example) \u00b6 With the previous deployment and service objects deployed, we can add an ingress object and configure a FQDN value for the host key. The ingress controller will match incoming HTTP traffic, and route it to the appropriate backend service based on the host key. For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object. For this tutorial, we have two endpoints, the service with LoadBalancer type and an ingress. For practical purposes, if an ingress is used, the service type can be changed to ClusterIP as two endpoints are unecessary in this scenario. IMPORTANT : This requires that an ingress controller has been installed in your Kubernetes cluster. EKS does not come with an ingress controller by default. A popular ingress controller is ingress-nginx , which can be installed by a helm chart or by manifests . Create an ingress resource manifest file named ingress.yaml with the contents below: --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : \"nginx\" # use the one that corresponds to your ingress controller. spec : rules : - host : server.example.com http : paths : - backend : service : name : nginx port : number : 80 path : / pathType : Prefix When ready, you can deploy this with: kubectl create --filename ingress.yaml --namespace ${ NGINXDEMO_NS :- \"default\" } Watch the status of the ingress until the ADDRESS field is populated. kubectl get ingress --watch --namespace ${ NGINXDEMO_NS :- \"default\" } You should see something like this: NAME CLASS HOSTS ADDRESS PORTS AGE nginx <none> server.example.com 80 47s nginx <none> server.example.com ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. 80 54s For the ingress test, run through similar checks, but using domain name used for the ingress: # check records on route53 aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'server.example.com.']\" # query using a route53 name server dig +short @ns-5514.awsdns-53.org. server.example.com. # query using the default name server dig +short server.example.com. # connect to the nginx web server through the ingress curl server.example.com. More service annotation options \u00b6 Custom TTL \u00b6 The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com external-dns.alpha.kubernetes.io/ttl : \"60\" spec : ... This will set the DNS record\u2019s TTL to 60 seconds. Routing policies \u00b6 Route53 offers different routing policies . The routing policy for a record can be controlled with the following annotations: external-dns.alpha.kubernetes.io/set-identifier : this needs to be set to use any of the following routing policies For any given DNS name, only one of the following routing policies can be used: Weighted records: external-dns.alpha.kubernetes.io/aws-weight Latency-based routing: external-dns.alpha.kubernetes.io/aws-region Failover: external-dns.alpha.kubernetes.io/aws-failover Geolocation-based routing: external-dns.alpha.kubernetes.io/aws-geolocation-continent-code external-dns.alpha.kubernetes.io/aws-geolocation-country-code external-dns.alpha.kubernetes.io/aws-geolocation-subdivision-code Multi-value answer: external-dns.alpha.kubernetes.io/aws-multi-value-answer Associating DNS records with healthchecks \u00b6 You can configure Route53 to associate DNS records with healthchecks for automated DNS failover using external-dns.alpha.kubernetes.io/aws-health-check-id: <health-check-id> annotation. Note: ExternalDNS does not support creating healthchecks, and assumes that <health-check-id> already exists. Govcloud caveats \u00b6 Due to the special nature with how Route53 runs in Govcloud, there are a few tweaks in the deployment settings. An Environment variable with name of AWS_REGION set to either us-gov-west-1 or us-gov-east-1 is required. Otherwise it tries to lookup a region that does not exist in Govcloud and it errors out. env : - name : AWS_REGION value : us-gov-west-1 Route53 in Govcloud does not allow aliases. Therefore, container args must be set so that it uses CNAMES and a txt-prefix must be set to something. Otherwise, it will try to create a TXT record with the same value than the CNAME itself, which is not allowed. args : - --aws-prefer-cname - --txt-prefix={{ YOUR_PREFIX }} The first two changes are needed if you use Route53 in Govcloud, which only supports private zones. There are also no cross account IAM whatsoever between Govcloud and commerical AWS accounts. If services and ingresses need to make Route 53 entries to an public zone in a commerical account, you will have set env variables of AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with a key and secret to the commerical account that has the sufficient rights. env : - name : AWS_ACCESS_KEY_ID value : XXXXXXXXX - name : AWS_SECRET_ACCESS_KEY valueFrom : secretKeyRef : name : {{ YOUR_SECRET_NAME }} key : {{ YOUR_SECRET_KEY }} Clean up \u00b6 Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly. kubectl delete service nginx IMPORTANT If you attached a policy to the Node IAM Role, then you will want to detach this before deleting the EKS cluster. Otherwise, the role resource will be locked, and the cluster cannot be deleted, especially if it was provisioned by automation like terraform or eksctl . aws iam detach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN If the cluster was provisioned using eksctl , you can delete the cluster with: eksctl delete cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose. aws route53 delete-hosted-zone --id $NODE_ID # e.g /hostedzone/ZEWFWZ4R16P7IB If IAM user credentials were used, you can remove the user with: aws iam detach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN aws iam delete-user --user-name \"externaldns\" If IRSA was used, you can remove the IRSA role with: aws iam detach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN aws iam delete-role --role-name $IRSA_ROLE Delete any unneeded policies: aws iam delete-policy --policy-arn $POLICY_ARN Throttling \u00b6 Route53 has a 5 API requests per second per account hard quota . Running several fast polling ExternalDNS instances in a given account can easily hit that limit. Some ways to reduce the request rate include: * Reduce the polling loop\u2019s synchronization interval at the possible cost of slower change propagation (but see --events below to reduce the impact). * --interval=5m (default 1m ) * Trigger the polling loop on changes to K8s objects, rather than only at interval , to have responsive updates with long poll intervals * --events * Limit the sources watched when the --events flag is specified to specific types, namespaces, labels, or annotations * --source=ingress --source=service - specify multiple times for multiple sources * --namespace=my-app * --label-filter=app in (my-app) * --annotation-filter=kubernetes.io/ingress.class in (nginx-external) - note that this filter would apply to services too.. * Limit services watched by type (not applicable to ingress or other types) * --service-type-filter=LoadBalancer default all * Limit the hosted zones considered * --zone-id-filter=ABCDEF12345678 - specify multiple times if needed * --domain-filter=example.com by domain suffix - specify multiple times if needed * --regex-domain-filter=example* by domain suffix but as a regex - overrides domain-filter * --exclude-domains=ignore.this.example.com to exclude a domain or subdomain * --regex-domain-exclusion=ignore* subtracts it\u2019s matches from regex-domain-filter \u2018s matches * --aws-zone-type=public only sync zones of this type [public|private] * --aws-zone-tags=owner=k8s only sync zones with this tag * If the list of zones managed by ExternalDNS doesn\u2019t change frequently, cache it by setting a TTL. * --aws-zones-cache-duration=3h (default 0 - disabled) * Increase the number of changes applied to Route53 in each batch * --aws-batch-change-size=4000 (default 1000 ) * Increase the interval between changes * --aws-batch-change-interval=10s (default 1s ) * Introducing some jitter to the pod initialization, so that when multiple instances of ExternalDNS are updated at the same time they do not make their requests on the same second. A simple way to implement randomised startup is with an init container: ... spec: initContainers: - name: init-jitter image: registry.k8s.io/external-dns/external-dns:v0.13.1 command: - /bin/sh - -c - 'FOR=$((RANDOM % 10))s;echo \"Sleeping for $FOR\";sleep $FOR' containers: ... EKS \u00b6 An effective starting point for EKS with an ingress controller might look like: --interval = 5m --events --source = ingress --domain-filter = example.com --aws-zones-cache-duration = 1h","title":"Setting up ExternalDNS for Services on AWS"},{"location":"tutorials/aws/#setting-up-externaldns-for-services-on-aws","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on AWS. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial","title":"Setting up ExternalDNS for Services on AWS"},{"location":"tutorials/aws/#iam-policy","text":"The following IAM Policy document allows ExternalDNS to update Route53 Resource Record Sets and Hosted Zones. You\u2019ll want to create this Policy in IAM first. In our example, we\u2019ll call the policy AllowExternalDNSUpdates (but you can call it whatever you prefer). If you prefer, you may fine-tune the policy to permit updates only to explicit Hosted Zone IDs. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : [ \"arn:aws:route53:::hostedzone/*\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ListHostedZones\" , \"route53:ListResourceRecordSets\" ], \"Resource\" : [ \"*\" ] } ] } If you are using the AWS CLI, you can run the following to install the above policy (saved as policy.json ). This can be use in subsequent steps to allow ExternalDNS to access Route53 zones. aws iam create-policy --policy-name \"AllowExternalDNSUpdates\" --policy-document file://policy.json # example: arn:aws:iam::XXXXXXXXXXXX:policy/AllowExternalDNSUpdates export POLICY_ARN = $( aws iam list-policies \\ --query 'Policies[?PolicyName==`AllowExternalDNSUpdates`].Arn' --output text )","title":"IAM Policy"},{"location":"tutorials/aws/#provisioning-a-kubernetes-cluster","text":"You can use eksctl to easily provision an Amazon Elastic Kubernetes Service ( EKS ) cluster that is suitable for this tutorial. See Getting started with Amazon EKS \u2013 eksctl . export EKS_CLUSTER_NAME = \"my-externaldns-cluster\" export EKS_CLUSTER_REGION = \"us-east-2\" export KUBECONFIG = \" $HOME /.kube/ ${ EKS_CLUSTER_NAME } - ${ EKS_CLUSTER_REGION } .yaml\" eksctl create cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION Feel free to use other provisioning tools or an existing cluster. If Terraform is used, vpc and eks modules are recommended for standing up an EKS cluster. Amazon has a workshop called Amazon EKS Terraform Workshop that may be useful for this process.","title":"Provisioning a Kubernetes cluster"},{"location":"tutorials/aws/#permissions-to-modify-dns-zone","text":"You will need to use the above policy (represented by the POLICY_ARN environment variable) to allow ExternalDNS to update records in Route53 DNS zones. Here are three common ways this can be accomplished: Node IAM Role Static credentials IAM Roles for Service Accounts For this tutorial, ExternalDNS will use the environment variable EXTERNALDNS_NS to represent the namespace, defaulted to default . Feel free to change this to something else, such externaldns or kube-addons . Make sure to edit the subjects[0].namespace for the ClusterRoleBinding resource when deploying ExternalDNS with RBAC enabled. See Manifest (for clusters with RBAC enabled) for more information. Additionally, throughout this tutorial, the example domain of example.com is used. Change this to appropriate domain under your control. See Set up a hosted zone section.","title":"Permissions to modify DNS zone"},{"location":"tutorials/aws/#node-iam-role","text":"In this method, you can attach a policy to the Node IAM Role. This will allow nodes in the Kubernetes cluster to access Route53 zones, which allows ExternalDNS to update DNS records. Given that this allows all containers to access Route53, not just ExternalDNS, running on the node with these privileges, this method is not recommended, and is only suitable for limited limited test environments. If you are using eksctl to provision a new cluster, you add the policy at creation time with: eksctl create cluster --external-dns-access \\ --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION \\ WARNING : This will assign allow read-write access to all nodes in the cluster, not just ExternalDNS. For this reason, this method is only suitable for limited test environments. If you already provisioned a cluster or use other provisioning tools like Terraform, you can use AWS CLI to attach the policy to the Node IAM Role.","title":"Node IAM Role"},{"location":"tutorials/aws/#get-the-node-iam-role-name","text":"The role name of the role associated with the node(s) where ExternalDNS will run is needed. An easy way to get the role name is to use the AWS web console (https://console.aws.amazon.com/eks/), and find any instance in the target node group and copy the role name associated with that instance.","title":"Get the Node IAM role name"},{"location":"tutorials/aws/#get-role-name-with-a-single-managed-nodegroup","text":"From the command line, if you have a single managed node group, the default with eksctl create cluster , you can find the role name with the following: # get managed node group name (assuming there's only one node group) GROUP_NAME = $( aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME \\ --query nodegroups --out text ) # fetch role arn given node group name ROLE_ARN = $( aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME \\ --nodegroup-name $GROUP_NAME --query nodegroup.nodeRole --out text ) # extract just the name part of role arn ROLE_NAME = ${ NODE_ROLE_ARN ##*/ }","title":"Get role name with a single managed nodegroup"},{"location":"tutorials/aws/#get-role-name-with-other-configurations","text":"If you have multiple node groups or any unmanaged node groups, the process gets more complex. The first step is to get the instance host name of the desired node to where ExternalDNS will be deployed or is already deployed: # node instance name of one of the external dns pods currently running INSTANCE_NAME = $( kubectl get pods --all-namespaces \\ --selector app.kubernetes.io/instance = external-dns \\ --output jsonpath = '{.items[0].spec.nodeName}' ) # instance name of one of the nodes (change if node group is different) INSTANCE_NAME = $( kubectl get nodes --output name | cut -d '/' -f2 | tail -1 ) With the instance host name, you can then get the instance id: get_instance_id () { INSTANCE_NAME = $1 # example: ip-192-168-74-34.us-east-2.compute.internal # get list of nodes # ip-192-168-74-34.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx # ip-192-168-86-105.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx NODES = $( kubectl get nodes \\ --output jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.providerID}{\"\\n\"}{end}' ) # print instance id from matching node grep $INSTANCE_NAME <<< \" $NODES \" | cut -d '/' -f5 } INSTANCE_ID = $( get_instance_id $INSTANCE_NAME ) With the instance id, you can get the associated role name: findRoleName () { INSTANCE_ID = $1 # get all of the roles ROLES =( $( aws iam list-roles --query Roles [ * ] .RoleName --out text ) ) for ROLE in ${ ROLES [*] } ; do # get instance profile arn PROFILE_ARN = $( aws iam list-instance-profiles-for-role \\ --role-name $ROLE --query InstanceProfiles [ 0 ] .Arn --output text ) # if there is an instance profile if [[ \" $PROFILE_ARN \" ! = \"None\" ]] ; then # get all the instances with this associated instance profile INSTANCES = $( aws ec2 describe-instances \\ --filters Name = iam-instance-profile.arn,Values = $PROFILE_ARN \\ --query Reservations [ * ] .Instances [ 0 ] .InstanceId --out text ) # find instances that match the instant profile for INSTANCE in ${ INSTANCES [*] } ; do # set role name value if there is a match if [[ \" $INSTANCE_ID \" == \" $INSTANCE \" ]] ; then ROLE_NAME = $ROLE ; fi done fi done echo $ROLE_NAME } NODE_ROLE_NAME = $( findRoleName $INSTANCE_ID ) Using the role name, you can associate the policy that was created earlier: # attach policy arn created earlier to node IAM role aws iam attach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN WARNING : This will assign allow read-write access to all pods running on the same node pool, not just the ExternalDNS pod(s).","title":"Get role name with other configurations"},{"location":"tutorials/aws/#deploy-externaldns-with-attached-policy-to-node-iam-role","text":"If ExternalDNS is not yet deployed, follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC. NOTE : Before deleting the cluster during, be sure to run aws iam detach-role-policy . Otherwise, there can be errors as the provisioning system, such as eksctl or terraform , will not be able to delete the roles with the attached policy.","title":"Deploy ExternalDNS with attached policy to Node IAM Role"},{"location":"tutorials/aws/#static-credentials","text":"In this method, the policy is attached to an IAM user, and the credentials secrets for the IAM user are then made available using a Kubernetes secret. This method is not the preferred method as the secrets in the credential file could be copied and used by an unauthorized threat actor. However, if the Kubernetes cluster is not hosted on AWS, it may be the only method available. Given this situation, it is important to limit the associated privileges to just minimal required privileges, i.e. read-write access to Route53, and not used a credentials file that has extra privileges beyond what is required.","title":"Static credentials"},{"location":"tutorials/aws/#create-iam-user-and-attach-the-policy","text":"# create IAM user aws iam create-user --user-name \"externaldns\" # attach policy arn created earlier to IAM user aws iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN","title":"Create IAM user and attach the policy"},{"location":"tutorials/aws/#create-the-static-credentials","text":"SECRET_ACCESS_KEY = $( aws iam create-access-key --user-name \"externaldns\" ) cat <<-EOF > /local/path/to/credentials [default] aws_access_key_id = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId') aws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey') EOF","title":"Create the static credentials"},{"location":"tutorials/aws/#create-kubernetes-secret-from-credentials","text":"kubectl create secret generic external-dns \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } --from-file /local/path/to/credentials","title":"Create Kubernetes secret from credentials"},{"location":"tutorials/aws/#deploy-externaldns-using-static-credentials","text":"Follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC. Make sure to uncomment the section that mounts volumes, so that the credentials can be mounted.","title":"Deploy ExternalDNS using static credentials"},{"location":"tutorials/aws/#iam-roles-for-service-accounts","text":"IRSA ( IAM roles for Service Accounts ) allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts. This essentially allows only ExternalDNS pods to access Route53 without exposing any static credentials. This is the preferred method as it implements PoLP ( Principal of Least Privilege ). IMPORTANT : This method requires using KSA (Kuberntes service account) and RBAC. This method requires deploying with RBAC. See Manifest (for clusters with RBAC enabled) when ready to deploy ExternalDNS. NOTE : Similar methods to IRSA on AWS are kiam , which is in maintenence mode, and has instructions for creating an IAM role, and also kube2iam . IRSA is the officially supported method for EKS clusters, and so for non-EKS clusters on AWS, these other tools could be an option.","title":"IAM Roles for Service Accounts"},{"location":"tutorials/aws/#verify-oidc-is-supported","text":"aws eks describe-cluster --name $EKS_CLUSTER_NAME \\ --query \"cluster.identity.oidc.issuer\" --output text","title":"Verify OIDC is supported"},{"location":"tutorials/aws/#associate-oidc-to-cluster","text":"Configure the cluster with an OIDC provider and add support for IRSA ( IAM roles for Service Accounts ). If you used eksctl to provision the EKS cluster, you can update it with the following command: eksctl utils associate-iam-oidc-provider \\ --cluster $EKS_CLUSTER_NAME --approve If the cluster was provisioned with Terraform, you can use the iam_openid_connect_provider resource ( ref ) to associate to the OIDC provider.","title":"Associate OIDC to cluster"},{"location":"tutorials/aws/#create-an-iam-role-bound-to-a-service-account","text":"For the next steps in this process, we will need to associate the external-dns service account and a role used to grant access to Route53. This requires the following steps: Create a role with a trust relationship to the cluster\u2019s OIDC provider Attach the AllowExternalDNSUpdates policy to the role Create the external-dns service account Add annotation to the service account with the role arn","title":"Create an IAM role bound to a service account"},{"location":"tutorials/aws/#use-eksctl-with-eksctl-created-eks-cluster","text":"If eksctl was used to provision the EKS cluster, you can perform all of these steps with the following command: eksctl create iamserviceaccount \\ --cluster $EKS_CLUSTER_NAME \\ --name \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --attach-policy-arn $POLICY_ARN \\ --approve","title":"Use eksctl with eksctl created EKS cluster"},{"location":"tutorials/aws/#use-aws-cli-with-any-eks-cluster","text":"Otherwise, we can do the following steps using aws commands (also see Creating an IAM role and policy for your service account ): ACCOUNT_ID = $( aws sts get-caller-identity \\ --query \"Account\" --output text ) OIDC_PROVIDER = $( aws eks describe-cluster --name $EKS_CLUSTER_NAME \\ --query \"cluster.identity.oidc.issuer\" --output text | sed -e 's|^https://||' ) cat <<-EOF > trust.json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$ACCOUNT_ID:oidc-provider/$OIDC_PROVIDER\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"$OIDC_PROVIDER:sub\": \"system:serviceaccount:${EXTERNALDNS_NS:-\"default\"}:external-dns\", \"$OIDC_PROVIDER:aud\": \"sts.amazonaws.com\" } } } ] } EOF IRSA_ROLE = \"external-dns-irsa-role\" aws iam create-role --role-name $IRSA_ROLE --assume-role-policy-document file://trust.json aws iam attach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN ROLE_ARN = $( aws iam get-role --role-name $IRSA_ROLE --query Role.Arn --output text ) # Create service account (skip is already created) kubectl create serviceaccount \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } # Add annotation referencing IRSA role kubectl patch serviceaccount \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } --patch \\ \"{\\\"metadata\\\": { \\\"annotations\\\": { \\\"eks.amazonaws.com/role-arn\\\": \\\" $ROLE_ARN \\\" }}}\" If any part of this step is misconfigured, such as the role with incorrect namespace configured in the trust relationship, annotation pointing the the wrong role, etc., you will see errors like WebIdentityErr: failed to retrieve credentials . Check the configuration and make corrections. When the service account annotations are updated, then the current running pods will have to be terminated, so that new pod(s) with proper configuration (environment variables) will be created automatically. When annotation is added to service account, the ExternalDNS pod(s) scheduled will have AWS_ROLE_ARN , AWS_STS_REGIONAL_ENDPOINTS , and AWS_WEB_IDENTITY_TOKEN_FILE environment variables injected automatically.","title":"Use aws cli with any EKS cluster"},{"location":"tutorials/aws/#deploy-externaldns-using-irsa","text":"Follow the steps under Manifest (for clusters with RBAC enabled) . Make sure to comment out the service account section if this has been created already. If you deployed ExternalDNS before adding the service account annotation and the corresponding role, you will likely see error with failed to list hosted zones: AccessDenied: User . You can delete the current running ExternalDNS pod(s) after updating the annotation, so that new pods scheduled will have appropriate configuration to access Route53.","title":"Deploy ExternalDNS using IRSA"},{"location":"tutorials/aws/#set-up-a-hosted-zone","text":"If you prefer to try-out ExternalDNS in one of the existing hosted-zones you can skip this step Create a DNS zone which will contain the managed DNS records. This tutorial will use the fictional domain of example.com . aws route53 create-hosted-zone --name \"example.com.\" \\ --caller-reference \"external-dns-test- $( date +%s ) \" Make a note of the nameservers that were assigned to your new zone. ZONE_ID = $( aws route53 list-hosted-zones-by-name --output json \\ --dns-name \"example.com.\" --query HostedZones [ 0 ] .Id --out text ) aws route53 list-resource-record-sets --output text \\ --hosted-zone-id $ZONE_ID --query \\ \"ResourceRecordSets[?Type == 'NS'].ResourceRecords[*].Value | []\" | tr '\\t' '\\n' This should yield something similar this: ns-695.awsdns-22.net. ns-1313.awsdns-36.org. ns-350.awsdns-43.com. ns-1805.awsdns-33.co.uk. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the from the list above. Please consult your registrar\u2019s documentation on how to do that.","title":"Set up a hosted zone"},{"location":"tutorials/aws/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. You can check if your cluster has RBAC by kubectl api-versions | grep rbac.authorization.k8s.io . For clusters with RBAC enabled, be sure to choose the correct namespace . For this tutorial, the enviornment variable EXTERNALDNS_NS will refer to the namespace. You can set this to a value of your choice: export EXTERNALDNS_NS = \"default\" # externaldns, kube-addons, etc # create namespace if it does not yet exist kubectl get namespaces | grep -q $EXTERNALDNS_NS || \\ kubectl create namespace $EXTERNALDNS_NS","title":"Deploy ExternalDNS"},{"location":"tutorials/aws/#manifest-for-clusters-without-rbac-enabled","text":"Save the following below as externaldns-no-rbac.yaml . apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-hostedzone-identifier env : - name : AWS_DEFAULT_REGION value : us-east-1 # change to region where EKS is installed # # Uncomment below if using static credentials # - name: AWS_SHARED_CREDENTIALS_FILE # value: /.aws/credentials # volumeMounts: # - name: aws-credentials # mountPath: /.aws # readOnly: true # volumes: # - name: aws-credentials # secret: # secretName: external-dns When ready you can deploy: kubectl create --filename externaldns-no-rbac.yaml \\ --namespace ${ EXTERNALDNS_NS :- \"default\" }","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/aws/#manifest-for-clusters-with-rbac-enabled","text":"Save the following below as externaldns-with-rbac.yaml . # comment out sa if it was previously created apiVersion : v1 kind : ServiceAccount metadata : name : external-dns labels : app.kubernetes.io/name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns labels : app.kubernetes.io/name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer labels : app.kubernetes.io/name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default # change to desired namespace: externaldns, kube-addons --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=external-dns env : - name : AWS_DEFAULT_REGION value : us-east-1 # change to region where EKS is installed # # Uncommend below if using static credentials # - name: AWS_SHARED_CREDENTIALS_FILE # value: /.aws/credentials # volumeMounts: # - name: aws-credentials # mountPath: /.aws # readOnly: true # volumes: # - name: aws-credentials # secret: # secretName: external-dns When ready deploy: kubectl create --filename externaldns-with-rbac.yaml \\ --namespace ${ EXTERNALDNS_NS :- \"default\" }","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/aws/#arguments","text":"This list is not the full list, but a few arguments that where chosen.","title":"Arguments"},{"location":"tutorials/aws/#aws-zone-type","text":"aws-zone-type allows filtering for private and public zones","title":"aws-zone-type"},{"location":"tutorials/aws/#annotations","text":"Annotations which are specific to AWS.","title":"Annotations"},{"location":"tutorials/aws/#alias","text":"external-dns.alpha.kubernetes.io/alias if set to true on an ingress, it will create an ALIAS record when the target is an ALIAS as well. To make the target an alias, the ingress needs to be configured correctly as described in the docs . In particular, the argument --publish-service=default/nginx-ingress-controller has to be set on the nginx-ingress-controller container. If one uses the nginx-ingress Helm chart, this flag can be set with the controller.publishService.enabled configuration option.","title":"alias"},{"location":"tutorials/aws/#verify-externaldns-works-service-example","text":"Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. If you want to give multiple names to service, you can set it to external-dns.alpha.kubernetes.io/hostname with a comma , separator. For this verification phase, you can use default or another namespace for the nginx demo, for example: NGINXDEMO_NS = \"nginx\" kubectl get namespaces | grep -q $NGINXDEMO_NS || kubectl create namespace $NGINXDEMO_NS Save the following manifest below as nginx.yaml : apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http Deploy the nginx deployment and service with: kubectl create --filename nginx.yaml --namespace ${ NGINXDEMO_NS :- \"default\" } Verify that the load balancer was allocated with: kubectl get service nginx --namespace ${ NGINXDEMO_NS :- \"default\" } This should show something like: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx LoadBalancer 10 .100.47.41 ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. 80 :32749/TCP 12m After roughly two minutes check that a corresponding DNS record for your service that was created. aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'A']\" This should show something like: [ { \"Name\" : \"nginx.example.com.\" , \"Type\" : \"A\" , \"AliasTarget\" : { \"HostedZoneId\" : \"ZEWFWZ4R16P7IB\" , \"DNSName\" : \"ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.\" , \"EvaluateTargetHealth\" : true } } ] You can also fetch the corresponding text records: aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'TXT']\" This will show something like: [ { \"Name\" : \"nginx.example.com.\" , \"Type\" : \"TXT\" , \"TTL\" : 300 , \"ResourceRecords\" : [ { \"Value\" : \"\\\"heritage=external-dns,external-dns/owner=external-dns,external-dns/resource=service/default/nginx\\\"\" } ] } ] Note created TXT record alongside ALIAS record. TXT record signifies that the corresponding ALIAS record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. For more information about ALIAS record, see Choosing between alias and non-alias records . Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. dig +short @ns-5514.awsdns-53.org. nginx.example.com. This should return 1+ IP addresses that correspond to the ELB FQDN, i.e. ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. . Next try the public nameservers configured by DNS client on your system: dig +short nginx.example.com. If you hooked up your DNS zone with its parent zone correctly you can use curl to access your site. curl nginx.example.com. This should show something like: <!DOCTYPE html> < html > < head > < title > Welcome to nginx! </ title > ... </ head > < body > < h1 > Welcome to nginx! </ h1 > ... </ body > </ html >","title":"Verify ExternalDNS works (Service example)"},{"location":"tutorials/aws/#verify-externaldns-works-ingress-example","text":"With the previous deployment and service objects deployed, we can add an ingress object and configure a FQDN value for the host key. The ingress controller will match incoming HTTP traffic, and route it to the appropriate backend service based on the host key. For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object. For this tutorial, we have two endpoints, the service with LoadBalancer type and an ingress. For practical purposes, if an ingress is used, the service type can be changed to ClusterIP as two endpoints are unecessary in this scenario. IMPORTANT : This requires that an ingress controller has been installed in your Kubernetes cluster. EKS does not come with an ingress controller by default. A popular ingress controller is ingress-nginx , which can be installed by a helm chart or by manifests . Create an ingress resource manifest file named ingress.yaml with the contents below: --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : \"nginx\" # use the one that corresponds to your ingress controller. spec : rules : - host : server.example.com http : paths : - backend : service : name : nginx port : number : 80 path : / pathType : Prefix When ready, you can deploy this with: kubectl create --filename ingress.yaml --namespace ${ NGINXDEMO_NS :- \"default\" } Watch the status of the ingress until the ADDRESS field is populated. kubectl get ingress --watch --namespace ${ NGINXDEMO_NS :- \"default\" } You should see something like this: NAME CLASS HOSTS ADDRESS PORTS AGE nginx <none> server.example.com 80 47s nginx <none> server.example.com ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com. 80 54s For the ingress test, run through similar checks, but using domain name used for the ingress: # check records on route53 aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\ --query \"ResourceRecordSets[?Name == 'server.example.com.']\" # query using a route53 name server dig +short @ns-5514.awsdns-53.org. server.example.com. # query using the default name server dig +short server.example.com. # connect to the nginx web server through the ingress curl server.example.com.","title":"Verify ExternalDNS works (Ingress example)"},{"location":"tutorials/aws/#more-service-annotation-options","text":"","title":"More service annotation options"},{"location":"tutorials/aws/#custom-ttl","text":"The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.example.com external-dns.alpha.kubernetes.io/ttl : \"60\" spec : ... This will set the DNS record\u2019s TTL to 60 seconds.","title":"Custom TTL"},{"location":"tutorials/aws/#routing-policies","text":"Route53 offers different routing policies . The routing policy for a record can be controlled with the following annotations: external-dns.alpha.kubernetes.io/set-identifier : this needs to be set to use any of the following routing policies For any given DNS name, only one of the following routing policies can be used: Weighted records: external-dns.alpha.kubernetes.io/aws-weight Latency-based routing: external-dns.alpha.kubernetes.io/aws-region Failover: external-dns.alpha.kubernetes.io/aws-failover Geolocation-based routing: external-dns.alpha.kubernetes.io/aws-geolocation-continent-code external-dns.alpha.kubernetes.io/aws-geolocation-country-code external-dns.alpha.kubernetes.io/aws-geolocation-subdivision-code Multi-value answer: external-dns.alpha.kubernetes.io/aws-multi-value-answer","title":"Routing policies"},{"location":"tutorials/aws/#associating-dns-records-with-healthchecks","text":"You can configure Route53 to associate DNS records with healthchecks for automated DNS failover using external-dns.alpha.kubernetes.io/aws-health-check-id: <health-check-id> annotation. Note: ExternalDNS does not support creating healthchecks, and assumes that <health-check-id> already exists.","title":"Associating DNS records with healthchecks"},{"location":"tutorials/aws/#govcloud-caveats","text":"Due to the special nature with how Route53 runs in Govcloud, there are a few tweaks in the deployment settings. An Environment variable with name of AWS_REGION set to either us-gov-west-1 or us-gov-east-1 is required. Otherwise it tries to lookup a region that does not exist in Govcloud and it errors out. env : - name : AWS_REGION value : us-gov-west-1 Route53 in Govcloud does not allow aliases. Therefore, container args must be set so that it uses CNAMES and a txt-prefix must be set to something. Otherwise, it will try to create a TXT record with the same value than the CNAME itself, which is not allowed. args : - --aws-prefer-cname - --txt-prefix={{ YOUR_PREFIX }} The first two changes are needed if you use Route53 in Govcloud, which only supports private zones. There are also no cross account IAM whatsoever between Govcloud and commerical AWS accounts. If services and ingresses need to make Route 53 entries to an public zone in a commerical account, you will have set env variables of AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with a key and secret to the commerical account that has the sufficient rights. env : - name : AWS_ACCESS_KEY_ID value : XXXXXXXXX - name : AWS_SECRET_ACCESS_KEY valueFrom : secretKeyRef : name : {{ YOUR_SECRET_NAME }} key : {{ YOUR_SECRET_KEY }}","title":"Govcloud caveats"},{"location":"tutorials/aws/#clean-up","text":"Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly. kubectl delete service nginx IMPORTANT If you attached a policy to the Node IAM Role, then you will want to detach this before deleting the EKS cluster. Otherwise, the role resource will be locked, and the cluster cannot be deleted, especially if it was provisioned by automation like terraform or eksctl . aws iam detach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN If the cluster was provisioned using eksctl , you can delete the cluster with: eksctl delete cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose. aws route53 delete-hosted-zone --id $NODE_ID # e.g /hostedzone/ZEWFWZ4R16P7IB If IAM user credentials were used, you can remove the user with: aws iam detach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN aws iam delete-user --user-name \"externaldns\" If IRSA was used, you can remove the IRSA role with: aws iam detach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN aws iam delete-role --role-name $IRSA_ROLE Delete any unneeded policies: aws iam delete-policy --policy-arn $POLICY_ARN","title":"Clean up"},{"location":"tutorials/aws/#throttling","text":"Route53 has a 5 API requests per second per account hard quota . Running several fast polling ExternalDNS instances in a given account can easily hit that limit. Some ways to reduce the request rate include: * Reduce the polling loop\u2019s synchronization interval at the possible cost of slower change propagation (but see --events below to reduce the impact). * --interval=5m (default 1m ) * Trigger the polling loop on changes to K8s objects, rather than only at interval , to have responsive updates with long poll intervals * --events * Limit the sources watched when the --events flag is specified to specific types, namespaces, labels, or annotations * --source=ingress --source=service - specify multiple times for multiple sources * --namespace=my-app * --label-filter=app in (my-app) * --annotation-filter=kubernetes.io/ingress.class in (nginx-external) - note that this filter would apply to services too.. * Limit services watched by type (not applicable to ingress or other types) * --service-type-filter=LoadBalancer default all * Limit the hosted zones considered * --zone-id-filter=ABCDEF12345678 - specify multiple times if needed * --domain-filter=example.com by domain suffix - specify multiple times if needed * --regex-domain-filter=example* by domain suffix but as a regex - overrides domain-filter * --exclude-domains=ignore.this.example.com to exclude a domain or subdomain * --regex-domain-exclusion=ignore* subtracts it\u2019s matches from regex-domain-filter \u2018s matches * --aws-zone-type=public only sync zones of this type [public|private] * --aws-zone-tags=owner=k8s only sync zones with this tag * If the list of zones managed by ExternalDNS doesn\u2019t change frequently, cache it by setting a TTL. * --aws-zones-cache-duration=3h (default 0 - disabled) * Increase the number of changes applied to Route53 in each batch * --aws-batch-change-size=4000 (default 1000 ) * Increase the interval between changes * --aws-batch-change-interval=10s (default 1s ) * Introducing some jitter to the pod initialization, so that when multiple instances of ExternalDNS are updated at the same time they do not make their requests on the same second. A simple way to implement randomised startup is with an init container: ... spec: initContainers: - name: init-jitter image: registry.k8s.io/external-dns/external-dns:v0.13.1 command: - /bin/sh - -c - 'FOR=$((RANDOM % 10))s;echo \"Sleeping for $FOR\";sleep $FOR' containers: ...","title":"Throttling"},{"location":"tutorials/aws/#eks","text":"An effective starting point for EKS with an ingress controller might look like: --interval = 5m --events --source = ingress --domain-filter = example.com --aws-zones-cache-duration = 1h","title":"EKS"},{"location":"tutorials/azure-private-dns/","text":"Set up ExternalDNS for Azure Private DNS \u00b6 This tutorial describes how to set up ExternalDNS for managing records in Azure Private DNS. It comprises of the following steps: 1) Install NGINX Ingress Controller 2) Provision Azure Private DNS 3) Configure service principal for managing the zone 4) Deploy ExternalDNS Everything will be deployed on Kubernetes. Therefore, please see the subsequent prerequisites. Prerequisites \u00b6 Azure Kubernetes Service is deployed and ready Azure CLI 2.0 and kubectl installed on the box to execute the subsequent steps Install NGINX Ingress Controller \u00b6 Helm is used to deploy the ingress controller. We employ the popular chart ingress-nginx . $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ helm install [RELEASE_NAME] ingress-nginx/ingress-nginx --set controller.publishService.enabled=true The parameter controller.publishService.enabled needs to be set to true. It will make the ingress controller update the endpoint records of ingress-resources to contain the external-ip of the loadbalancer serving the ingress-controller. This is crucial as ExternalDNS reads those endpoints records when creating DNS-Records from ingress-resources. In the subsequent parameter we will make use of this. If you don\u2019t want to work with ingress-resources in your later use, you can leave the parameter out. Verify the correct propagation of the loadbalancer\u2019s ip by listing the ingresses. $ kubectl get ingress The address column should contain the ip for each ingress. ExternalDNS will pick up exactly this piece of information. NAME HOSTS ADDRESS PORTS AGE nginx1 sample1.aks.com 52.167.195.110 80 6d22h nginx2 sample2.aks.com 52.167.195.110 80 6d21h If you do not want to deploy the ingress controller with Helm, ensure to pass the following cmdline-flags to it through the mechanism of your choice: flags: --publish-service=<namespace of ingress-controller >/<svcname of ingress-controller> --update-status=true (default-value) example: ./nginx-ingress-controller --publish-service=default/nginx-ingress-controller Provision Azure Private DNS \u00b6 The provider will find suitable zones for domains it manages. It will not automatically create zones. For this tutorial, we will create a Azure resource group named \u2018externaldns\u2019 that can easily be deleted later. $ az group create -n externaldns -l westeurope Substitute a more suitable location for the resource group if desired. As a prerequisite for Azure Private DNS to resolve records is to define links with VNETs. Thus, first create a VNET. $ az network vnet create \\ --name myvnet \\ --resource-group externaldns \\ --location westeurope \\ --address-prefix 10.2.0.0/16 \\ --subnet-name mysubnet \\ --subnet-prefixes 10.2.0.0/24 Next, create a Azure Private DNS zone for \u201cexample.com\u201d: $ az network private-dns zone create -g externaldns -n example.com Substitute a domain you own for \u201cexample.com\u201d if desired. Finally, create the mentioned link with the VNET. $ az network private-dns link vnet create -g externaldns -n mylink \\ -z example.com -v myvnet --registration-enabled false Configure service principal for managing the zone \u00b6 ExternalDNS needs permissions to make changes in Azure Private DNS. These permissions are roles assigned to the service principal used by ExternalDNS. A service principal with a minimum access level of Private DNS Zone Contributor to the Private DNS zone(s) and Reader to the resource group containing the Azure Private DNS zone(s) is necessary. More powerful role-assignments like Owner or assignments on subscription-level work too. Start off by creating the service principal without role-assignments. $ az ad sp create-for-rbac --skip-assignment -n http://externaldns-sp { \"appId\": \"appId GUID\", <-- aadClientId value ... \"password\": \"password\", <-- aadClientSecret value \"tenant\": \"AzureAD Tenant Id\" <-- tenantId value } Note: Alternatively, you can issue az account show --query \"tenantId\" to retrieve the id of your AAD Tenant too. Next, assign the roles to the service principal. But first retrieve the ID\u2019s of the objects to assign roles on. # find out the resource ids of the resource group where the dns zone is deployed, and the dns zone itself $ az group show --name externaldns --query id -o tsv /subscriptions/id/resourceGroups/externaldns $ az network private-dns zone show --name example.com -g externaldns --query id -o tsv /subscriptions/.../resourceGroups/externaldns/providers/Microsoft.Network/privateDnsZones/example.com Now, create role assignments . # 1. as a reader to the resource group $ az role assignment create --role \"Reader\" --assignee <appId GUID> --scope <resource group resource id> # 2. as a contributor to DNS Zone itself $ az role assignment create --role \"Private DNS Zone Contributor\" --assignee <appId GUID> --scope <dns zone resource id> Deploy ExternalDNS \u00b6 Configure kubectl to be able to communicate and authenticate with your cluster. This is per default done through the file ~/.kube/config . For general background information on this see kubernetes-docs . Azure-CLI features functionality for automatically maintaining this file for AKS-Clusters. See Azure-Docs . Follow the steps for azure-dns provider to create a configuration file. Then apply one of the following manifests depending on whether you use RBAC or not. The credentials of the service principal are provided to ExternalDNS as environment-variables. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Manifest (for clusters with RBAC enabled, cluster access) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : externaldns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : externaldns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : externaldns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : externaldns subjects : - kind : ServiceAccount name : externaldns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : serviceAccountName : externaldns containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Manifest (for clusters with RBAC enabled, namespace access) \u00b6 This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type NodePort will be skipped! apiVersion : v1 kind : ServiceAccount metadata : name : externaldns --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : externaldns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : externaldns roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : externaldns subjects : - kind : ServiceAccount name : externaldns --- apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : serviceAccountName : externaldns containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml Deploying sample service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx-svc spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : nginx type : ClusterIP --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : server.example.com http : paths : - backend : service : name : nginx-svc port : number : 80 pathType : Prefix When using ExternalDNS with ingress objects it will automatically create DNS records based on host names specified in ingress objects that match the domain-filter argument in the externaldns deployment manifest. When those host names are removed or renamed the corresponding DNS records are also altered. Create the deployment, service and ingress object: $ kubectl create -f nginx.yaml Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute. Verify created records \u00b6 Run the following command to view the A records for your Azure Private DNS zone: $ az network private-dns record-set a list -g externaldns -z example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).","title":"Set up ExternalDNS for Azure Private DNS"},{"location":"tutorials/azure-private-dns/#set-up-externaldns-for-azure-private-dns","text":"This tutorial describes how to set up ExternalDNS for managing records in Azure Private DNS. It comprises of the following steps: 1) Install NGINX Ingress Controller 2) Provision Azure Private DNS 3) Configure service principal for managing the zone 4) Deploy ExternalDNS Everything will be deployed on Kubernetes. Therefore, please see the subsequent prerequisites.","title":"Set up ExternalDNS for Azure Private DNS"},{"location":"tutorials/azure-private-dns/#prerequisites","text":"Azure Kubernetes Service is deployed and ready Azure CLI 2.0 and kubectl installed on the box to execute the subsequent steps","title":"Prerequisites"},{"location":"tutorials/azure-private-dns/#install-nginx-ingress-controller","text":"Helm is used to deploy the ingress controller. We employ the popular chart ingress-nginx . $ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx $ helm repo update $ helm install [RELEASE_NAME] ingress-nginx/ingress-nginx --set controller.publishService.enabled=true The parameter controller.publishService.enabled needs to be set to true. It will make the ingress controller update the endpoint records of ingress-resources to contain the external-ip of the loadbalancer serving the ingress-controller. This is crucial as ExternalDNS reads those endpoints records when creating DNS-Records from ingress-resources. In the subsequent parameter we will make use of this. If you don\u2019t want to work with ingress-resources in your later use, you can leave the parameter out. Verify the correct propagation of the loadbalancer\u2019s ip by listing the ingresses. $ kubectl get ingress The address column should contain the ip for each ingress. ExternalDNS will pick up exactly this piece of information. NAME HOSTS ADDRESS PORTS AGE nginx1 sample1.aks.com 52.167.195.110 80 6d22h nginx2 sample2.aks.com 52.167.195.110 80 6d21h If you do not want to deploy the ingress controller with Helm, ensure to pass the following cmdline-flags to it through the mechanism of your choice: flags: --publish-service=<namespace of ingress-controller >/<svcname of ingress-controller> --update-status=true (default-value) example: ./nginx-ingress-controller --publish-service=default/nginx-ingress-controller","title":"Install NGINX Ingress Controller"},{"location":"tutorials/azure-private-dns/#provision-azure-private-dns","text":"The provider will find suitable zones for domains it manages. It will not automatically create zones. For this tutorial, we will create a Azure resource group named \u2018externaldns\u2019 that can easily be deleted later. $ az group create -n externaldns -l westeurope Substitute a more suitable location for the resource group if desired. As a prerequisite for Azure Private DNS to resolve records is to define links with VNETs. Thus, first create a VNET. $ az network vnet create \\ --name myvnet \\ --resource-group externaldns \\ --location westeurope \\ --address-prefix 10.2.0.0/16 \\ --subnet-name mysubnet \\ --subnet-prefixes 10.2.0.0/24 Next, create a Azure Private DNS zone for \u201cexample.com\u201d: $ az network private-dns zone create -g externaldns -n example.com Substitute a domain you own for \u201cexample.com\u201d if desired. Finally, create the mentioned link with the VNET. $ az network private-dns link vnet create -g externaldns -n mylink \\ -z example.com -v myvnet --registration-enabled false","title":"Provision Azure Private DNS"},{"location":"tutorials/azure-private-dns/#configure-service-principal-for-managing-the-zone","text":"ExternalDNS needs permissions to make changes in Azure Private DNS. These permissions are roles assigned to the service principal used by ExternalDNS. A service principal with a minimum access level of Private DNS Zone Contributor to the Private DNS zone(s) and Reader to the resource group containing the Azure Private DNS zone(s) is necessary. More powerful role-assignments like Owner or assignments on subscription-level work too. Start off by creating the service principal without role-assignments. $ az ad sp create-for-rbac --skip-assignment -n http://externaldns-sp { \"appId\": \"appId GUID\", <-- aadClientId value ... \"password\": \"password\", <-- aadClientSecret value \"tenant\": \"AzureAD Tenant Id\" <-- tenantId value } Note: Alternatively, you can issue az account show --query \"tenantId\" to retrieve the id of your AAD Tenant too. Next, assign the roles to the service principal. But first retrieve the ID\u2019s of the objects to assign roles on. # find out the resource ids of the resource group where the dns zone is deployed, and the dns zone itself $ az group show --name externaldns --query id -o tsv /subscriptions/id/resourceGroups/externaldns $ az network private-dns zone show --name example.com -g externaldns --query id -o tsv /subscriptions/.../resourceGroups/externaldns/providers/Microsoft.Network/privateDnsZones/example.com Now, create role assignments . # 1. as a reader to the resource group $ az role assignment create --role \"Reader\" --assignee <appId GUID> --scope <resource group resource id> # 2. as a contributor to DNS Zone itself $ az role assignment create --role \"Private DNS Zone Contributor\" --assignee <appId GUID> --scope <dns zone resource id>","title":"Configure service principal for managing the zone"},{"location":"tutorials/azure-private-dns/#deploy-externaldns","text":"Configure kubectl to be able to communicate and authenticate with your cluster. This is per default done through the file ~/.kube/config . For general background information on this see kubernetes-docs . Azure-CLI features functionality for automatically maintaining this file for AKS-Clusters. See Azure-Docs . Follow the steps for azure-dns provider to create a configuration file. Then apply one of the following manifests depending on whether you use RBAC or not. The credentials of the service principal are provided to ExternalDNS as environment-variables.","title":"Deploy ExternalDNS"},{"location":"tutorials/azure-private-dns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/azure-private-dns/#manifest-for-clusters-with-rbac-enabled-cluster-access","text":"apiVersion : v1 kind : ServiceAccount metadata : name : externaldns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : externaldns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : externaldns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : externaldns subjects : - kind : ServiceAccount name : externaldns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : serviceAccountName : externaldns containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file","title":"Manifest (for clusters with RBAC enabled, cluster access)"},{"location":"tutorials/azure-private-dns/#manifest-for-clusters-with-rbac-enabled-namespace-access","text":"This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type NodePort will be skipped! apiVersion : v1 kind : ServiceAccount metadata : name : externaldns --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : externaldns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : externaldns roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : externaldns subjects : - kind : ServiceAccount name : externaldns --- apiVersion : apps/v1 kind : Deployment metadata : name : externaldns spec : selector : matchLabels : app : externaldns strategy : type : Recreate template : metadata : labels : app : externaldns spec : serviceAccountName : externaldns containers : - name : externaldns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com - --provider=azure-private-dns - --azure-resource-group=externaldns - --azure-subscription-id=<use the id of your subscription> volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml","title":"Manifest (for clusters with RBAC enabled, namespace access)"},{"location":"tutorials/azure-private-dns/#deploying-sample-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx-svc spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : nginx type : ClusterIP --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : server.example.com http : paths : - backend : service : name : nginx-svc port : number : 80 pathType : Prefix When using ExternalDNS with ingress objects it will automatically create DNS records based on host names specified in ingress objects that match the domain-filter argument in the externaldns deployment manifest. When those host names are removed or renamed the corresponding DNS records are also altered. Create the deployment, service and ingress object: $ kubectl create -f nginx.yaml Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute.","title":"Deploying sample service"},{"location":"tutorials/azure-private-dns/#verify-created-records","text":"Run the following command to view the A records for your Azure Private DNS zone: $ az network private-dns record-set a list -g externaldns -z example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).","title":"Verify created records"},{"location":"tutorials/azure/","text":"Setting up ExternalDNS for Services on Azure \u00b6 This tutorial describes how to setup ExternalDNS for Azure DNS with Azure Kubernetes Service . Make sure to use >=0.11.0 version of ExternalDNS for this tutorial. This tutorial uses Azure CLI 2.0 for all Azure commands and assumes that the Kubernetes cluster was created via Azure Container Services and kubectl commands are being run on an orchestration node. Creating an Azure DNS zone \u00b6 The Azure provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. For this tutorial, we will create a Azure resource group named MyDnsResourceGroup that can easily be deleted later: $ az group create --name \"MyDnsResourceGroup\" --location \"eastus\" Substitute a more suitable location for the resource group if desired. Next, create a Azure DNS zone for example.com : $ az network dns zone create --resource-group \"MyDnsResourceGroup\" --name \"example.com\" Substitute a domain you own for example.com if desired. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the nameServers field from the JSON data returned by the az network dns zone create command. Please consult your registrar\u2019s documentation on how to do that. Configuration file \u00b6 The azure provider will reference a configuration file called azure.json . The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named azure.json with content similar to this: { \"tenantId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"subscriptionId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"resourceGroup\" : \"MyDnsResourceGroup\" , \"aadClientId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"aadClientSecret\" : \"uKiuXeiwui4jo9quae9o\" } The following fields are used: tenantId ( required ) - run az account show --query \"tenantId\" or by selecting Azure Active Directory in the Azure Portal and checking the Directory ID under Properties. subscriptionId ( required ) - run az account show --query \"id\" or by selecting Subscriptions in the Azure Portal. resourceGroup ( required ) is the Resource Group created in a previous step that contains the Azure DNS Zone. aadClientID and aaClientSecret are associated with the Service Principal. This is only used with Service Principal method documented in the next section. useManagedIdentityExtension - this is set to true if you use either AKS Kubelet Identity or AAD Pod Identities methods documented in the next section. userAssignedIdentityID - this contains the client id from the Managed identitty when using the AAD Pod Identities method documented in the next setion. The Azure DNS provider expects, by default, that the configuration file is at /etc/kubernetes/azure.json . This can be overridden with the --azure-config-file option when starting ExternalDNS. Permissions to modify DNS zone \u00b6 ExternalDNS needs permissions to make changes to the Azure DNS zone. There are three ways configure the access needed: Service Principal Managed Identity Using AKS Kubelet Identity Managed Identity Using AAD Pod Identities Service Principal \u00b6 These permissions are defined in a Service Principal that should be made available to ExternalDNS as a configuration file azure.json . Creating a service principal \u00b6 A Service Principal with a minimum access level of DNS Zone Contributor or Contributor to the DNS zone(s) and Reader to the resource group containing the Azure DNS zone(s) is necessary for ExternalDNS to be able to edit DNS records. However, other more permissive access levels will work too (e.g. Contributor to the resource group or the whole subscription). This is an Azure CLI example on how to query the Azure API for the information required for the Resource Group and DNS zone you would have already created in previous steps (requires azure-cli and jq ) $ EXTERNALDNS_NEW_SP_NAME = \"ExternalDnsServicePrincipal\" # name of the service principal $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # name of resource group where dns zone is hosted $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com # Create the service principal $ DNS_SP = $( az ad sp create-for-rbac --name $EXTERNALDNS_NEW_SP_NAME ) $ EXTERNALDNS_SP_APP_ID = $( echo $DNS_SP | jq -r '.appId' ) $ EXTERNALDNS_SP_PASSWORD = $( echo $DNS_SP | jq -r '.password' ) Assign the rights for the service principal \u00b6 Grant access to Azure DNS zone for the service principal. # fetch DNS id used to grant access to the service principal DNS_ID = $( az network dns zone show --name $AZURE_DNS_ZONE \\ --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv ) # 1. as a reader to the resource group $ az role assignment create --role \"Reader\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID # 2. as a contributor to DNS Zone itself $ az role assignment create --role \"Contributor\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID Creating a configuration file for the service principal \u00b6 Create the file azure.json with values gather from previous steps. cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"aadClientId\": \"$EXTERNALDNS_SP_APP_ID\", \"aadClientSecret\": \"$EXTERNALDNS_SP_PASSWORD\" } EOF Use this file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json Managed identity using AKS Kubelet identity \u00b6 The managed identity that is assigned to the underlying node pool in the AKS cluster can be given permissions to access Azure DNS. Managed identities are essentially a service principal whose lifecycle is managed, such as deleting the AKS cluster will also delete the service principals associated with the AKS cluster. The managed identity assigned Kuberetes node pool, or specifically the VMSS , is called the Kubelet identity. The managed identites were previously called MSI (Managed Service Identity) and are enabled by default when creating an AKS cluster. Note that permissions granted to this identity will be accessible to all containers running inside the Kubernetes cluster, not just the ExternalDNS container(s). For the managed identity, the contents of azure.json should be similar to this: { \"tenantId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"subscriptionId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"resourceGroup\" : \"MyDnsResourceGroup\" , \"useManagedIdentityExtension\" : true } Fetching the Kubelet identity \u00b6 For this process, you will need to get the kublet identity: $ PRINCIPAL_ID = $( az aks show --resource-group $CLUSTER_GROUP --name $CLUSTERNAME \\ --query \"identityProfile.kubeletidentity.objectId\" --output tsv ) Assign rights for the Kubelet identity \u00b6 Grant access to Azure DNS zone for the kublet identity. $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # resource group where DNS zone is hosted # fetch DNS id used to grant access to the kublet identity $ DNS_ID = $( az network dns zone show --name $AZURE_DNS_ZONE \\ --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv ) $ az role assignment create --role \"DNS Zone Contributor\" --assignee $PRINCIPAL_ID --scope $DNS_ID Creating a configuration file for the kubelet identity \u00b6 Create the file azure.json with values gather from previous steps. cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"useManagedIdentityExtension\": true } EOF Use the azure.json file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json Managed identity using AAD Pod Identities \u00b6 For this process, we will create a managed identity that will be explicitly used by the ExternalDNS container. This process is similar to Kubelet identity except that this managed identity is not associated with the Kubernetes node pool, but rather associated with explicit ExternalDNS containers. Enable the AAD Pod Identities feature \u00b6 For this solution, AAD Pod Identities preview feature can be enabled. The commands below should do the trick to enable this feature: $ az feature register --name EnablePodIdentityPreview --namespace Microsoft.ContainerService $ az feature register --name AutoUpgradePreview --namespace Microsoft.ContainerService $ az extension add --name aks-preview $ az extension update --name aks-preview $ az provider register --namespace Microsoft.ContainerService Deploy the AAD Pod Identities service \u00b6 Once enabled, you can update your cluster and install needed services for the AAD Pod Identities feature. $ AZURE_AKS_RESOURCE_GROUP = \"my-aks-cluster-group\" # name of resource group where aks cluster was created $ AZURE_AKS_CLUSTER_NAME = \"my-aks-cluster\" # name of aks cluster previously created $ az aks update --resource-group ${ AZURE_AKS_RESOURCE_GROUP } --name ${ AZURE_AKS_CLUSTER_NAME } --enable-pod-identity Note that, if you use the default network plugin kubenet , then you need to add the command line option --enable-pod-identity-with-kubenet to the above command. Creating the managed identity \u00b6 After this process is finished, create a managed identity. $ IDENTITY_RESOURCE_GROUP = $AZURE_AKS_RESOURCE_GROUP # custom group or reuse AKS group $ IDENTITY_NAME = \"example-com-identity\" # create a managed identity $ az identity create --resource-group \" ${ IDENTITY_RESOURCE_GROUP } \" --name \" ${ IDENTITY_NAME } \" Assign rights for the managed identity \u00b6 Grant access to Azure DNS zone for the managed identity. $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # name of resource group where dns zone is hosted $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com # fetch identity client id from managed identity created earlier $ IDENTITY_CLIENT_ID = $( az identity show --resource-group \" ${ IDENTITY_RESOURCE_GROUP } \" \\ --name \" ${ IDENTITY_NAME } \" --query \"clientId\" --output tsv ) # fetch DNS id used to grant access to the managed identity $ DNS_ID = $( az network dns zone show --name \" ${ AZURE_DNS_ZONE } \" \\ --resource-group \" ${ AZURE_DNS_ZONE_RESOURCE_GROUP } \" --query \"id\" --output tsv ) $ az role assignment create --role \"DNS Zone Contributor\" \\ --assignee \" ${ IDENTITY_CLIENT_ID } \" --scope \" ${ DNS_ID } \" Creating a configuration file for the managed identity \u00b6 Create the file azure.json with the values from previous steps: cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"useManagedIdentityExtension\": true, \"userAssignedIdentityID\": \"$IDENTITY_CLIENT_ID\" } EOF Use the azure.json file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json Creating an Azure identity binding \u00b6 A binding between the managed identity and the ExternalDNS pods needs to be setup by creating AzureIdentity and AzureIdentityBinding resources. This will allow appropriately labeled ExternalDNS pods to authenticate using the managed identity. When AAD Pod Identity feature is enabled from previous steps above, the az aks pod-identity add can be used to create these resources: $ IDENTITY_RESOURCE_ID = $( az identity show --resource-group ${ IDENTITY_RESOURCE_GROUP } \\ --name ${ IDENTITY_NAME } --query id --output tsv ) $ az aks pod-identity add --resource-group ${ AZURE_AKS_RESOURCE_GROUP } \\ --cluster-name ${ AZURE_AKS_CLUSTER_NAME } --namespace \"default\" \\ --name \"external-dns\" --identity-resource-id ${ IDENTITY_RESOURCE_ID } This will add something similar to the following resouces: apiVersion : aadpodidentity.k8s.io/v1 kind : AzureIdentity metadata : labels : addonmanager.kubernetes.io/mode : Reconcile kubernetes.azure.com/managedby : aks name : external-dns spec : clientID : $IDENTITY_CLIENT_ID resourceID : $IDENTITY_RESOURCE_ID type : 0 --- apiVersion : aadpodidentity.k8s.io/v1 kind : AzureIdentityBinding metadata : annotations : labels : addonmanager.kubernetes.io/mode : Reconcile kubernetes.azure.com/managedby : aks name : external-dns-binding spec : azureIdentity : external-dns selector : external-dns Update ExternalDNS labels \u00b6 When deploying ExternalDNS, you want to make sure that deployed pod(s) will have the label aadpodidbinding: external-dns to enable AAD Pod Identities. You can patch an existing deployment of ExternalDNS with this command: kubectl patch deployment external-dns --namespace \"default\" --patch \\ '{\"spec\": {\"template\": {\"metadata\": {\"labels\": {\"aadpodidbinding\": \"external-dns\"}}}}}' Ingress used with ExternalDNS \u00b6 This deployment assumes that you will be using nginx-ingress. When using nginx-ingress do not deploy it as a Daemon Set. This causes nginx-ingress to write the Cluster IP of the backend pods in the ingress status.loadbalancer.ip property which then has external-dns write the Cluster IP(s) in DNS vs. the nginx-ingress service external IP. Ensure that your nginx-ingress deployment has the following arg: added to it: - --publish-service=namespace/nginx-ingress-controller-svcname For more details see here: nginx-ingress external-dns Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. The deployment assumes that ExternalDNS will be installed into the default namespace. If this namespace is different, the ClusterRoleBinding will need to be updated to reflect the desired alternative namespace, such as external-dns , kube-addons , etc. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Manifest (for clusters with RBAC enabled, cluster access) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group - --txt-prefix=externaldns- volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Manifest (for clusters with RBAC enabled, namespace access) \u00b6 This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type NodePort will be skipped! apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : external-dns subjects : - kind : ServiceAccount name : external-dns --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Create the deployment for ExternalDNS: $ kubectl create --namespace \"default\" --filename externaldns.yaml Deploying an Nginx Service \u00b6 Create a file called nginx.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx-svc spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : nginx type : ClusterIP --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : server.example.com http : paths : - path : / pathType : Prefix backend : service : name : nginx-svc port : number : 80 When using ExternalDNS with ingress objects it will automatically create DNS records based on host names specified in ingress objects that match the domain-filter argument in the external-dns deployment manifest. When those host names are removed or renamed the corresponding DNS records are also altered. Create the deployment, service and ingress object: $ kubectl create --namespace \"default\" --filename nginx.yaml Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute. Verifying Azure DNS records \u00b6 Run the following command to view the A records for your Azure DNS zone: $ az network dns record-set a list --resource-group \"MyDnsResourceGroup\" --zone-name example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself). Delete Azure Resource Group \u00b6 Now that we have verified that ExternalDNS will automatically manage Azure DNS records, we can delete the tutorial\u2019s resource group: $ az group delete --name \"MyDnsResourceGroup\"","title":"Setting up ExternalDNS for Services on Azure"},{"location":"tutorials/azure/#setting-up-externaldns-for-services-on-azure","text":"This tutorial describes how to setup ExternalDNS for Azure DNS with Azure Kubernetes Service . Make sure to use >=0.11.0 version of ExternalDNS for this tutorial. This tutorial uses Azure CLI 2.0 for all Azure commands and assumes that the Kubernetes cluster was created via Azure Container Services and kubectl commands are being run on an orchestration node.","title":"Setting up ExternalDNS for Services on Azure"},{"location":"tutorials/azure/#creating-an-azure-dns-zone","text":"The Azure provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. For this tutorial, we will create a Azure resource group named MyDnsResourceGroup that can easily be deleted later: $ az group create --name \"MyDnsResourceGroup\" --location \"eastus\" Substitute a more suitable location for the resource group if desired. Next, create a Azure DNS zone for example.com : $ az network dns zone create --resource-group \"MyDnsResourceGroup\" --name \"example.com\" Substitute a domain you own for example.com if desired. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the nameServers field from the JSON data returned by the az network dns zone create command. Please consult your registrar\u2019s documentation on how to do that.","title":"Creating an Azure DNS zone"},{"location":"tutorials/azure/#configuration-file","text":"The azure provider will reference a configuration file called azure.json . The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named azure.json with content similar to this: { \"tenantId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"subscriptionId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"resourceGroup\" : \"MyDnsResourceGroup\" , \"aadClientId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"aadClientSecret\" : \"uKiuXeiwui4jo9quae9o\" } The following fields are used: tenantId ( required ) - run az account show --query \"tenantId\" or by selecting Azure Active Directory in the Azure Portal and checking the Directory ID under Properties. subscriptionId ( required ) - run az account show --query \"id\" or by selecting Subscriptions in the Azure Portal. resourceGroup ( required ) is the Resource Group created in a previous step that contains the Azure DNS Zone. aadClientID and aaClientSecret are associated with the Service Principal. This is only used with Service Principal method documented in the next section. useManagedIdentityExtension - this is set to true if you use either AKS Kubelet Identity or AAD Pod Identities methods documented in the next section. userAssignedIdentityID - this contains the client id from the Managed identitty when using the AAD Pod Identities method documented in the next setion. The Azure DNS provider expects, by default, that the configuration file is at /etc/kubernetes/azure.json . This can be overridden with the --azure-config-file option when starting ExternalDNS.","title":"Configuration file"},{"location":"tutorials/azure/#permissions-to-modify-dns-zone","text":"ExternalDNS needs permissions to make changes to the Azure DNS zone. There are three ways configure the access needed: Service Principal Managed Identity Using AKS Kubelet Identity Managed Identity Using AAD Pod Identities","title":"Permissions to modify DNS zone"},{"location":"tutorials/azure/#service-principal","text":"These permissions are defined in a Service Principal that should be made available to ExternalDNS as a configuration file azure.json .","title":"Service Principal"},{"location":"tutorials/azure/#creating-a-service-principal","text":"A Service Principal with a minimum access level of DNS Zone Contributor or Contributor to the DNS zone(s) and Reader to the resource group containing the Azure DNS zone(s) is necessary for ExternalDNS to be able to edit DNS records. However, other more permissive access levels will work too (e.g. Contributor to the resource group or the whole subscription). This is an Azure CLI example on how to query the Azure API for the information required for the Resource Group and DNS zone you would have already created in previous steps (requires azure-cli and jq ) $ EXTERNALDNS_NEW_SP_NAME = \"ExternalDnsServicePrincipal\" # name of the service principal $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # name of resource group where dns zone is hosted $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com # Create the service principal $ DNS_SP = $( az ad sp create-for-rbac --name $EXTERNALDNS_NEW_SP_NAME ) $ EXTERNALDNS_SP_APP_ID = $( echo $DNS_SP | jq -r '.appId' ) $ EXTERNALDNS_SP_PASSWORD = $( echo $DNS_SP | jq -r '.password' )","title":"Creating a service principal"},{"location":"tutorials/azure/#assign-the-rights-for-the-service-principal","text":"Grant access to Azure DNS zone for the service principal. # fetch DNS id used to grant access to the service principal DNS_ID = $( az network dns zone show --name $AZURE_DNS_ZONE \\ --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv ) # 1. as a reader to the resource group $ az role assignment create --role \"Reader\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID # 2. as a contributor to DNS Zone itself $ az role assignment create --role \"Contributor\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID","title":"Assign the rights for the service principal"},{"location":"tutorials/azure/#creating-a-configuration-file-for-the-service-principal","text":"Create the file azure.json with values gather from previous steps. cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"aadClientId\": \"$EXTERNALDNS_SP_APP_ID\", \"aadClientSecret\": \"$EXTERNALDNS_SP_PASSWORD\" } EOF Use this file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json","title":"Creating a configuration file for the service principal"},{"location":"tutorials/azure/#managed-identity-using-aks-kubelet-identity","text":"The managed identity that is assigned to the underlying node pool in the AKS cluster can be given permissions to access Azure DNS. Managed identities are essentially a service principal whose lifecycle is managed, such as deleting the AKS cluster will also delete the service principals associated with the AKS cluster. The managed identity assigned Kuberetes node pool, or specifically the VMSS , is called the Kubelet identity. The managed identites were previously called MSI (Managed Service Identity) and are enabled by default when creating an AKS cluster. Note that permissions granted to this identity will be accessible to all containers running inside the Kubernetes cluster, not just the ExternalDNS container(s). For the managed identity, the contents of azure.json should be similar to this: { \"tenantId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"subscriptionId\" : \"01234abc-de56-ff78-abc1-234567890def\" , \"resourceGroup\" : \"MyDnsResourceGroup\" , \"useManagedIdentityExtension\" : true }","title":"Managed identity using AKS Kubelet identity"},{"location":"tutorials/azure/#fetching-the-kubelet-identity","text":"For this process, you will need to get the kublet identity: $ PRINCIPAL_ID = $( az aks show --resource-group $CLUSTER_GROUP --name $CLUSTERNAME \\ --query \"identityProfile.kubeletidentity.objectId\" --output tsv )","title":"Fetching the Kubelet identity"},{"location":"tutorials/azure/#assign-rights-for-the-kubelet-identity","text":"Grant access to Azure DNS zone for the kublet identity. $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # resource group where DNS zone is hosted # fetch DNS id used to grant access to the kublet identity $ DNS_ID = $( az network dns zone show --name $AZURE_DNS_ZONE \\ --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv ) $ az role assignment create --role \"DNS Zone Contributor\" --assignee $PRINCIPAL_ID --scope $DNS_ID","title":"Assign rights for the Kubelet identity"},{"location":"tutorials/azure/#creating-a-configuration-file-for-the-kubelet-identity","text":"Create the file azure.json with values gather from previous steps. cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"useManagedIdentityExtension\": true } EOF Use the azure.json file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json","title":"Creating a configuration file for the kubelet identity"},{"location":"tutorials/azure/#managed-identity-using-aad-pod-identities","text":"For this process, we will create a managed identity that will be explicitly used by the ExternalDNS container. This process is similar to Kubelet identity except that this managed identity is not associated with the Kubernetes node pool, but rather associated with explicit ExternalDNS containers.","title":"Managed identity using AAD Pod Identities"},{"location":"tutorials/azure/#enable-the-aad-pod-identities-feature","text":"For this solution, AAD Pod Identities preview feature can be enabled. The commands below should do the trick to enable this feature: $ az feature register --name EnablePodIdentityPreview --namespace Microsoft.ContainerService $ az feature register --name AutoUpgradePreview --namespace Microsoft.ContainerService $ az extension add --name aks-preview $ az extension update --name aks-preview $ az provider register --namespace Microsoft.ContainerService","title":"Enable the AAD Pod Identities feature"},{"location":"tutorials/azure/#deploy-the-aad-pod-identities-service","text":"Once enabled, you can update your cluster and install needed services for the AAD Pod Identities feature. $ AZURE_AKS_RESOURCE_GROUP = \"my-aks-cluster-group\" # name of resource group where aks cluster was created $ AZURE_AKS_CLUSTER_NAME = \"my-aks-cluster\" # name of aks cluster previously created $ az aks update --resource-group ${ AZURE_AKS_RESOURCE_GROUP } --name ${ AZURE_AKS_CLUSTER_NAME } --enable-pod-identity Note that, if you use the default network plugin kubenet , then you need to add the command line option --enable-pod-identity-with-kubenet to the above command.","title":"Deploy the AAD Pod Identities service"},{"location":"tutorials/azure/#creating-the-managed-identity","text":"After this process is finished, create a managed identity. $ IDENTITY_RESOURCE_GROUP = $AZURE_AKS_RESOURCE_GROUP # custom group or reuse AKS group $ IDENTITY_NAME = \"example-com-identity\" # create a managed identity $ az identity create --resource-group \" ${ IDENTITY_RESOURCE_GROUP } \" --name \" ${ IDENTITY_NAME } \"","title":"Creating the managed identity"},{"location":"tutorials/azure/#assign-rights-for-the-managed-identity","text":"Grant access to Azure DNS zone for the managed identity. $ AZURE_DNS_ZONE_RESOURCE_GROUP = \"MyDnsResourceGroup\" # name of resource group where dns zone is hosted $ AZURE_DNS_ZONE = \"example.com\" # DNS zone name like example.com or sub.example.com # fetch identity client id from managed identity created earlier $ IDENTITY_CLIENT_ID = $( az identity show --resource-group \" ${ IDENTITY_RESOURCE_GROUP } \" \\ --name \" ${ IDENTITY_NAME } \" --query \"clientId\" --output tsv ) # fetch DNS id used to grant access to the managed identity $ DNS_ID = $( az network dns zone show --name \" ${ AZURE_DNS_ZONE } \" \\ --resource-group \" ${ AZURE_DNS_ZONE_RESOURCE_GROUP } \" --query \"id\" --output tsv ) $ az role assignment create --role \"DNS Zone Contributor\" \\ --assignee \" ${ IDENTITY_CLIENT_ID } \" --scope \" ${ DNS_ID } \"","title":"Assign rights for the managed identity"},{"location":"tutorials/azure/#creating-a-configuration-file-for-the-managed-identity","text":"Create the file azure.json with the values from previous steps: cat <<-EOF > /local/path/to/azure.json { \"tenantId\": \"$(az account show --query tenantId -o tsv)\", \"subscriptionId\": \"$(az account show --query id -o tsv)\", \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\", \"useManagedIdentityExtension\": true, \"userAssignedIdentityID\": \"$IDENTITY_CLIENT_ID\" } EOF Use the azure.json file to create a Kubernetes secret: $ kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json","title":"Creating a configuration file for the managed identity"},{"location":"tutorials/azure/#creating-an-azure-identity-binding","text":"A binding between the managed identity and the ExternalDNS pods needs to be setup by creating AzureIdentity and AzureIdentityBinding resources. This will allow appropriately labeled ExternalDNS pods to authenticate using the managed identity. When AAD Pod Identity feature is enabled from previous steps above, the az aks pod-identity add can be used to create these resources: $ IDENTITY_RESOURCE_ID = $( az identity show --resource-group ${ IDENTITY_RESOURCE_GROUP } \\ --name ${ IDENTITY_NAME } --query id --output tsv ) $ az aks pod-identity add --resource-group ${ AZURE_AKS_RESOURCE_GROUP } \\ --cluster-name ${ AZURE_AKS_CLUSTER_NAME } --namespace \"default\" \\ --name \"external-dns\" --identity-resource-id ${ IDENTITY_RESOURCE_ID } This will add something similar to the following resouces: apiVersion : aadpodidentity.k8s.io/v1 kind : AzureIdentity metadata : labels : addonmanager.kubernetes.io/mode : Reconcile kubernetes.azure.com/managedby : aks name : external-dns spec : clientID : $IDENTITY_CLIENT_ID resourceID : $IDENTITY_RESOURCE_ID type : 0 --- apiVersion : aadpodidentity.k8s.io/v1 kind : AzureIdentityBinding metadata : annotations : labels : addonmanager.kubernetes.io/mode : Reconcile kubernetes.azure.com/managedby : aks name : external-dns-binding spec : azureIdentity : external-dns selector : external-dns","title":"Creating an Azure identity binding"},{"location":"tutorials/azure/#update-externaldns-labels","text":"When deploying ExternalDNS, you want to make sure that deployed pod(s) will have the label aadpodidbinding: external-dns to enable AAD Pod Identities. You can patch an existing deployment of ExternalDNS with this command: kubectl patch deployment external-dns --namespace \"default\" --patch \\ '{\"spec\": {\"template\": {\"metadata\": {\"labels\": {\"aadpodidbinding\": \"external-dns\"}}}}}'","title":"Update ExternalDNS labels"},{"location":"tutorials/azure/#ingress-used-with-externaldns","text":"This deployment assumes that you will be using nginx-ingress. When using nginx-ingress do not deploy it as a Daemon Set. This causes nginx-ingress to write the Cluster IP of the backend pods in the ingress status.loadbalancer.ip property which then has external-dns write the Cluster IP(s) in DNS vs. the nginx-ingress service external IP. Ensure that your nginx-ingress deployment has the following arg: added to it: - --publish-service=namespace/nginx-ingress-controller-svcname For more details see here: nginx-ingress external-dns","title":"Ingress used with ExternalDNS"},{"location":"tutorials/azure/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. The deployment assumes that ExternalDNS will be installed into the default namespace. If this namespace is different, the ClusterRoleBinding will need to be updated to reflect the desired alternative namespace, such as external-dns , kube-addons , etc.","title":"Deploy ExternalDNS"},{"location":"tutorials/azure/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/azure/#manifest-for-clusters-with-rbac-enabled-cluster-access","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group - --txt-prefix=externaldns- volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file","title":"Manifest (for clusters with RBAC enabled, cluster access)"},{"location":"tutorials/azure/#manifest-for-clusters-with-rbac-enabled-namespace-access","text":"This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type NodePort will be skipped! apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : external-dns subjects : - kind : ServiceAccount name : external-dns --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=azure - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group volumeMounts : - name : azure-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : azure-config-file secret : secretName : azure-config-file Create the deployment for ExternalDNS: $ kubectl create --namespace \"default\" --filename externaldns.yaml","title":"Manifest (for clusters with RBAC enabled, namespace access)"},{"location":"tutorials/azure/#deploying-an-nginx-service","text":"Create a file called nginx.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx-svc spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : nginx type : ClusterIP --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : server.example.com http : paths : - path : / pathType : Prefix backend : service : name : nginx-svc port : number : 80 When using ExternalDNS with ingress objects it will automatically create DNS records based on host names specified in ingress objects that match the domain-filter argument in the external-dns deployment manifest. When those host names are removed or renamed the corresponding DNS records are also altered. Create the deployment, service and ingress object: $ kubectl create --namespace \"default\" --filename nginx.yaml Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute.","title":"Deploying an Nginx Service"},{"location":"tutorials/azure/#verifying-azure-dns-records","text":"Run the following command to view the A records for your Azure DNS zone: $ az network dns record-set a list --resource-group \"MyDnsResourceGroup\" --zone-name example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).","title":"Verifying Azure DNS records"},{"location":"tutorials/azure/#delete-azure-resource-group","text":"Now that we have verified that ExternalDNS will automatically manage Azure DNS records, we can delete the tutorial\u2019s resource group: $ az group delete --name \"MyDnsResourceGroup\"","title":"Delete Azure Resource Group"},{"location":"tutorials/bluecat/","text":"Setting up external-dns for BlueCat \u00b6 The first external-dns release with with BlueCat provider support is v0.8.0. Prerequisites \u00b6 Install the BlueCat Gateway product and deploy the community gateway workflows . Configuration Options \u00b6 There are two ways to pass configuration options to the Bluecat Provider JSON configuration file and command line flags. Currently if a valid configuration file is used all BlueCat provider configurations will be taken from the configuration file. If a configuraiton file is not provided or cannot be read then all BlueCat provider configurations will be taken from the command line flags. In the future an enhancement will be made to merge configuration options from the configuration file and command line flags if both are provided. BlueCat provider supports getting the proxy URL from the environment variables. The format is the one specified by golang\u2019s http.ProxyFromEnvironment . Using CLI Flags \u00b6 When using CLI flags to configure the Bluecat Provider the BlueCat Gateway credentials are passed in using environment variables BLUECAT_USERNAME and BLUECAT_PASSWORD . Deploy \u00b6 Setup up namespace, deployment, and service account: kubectl create namespace bluecat-example kubectl create secret generic bluecat-credentials --from-literal=username=bluecatuser --from-literal=password=bluecatpassword -n bluecat-example cat << EOF > ~/bluecat.yml --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --log-level=debug - --source=service - --provider=bluecat - --txt-owner-id=bluecat-example - --bluecat-dns-configuration=Example - --bluecat-dns-view=Internal - --bluecat-gateway-host=https://bluecatgw.example.com - --bluecat-root-zone=example.com env: - name: BLUECAT_USERNAME valueFrom: secretKeyRef: name: bluecat-credentials key: username - name: BLUECAT_PASSWORD valueFrom: secretKeyRef: name: bluecat-credentials key: password EOF kubectl apply -f ~/bluecat.yml -n bluecat-example Using JSON Configuration File \u00b6 The options for configuring the Bluecat Provider are available through the JSON file provided to External-DNS via the flag --bluecat-config-file . Key Required gatewayHost Yes gatewayUsername No gatewayPassword No dnsConfiguration Yes dnsView Yes rootZone Yes dnsServerName No dnsDeployType No skipTLSVerify No (default false) Deploy \u00b6 Setup configuration file as k8s Secret . cat << EOF > ~/bluecat.json { \"gatewayHost\": \"https://bluecatgw.example.com\", \"gatewayUsername\": \"user\", \"gatewayPassword\": \"pass\", \"dnsConfiguration\": \"Example\", \"dnsView\": \"Internal\", \"rootZone\": \"example.com\", \"skipTLSVerify\": false } EOF kubectl create secret generic bluecatconfig --from-file ~/bluecat.json -n bluecat-example Setup up namespace, deployment, and service account: kubectl create namespace bluecat-example cat << EOF > ~/bluecat.yml --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns volumes: - name: bluecatconfig secret: secretName: bluecatconfig containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 volumeMounts: - name: bluecatconfig mountPath: \"/etc/external-dns/\" readOnly: true args: - --log-level=debug - --source=service - --provider=bluecat - --txt-owner-id=bluecat-example - --bluecat-config-file=/etc/external-dns/bluecat.json EOF kubectl apply -f ~/bluecat.yml -n bluecat-example","title":"Setting up external-dns for BlueCat"},{"location":"tutorials/bluecat/#setting-up-external-dns-for-bluecat","text":"The first external-dns release with with BlueCat provider support is v0.8.0.","title":"Setting up external-dns for BlueCat"},{"location":"tutorials/bluecat/#prerequisites","text":"Install the BlueCat Gateway product and deploy the community gateway workflows .","title":"Prerequisites"},{"location":"tutorials/bluecat/#configuration-options","text":"There are two ways to pass configuration options to the Bluecat Provider JSON configuration file and command line flags. Currently if a valid configuration file is used all BlueCat provider configurations will be taken from the configuration file. If a configuraiton file is not provided or cannot be read then all BlueCat provider configurations will be taken from the command line flags. In the future an enhancement will be made to merge configuration options from the configuration file and command line flags if both are provided. BlueCat provider supports getting the proxy URL from the environment variables. The format is the one specified by golang\u2019s http.ProxyFromEnvironment .","title":"Configuration Options"},{"location":"tutorials/bluecat/#using-cli-flags","text":"When using CLI flags to configure the Bluecat Provider the BlueCat Gateway credentials are passed in using environment variables BLUECAT_USERNAME and BLUECAT_PASSWORD .","title":"Using CLI Flags"},{"location":"tutorials/bluecat/#deploy","text":"Setup up namespace, deployment, and service account: kubectl create namespace bluecat-example kubectl create secret generic bluecat-credentials --from-literal=username=bluecatuser --from-literal=password=bluecatpassword -n bluecat-example cat << EOF > ~/bluecat.yml --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --log-level=debug - --source=service - --provider=bluecat - --txt-owner-id=bluecat-example - --bluecat-dns-configuration=Example - --bluecat-dns-view=Internal - --bluecat-gateway-host=https://bluecatgw.example.com - --bluecat-root-zone=example.com env: - name: BLUECAT_USERNAME valueFrom: secretKeyRef: name: bluecat-credentials key: username - name: BLUECAT_PASSWORD valueFrom: secretKeyRef: name: bluecat-credentials key: password EOF kubectl apply -f ~/bluecat.yml -n bluecat-example","title":"Deploy"},{"location":"tutorials/bluecat/#using-json-configuration-file","text":"The options for configuring the Bluecat Provider are available through the JSON file provided to External-DNS via the flag --bluecat-config-file . Key Required gatewayHost Yes gatewayUsername No gatewayPassword No dnsConfiguration Yes dnsView Yes rootZone Yes dnsServerName No dnsDeployType No skipTLSVerify No (default false)","title":"Using JSON Configuration File"},{"location":"tutorials/bluecat/#deploy_1","text":"Setup configuration file as k8s Secret . cat << EOF > ~/bluecat.json { \"gatewayHost\": \"https://bluecatgw.example.com\", \"gatewayUsername\": \"user\", \"gatewayPassword\": \"pass\", \"dnsConfiguration\": \"Example\", \"dnsView\": \"Internal\", \"rootZone\": \"example.com\", \"skipTLSVerify\": false } EOF kubectl create secret generic bluecatconfig --from-file ~/bluecat.json -n bluecat-example Setup up namespace, deployment, and service account: kubectl create namespace bluecat-example cat << EOF > ~/bluecat.yml --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns volumes: - name: bluecatconfig secret: secretName: bluecatconfig containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 volumeMounts: - name: bluecatconfig mountPath: \"/etc/external-dns/\" readOnly: true args: - --log-level=debug - --source=service - --provider=bluecat - --txt-owner-id=bluecat-example - --bluecat-config-file=/etc/external-dns/bluecat.json EOF kubectl apply -f ~/bluecat.yml -n bluecat-example","title":"Deploy"},{"location":"tutorials/civo/","text":"Setting up ExternalDNS for Services on Civo \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Civo DNS Manager. Make sure to use >0.12.2 version of ExternalDNS for this tutorial. Managing DNS with Civo \u00b6 If you want to learn about how to use Civo DNS Manager read the following tutorials: An Introduction to Managing DNS Get Civo Token \u00b6 Copy the token in the settings fo your account The environment variable CIVO_TOKEN will be needed to run ExternalDNS with Civo. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=civo env : - name : CIVO_TOKEN value : \"YOUR_CIVO_API_TOKEN\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=civo env : - name : CIVO_TOKEN value : \"YOUR_CIVO_API_TOKEN\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Civo DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Civo DNS records. Verifying Civo DNS records \u00b6 Check your Civo UI to view the records for your Civo DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Civo DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on Civo"},{"location":"tutorials/civo/#setting-up-externaldns-for-services-on-civo","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Civo DNS Manager. Make sure to use >0.12.2 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Civo"},{"location":"tutorials/civo/#managing-dns-with-civo","text":"If you want to learn about how to use Civo DNS Manager read the following tutorials: An Introduction to Managing DNS","title":"Managing DNS with Civo"},{"location":"tutorials/civo/#get-civo-token","text":"Copy the token in the settings fo your account The environment variable CIVO_TOKEN will be needed to run ExternalDNS with Civo.","title":"Get Civo Token"},{"location":"tutorials/civo/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/civo/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=civo env : - name : CIVO_TOKEN value : \"YOUR_CIVO_API_TOKEN\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/civo/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=civo env : - name : CIVO_TOKEN value : \"YOUR_CIVO_API_TOKEN\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/civo/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Civo DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Civo DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/civo/#verifying-civo-dns-records","text":"Check your Civo UI to view the records for your Civo DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Civo DNS records"},{"location":"tutorials/civo/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Civo DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/cloudflare/","text":"Setting up ExternalDNS for Services on Cloudflare \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Cloudflare DNS. Make sure to use >=0.4.2 version of ExternalDNS for this tutorial. Creating a Cloudflare DNS zone \u00b6 We highly recommend to read this tutorial if you haven\u2019t used Cloudflare before: Create a Cloudflare account and add a website Creating Cloudflare Credentials \u00b6 Snippet from Cloudflare - Getting Started : Cloudflare\u2019s API exposes the entire Cloudflare infrastructure via a standardized programmatic interface. Using Cloudflare\u2019s API, you can do just about anything you can do on cloudflare.com via the customer dashboard. The Cloudflare API is a RESTful API based on HTTPS requests and JSON responses. If you are registered with Cloudflare, you can obtain your API key from the bottom of the \u201cMy Account\u201d page, found here: Go to My account . API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. When using API Token authentication, the token should be granted Zone Read , DNS Edit privileges, and access to All zones . If you would like to further restrict the API permissions to a specific zone (or zones), you also need to use the --zone-id-filter so that the underlying API requests only access the zones that you explicitly specify, as opposed to accessing all zones. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) env : - name : CF_API_KEY value : \"YOUR_CLOUDFLARE_API_KEY\" - name : CF_API_EMAIL value : \"YOUR_CLOUDFLARE_EMAIL\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) env : - name : CF_API_KEY value : \"YOUR_CLOUDFLARE_API_KEY\" - name : CF_API_EMAIL value : \"YOUR_CLOUDFLARE_EMAIL\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Cloudflare DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. For Cloudflare proxied entries, set the TTL annotation to 1 (automatic), or do not set it. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Cloudflare DNS records. Verifying Cloudflare DNS records \u00b6 Check your Cloudflare dashboard to view the records for your Cloudflare DNS zone. Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Cloudflare DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml Setting cloudflare-proxied on a per-ingress basis \u00b6 Using the external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" annotation on your ingress, you can specify if the proxy feature of Cloudflare should be enabled for that record. This setting will override the global --cloudflare-proxied setting.","title":"Setting up ExternalDNS for Services on Cloudflare"},{"location":"tutorials/cloudflare/#setting-up-externaldns-for-services-on-cloudflare","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Cloudflare DNS. Make sure to use >=0.4.2 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Cloudflare"},{"location":"tutorials/cloudflare/#creating-a-cloudflare-dns-zone","text":"We highly recommend to read this tutorial if you haven\u2019t used Cloudflare before: Create a Cloudflare account and add a website","title":"Creating a Cloudflare DNS zone"},{"location":"tutorials/cloudflare/#creating-cloudflare-credentials","text":"Snippet from Cloudflare - Getting Started : Cloudflare\u2019s API exposes the entire Cloudflare infrastructure via a standardized programmatic interface. Using Cloudflare\u2019s API, you can do just about anything you can do on cloudflare.com via the customer dashboard. The Cloudflare API is a RESTful API based on HTTPS requests and JSON responses. If you are registered with Cloudflare, you can obtain your API key from the bottom of the \u201cMy Account\u201d page, found here: Go to My account . API Token will be preferred for authentication if CF_API_TOKEN environment variable is set. Otherwise CF_API_KEY and CF_API_EMAIL should be set to run ExternalDNS with Cloudflare. When using API Token authentication, the token should be granted Zone Read , DNS Edit privileges, and access to All zones . If you would like to further restrict the API permissions to a specific zone (or zones), you also need to use the --zone-id-filter so that the underlying API requests only access the zones that you explicitly specify, as opposed to accessing all zones.","title":"Creating Cloudflare Credentials"},{"location":"tutorials/cloudflare/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/cloudflare/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) env : - name : CF_API_KEY value : \"YOUR_CLOUDFLARE_API_KEY\" - name : CF_API_EMAIL value : \"YOUR_CLOUDFLARE_EMAIL\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/cloudflare/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone. - --provider=cloudflare - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...) env : - name : CF_API_KEY value : \"YOUR_CLOUDFLARE_API_KEY\" - name : CF_API_EMAIL value : \"YOUR_CLOUDFLARE_EMAIL\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/cloudflare/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Cloudflare DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. For Cloudflare proxied entries, set the TTL annotation to 1 (automatic), or do not set it. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Cloudflare DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/cloudflare/#verifying-cloudflare-dns-records","text":"Check your Cloudflare dashboard to view the records for your Cloudflare DNS zone. Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Cloudflare DNS records"},{"location":"tutorials/cloudflare/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Cloudflare DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/cloudflare/#setting-cloudflare-proxied-on-a-per-ingress-basis","text":"Using the external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\" annotation on your ingress, you can specify if the proxy feature of Cloudflare should be enabled for that record. This setting will override the global --cloudflare-proxied setting.","title":"Setting cloudflare-proxied on a per-ingress basis"},{"location":"tutorials/contour/","text":"Setting up External DNS with Contour \u00b6 This tutorial describes how to configure External DNS to use either the Contour IngressRoute or HTTPProxy source. The IngressRoute CRD is deprecated but still in-use in many clusters however it\u2019s recommended that you migrate to the HTTPProxy resource. Using the HTTPProxy resource with External DNS requires Contour version 1.5 or greater. Example manifests for External DNS \u00b6 Without RBAC \u00b6 Note that you don\u2019t need to enable both of the sources and if you don\u2019t enable contour-ingressroute you also don\u2019t need to configure the contour-load-balancer setting. apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=contour-ingressroute # To enable IngressRoute support - --source=contour-httpproxy # To enable HTTPProxy support - --contour-load-balancer=custom-contour-namespace/custom-contour-lb # For IngressRoute ONLY: load balancer service to be used. Omit to use the default (heptio-contour/contour) - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier With RBAC \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] # This section is only for IngressRoute - apiGroups : [ \"contour.heptio.com\" ] resources : [ \"ingressroutes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] # This section is only for HTTPProxy - apiGroups : [ \"projectcontour.io\" ] resources : [ \"httpproxies\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=contour-ingressroute # To enable IngressRoute support - --source=contour-httpproxy # To enable HTTPProxy support - --contour-load-balancer=custom-contour-namespace/custom-contour-lb # For IngressRoute ONLY: load balancer service to be used. Omit to use the default (heptio-contour/contour) - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier Verify External DNS works \u00b6 The following instructions are based on the Contour example workload . Install a sample service \u00b6 $ kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: app: kuard name: kuard spec: replicas: 3 selector: matchLabels: app: kuard template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 name: kuard --- apiVersion: v1 kind: Service metadata: labels: app: kuard name: kuard spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: kuard sessionAffinity: None type: ClusterIP EOF Then create either a HTTPProxy or an IngressRoute HTTPProxy \u00b6 $ kubectl apply -f - <<EOF apiVersion: projectcontour.io/v1 kind: HTTPProxy metadata: labels: app: kuard name: kuard namespace: default spec: virtualhost: fqdn: kuard.example.com routes: - conditions: - prefix: / services: - name: kuard port: 80 EOF IngressRoute \u00b6 $ kubectl apply -f - <<EOF apiVersion: contour.heptio.com/v1beta1 kind: IngressRoute metadata: labels: app: kuard name: kuard namespace: default spec: virtualhost: fqdn: kuard.example.com routes: - match: / services: - name: kuard port: 80 EOF Access the sample service using curl \u00b6 $ curl -i http://kuard.example.com/healthy HTTP/1.1 200 OK Content-Type: text/plain Date: Thu, 27 Jun 2019 19 :42:26 GMT Content-Length: 2 ok","title":"Setting up External DNS with Contour"},{"location":"tutorials/contour/#setting-up-external-dns-with-contour","text":"This tutorial describes how to configure External DNS to use either the Contour IngressRoute or HTTPProxy source. The IngressRoute CRD is deprecated but still in-use in many clusters however it\u2019s recommended that you migrate to the HTTPProxy resource. Using the HTTPProxy resource with External DNS requires Contour version 1.5 or greater.","title":"Setting up External DNS with Contour"},{"location":"tutorials/contour/#example-manifests-for-external-dns","text":"","title":"Example manifests for External DNS"},{"location":"tutorials/contour/#without-rbac","text":"Note that you don\u2019t need to enable both of the sources and if you don\u2019t enable contour-ingressroute you also don\u2019t need to configure the contour-load-balancer setting. apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=contour-ingressroute # To enable IngressRoute support - --source=contour-httpproxy # To enable HTTPProxy support - --contour-load-balancer=custom-contour-namespace/custom-contour-lb # For IngressRoute ONLY: load balancer service to be used. Omit to use the default (heptio-contour/contour) - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"Without RBAC"},{"location":"tutorials/contour/#with-rbac","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] # This section is only for IngressRoute - apiGroups : [ \"contour.heptio.com\" ] resources : [ \"ingressroutes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] # This section is only for HTTPProxy - apiGroups : [ \"projectcontour.io\" ] resources : [ \"httpproxies\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=contour-ingressroute # To enable IngressRoute support - --source=contour-httpproxy # To enable HTTPProxy support - --contour-load-balancer=custom-contour-namespace/custom-contour-lb # For IngressRoute ONLY: load balancer service to be used. Omit to use the default (heptio-contour/contour) - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"With RBAC"},{"location":"tutorials/contour/#verify-external-dns-works","text":"The following instructions are based on the Contour example workload .","title":"Verify External DNS works"},{"location":"tutorials/contour/#install-a-sample-service","text":"$ kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: app: kuard name: kuard spec: replicas: 3 selector: matchLabels: app: kuard template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 name: kuard --- apiVersion: v1 kind: Service metadata: labels: app: kuard name: kuard spec: ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: kuard sessionAffinity: None type: ClusterIP EOF Then create either a HTTPProxy or an IngressRoute","title":"Install a sample service"},{"location":"tutorials/contour/#httpproxy","text":"$ kubectl apply -f - <<EOF apiVersion: projectcontour.io/v1 kind: HTTPProxy metadata: labels: app: kuard name: kuard namespace: default spec: virtualhost: fqdn: kuard.example.com routes: - conditions: - prefix: / services: - name: kuard port: 80 EOF","title":"HTTPProxy"},{"location":"tutorials/contour/#ingressroute","text":"$ kubectl apply -f - <<EOF apiVersion: contour.heptio.com/v1beta1 kind: IngressRoute metadata: labels: app: kuard name: kuard namespace: default spec: virtualhost: fqdn: kuard.example.com routes: - match: / services: - name: kuard port: 80 EOF","title":"IngressRoute"},{"location":"tutorials/contour/#access-the-sample-service-using-curl","text":"$ curl -i http://kuard.example.com/healthy HTTP/1.1 200 OK Content-Type: text/plain Date: Thu, 27 Jun 2019 19 :42:26 GMT Content-Length: 2 ok","title":"Access the sample service using curl"},{"location":"tutorials/coredns/","text":"Setting up ExternalDNS for CoreDNS with minikube \u00b6 This tutorial describes how to setup ExternalDNS for usage within a minikube cluster that makes use of CoreDNS and nginx ingress controller . You need to: * install CoreDNS with etcd enabled * install external-dns with coredns as a provider * enable ingress controller for the minikube cluster Creating a cluster \u00b6 minikube start Installing CoreDNS with etcd enabled \u00b6 Helm chart is used to install etcd and CoreDNS. Initializing helm chart \u00b6 helm init Installing etcd \u00b6 etcd operator is used to manage etcd clusters. helm install stable/etcd-operator --name my-etcd-op etcd cluster is installed with example yaml from etcd operator website. kubectl apply -f https://raw.githubusercontent.com/coreos/etcd-operator/HEAD/example/example-etcd-cluster.yaml Installing CoreDNS \u00b6 In order to make CoreDNS work with etcd backend, values.yaml of the chart should be changed with corresponding configurations. wget https://raw.githubusercontent.com/helm/charts/HEAD/stable/coredns/values.yaml You need to edit/patch the file with below diff diff --git a/values.yaml b/values.yaml index 964e72b..e2fa934 100644 --- a/values.yaml +++ b/values.yaml @@ -27,12 +27,12 @@ service: rbac: # If true, create & use RBAC resources - create: false + create: true # Ignored if rbac.create is true serviceAccountName: default # isClusterService specifies whether chart should be deployed as cluster-service or normal k8s app. -isClusterService: true +isClusterService: false servers: - zones: @@ -51,6 +51,12 @@ servers: parameters: 0.0.0.0:9153 - name: proxy parameters: . /etc/resolv.conf + - name: etcd + parameters: example.org + configBlock: |- + stubzones + path /skydns + endpoint http://10.105.68.165:2379 # Complete example with all the options: # - zones: # the `zones` block can be left out entirely, defaults to \".\" Note : * IP address of etcd\u2019s endpoint should be get from etcd client service. It should be \u201cexample-etcd-cluster-client\u201d in this example. This IP address is used through this document for etcd endpoint configuration. $ kubectl get svc example-etcd-cluster-client NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE example-etcd-cluster-client ClusterIP 10.105.68.165 <none> 2379/TCP 16m * Parameters should configure your own domain. \u201cexample.org\u201d is used in this example. After configuration done in values.yaml, you can install coredns chart. helm install --name my-coredns --values values.yaml stable/coredns Installing ExternalDNS \u00b6 Install external ExternalDNS \u00b6 ETCD_URLS is configured to etcd client service address. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=coredns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://10.105.68.165:2379 Manifest (for clusters with RBAC enabled) \u00b6 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : kube-system --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : kube-system --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=coredns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://10.105.68.165:2379 Enable the ingress controller \u00b6 You can use the ingress controller in minikube cluster. It needs to enable ingress addon in the cluster. minikube addons enable ingress Testing ingress example \u00b6 $ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: nginx.example.org http: paths: - backend: serviceName: nginx servicePort: 80 $ kubectl apply -f ingress.yaml ingress.extensions \"nginx\" created Wait a moment until DNS has the ingress IP. The DNS service IP is from CoreDNS service. It is \u201cmy-coredns-coredns\u201d in this example. $ kubectl get svc my-coredns-coredns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-coredns-coredns ClusterIP 10.100.4.143 <none> 53/UDP 12m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx nginx.example.org 10.0.2.15 80 2m $ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools If you don't see a command prompt, try pressing enter. dnstools# dig @10.100.4.143 nginx.example.org +short 10.0.2.15 dnstools#","title":"Setting up ExternalDNS for CoreDNS with minikube"},{"location":"tutorials/coredns/#setting-up-externaldns-for-coredns-with-minikube","text":"This tutorial describes how to setup ExternalDNS for usage within a minikube cluster that makes use of CoreDNS and nginx ingress controller . You need to: * install CoreDNS with etcd enabled * install external-dns with coredns as a provider * enable ingress controller for the minikube cluster","title":"Setting up ExternalDNS for CoreDNS with minikube"},{"location":"tutorials/coredns/#creating-a-cluster","text":"minikube start","title":"Creating a cluster"},{"location":"tutorials/coredns/#installing-coredns-with-etcd-enabled","text":"Helm chart is used to install etcd and CoreDNS.","title":"Installing CoreDNS with etcd enabled"},{"location":"tutorials/coredns/#initializing-helm-chart","text":"helm init","title":"Initializing helm chart"},{"location":"tutorials/coredns/#installing-etcd","text":"etcd operator is used to manage etcd clusters. helm install stable/etcd-operator --name my-etcd-op etcd cluster is installed with example yaml from etcd operator website. kubectl apply -f https://raw.githubusercontent.com/coreos/etcd-operator/HEAD/example/example-etcd-cluster.yaml","title":"Installing etcd"},{"location":"tutorials/coredns/#installing-coredns","text":"In order to make CoreDNS work with etcd backend, values.yaml of the chart should be changed with corresponding configurations. wget https://raw.githubusercontent.com/helm/charts/HEAD/stable/coredns/values.yaml You need to edit/patch the file with below diff diff --git a/values.yaml b/values.yaml index 964e72b..e2fa934 100644 --- a/values.yaml +++ b/values.yaml @@ -27,12 +27,12 @@ service: rbac: # If true, create & use RBAC resources - create: false + create: true # Ignored if rbac.create is true serviceAccountName: default # isClusterService specifies whether chart should be deployed as cluster-service or normal k8s app. -isClusterService: true +isClusterService: false servers: - zones: @@ -51,6 +51,12 @@ servers: parameters: 0.0.0.0:9153 - name: proxy parameters: . /etc/resolv.conf + - name: etcd + parameters: example.org + configBlock: |- + stubzones + path /skydns + endpoint http://10.105.68.165:2379 # Complete example with all the options: # - zones: # the `zones` block can be left out entirely, defaults to \".\" Note : * IP address of etcd\u2019s endpoint should be get from etcd client service. It should be \u201cexample-etcd-cluster-client\u201d in this example. This IP address is used through this document for etcd endpoint configuration. $ kubectl get svc example-etcd-cluster-client NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE example-etcd-cluster-client ClusterIP 10.105.68.165 <none> 2379/TCP 16m * Parameters should configure your own domain. \u201cexample.org\u201d is used in this example. After configuration done in values.yaml, you can install coredns chart. helm install --name my-coredns --values values.yaml stable/coredns","title":"Installing CoreDNS"},{"location":"tutorials/coredns/#installing-externaldns","text":"","title":"Installing ExternalDNS"},{"location":"tutorials/coredns/#install-external-externaldns","text":"ETCD_URLS is configured to etcd client service address.","title":"Install external ExternalDNS"},{"location":"tutorials/coredns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=coredns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://10.105.68.165:2379","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/coredns/#manifest-for-clusters-with-rbac-enabled","text":"--- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : kube-system --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : kube-system --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=coredns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://10.105.68.165:2379","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/coredns/#enable-the-ingress-controller","text":"You can use the ingress controller in minikube cluster. It needs to enable ingress addon in the cluster. minikube addons enable ingress","title":"Enable the ingress controller"},{"location":"tutorials/coredns/#testing-ingress-example","text":"$ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: nginx.example.org http: paths: - backend: serviceName: nginx servicePort: 80 $ kubectl apply -f ingress.yaml ingress.extensions \"nginx\" created Wait a moment until DNS has the ingress IP. The DNS service IP is from CoreDNS service. It is \u201cmy-coredns-coredns\u201d in this example. $ kubectl get svc my-coredns-coredns NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-coredns-coredns ClusterIP 10.100.4.143 <none> 53/UDP 12m $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx nginx.example.org 10.0.2.15 80 2m $ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools If you don't see a command prompt, try pressing enter. dnstools# dig @10.100.4.143 nginx.example.org +short 10.0.2.15 dnstools#","title":"Testing ingress example"},{"location":"tutorials/designate/","text":"Setting up ExternalDNS for Services on OpenStack Designate \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OpenStack Designate DNS. Authenticating with OpenStack \u00b6 We are going to use OpenStack CLI - openstack utility, which is an umbrella application for most of OpenStack clients including designate . All OpenStack CLIs require authentication parameters to be provided. These parameters include: * URL of the OpenStack identity service ( keystone ) which is responsible for user authentication and also served as a registry for other OpenStack services. Designate endpoints must be registered in keystone in order to ExternalDNS and OpenStack CLI be able to find them. * OpenStack region name * User login name. * User project (tenant) name. * User domain (only when using keystone API v3) Although these parameters can be passed explicitly through the CLI flags, traditionally it is done by sourcing openrc file ( source ~/openrc ) that is a shell snippet that sets environment variables that all OpenStack CLI understand by convention. Recent versions of OpenStack Dashboard have a nice UI to download openrc file for both v2 and v3 auth protocols. Both protocols can be used with ExternalDNS. v3 is generally preferred over v2, but might not be available in some OpenStack installations. Installing OpenStack Designate \u00b6 Please refer to the Designate deployment tutorial for instructions on how to install and test Designate with BIND backend. You will be required to have admin rights in existing OpenStack installation to do this. One convenient way to get yourself an OpenStack installation to play with is to use DevStack . Creating DNS zones \u00b6 All domain names that are ExternalDNS is going to create must belong to one of DNS zones created in advance. Here is an example of how to create example.com DNS zone: $ openstack zone create --email dnsmaster@example.com example.com. It is important to manually create all the zones that are going to be used for kubernetes entities (ExternalDNS sources) before starting ExternalDNS. Deploy ExternalDNS \u00b6 Create a deployment file called externaldns.yaml with the following contents: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=designate env : # values from openrc file - name : OS_AUTH_URL value : https://controller/identity/v3 - name : OS_REGION_NAME value : RegionOne - name : OS_USERNAME value : admin - name : OS_PASSWORD value : p@ssw0rd - name : OS_PROJECT_NAME value : demo - name : OS_USER_DOMAIN_NAME value : Default Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=designate env : # values from openrc file - name : OS_AUTH_URL value : https://controller/identity/v3 - name : OS_REGION_NAME value : RegionOne - name : OS_USERNAME value : admin - name : OS_PASSWORD value : p@ssw0rd - name : OS_PROJECT_NAME value : demo - name : OS_USER_DOMAIN_NAME value : Default Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml Optional: Trust self-sign certificates \u00b6 If your OpenStack-Installation is configured with a self-sign certificate, you could extend the pod.spec with following secret-mount: volumeMounts : - mountPath : /etc/ssl/certs/ name : cacerts volumes : - name : cacerts secret : defaultMode : 420 secretName : self-sign-certs content of the secret self-sign-certs must be the certificate/chain in PEM format. Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and notify Designate, which in turn synchronize DNS records with underlying DNS server backend. Verifying DNS records \u00b6 To verify that DNS record was indeed created, you can use the following command: $ openstack recordset list example.com. There should be a record for my-app.example.com having ACTIVE status. And of course, the ultimate method to verify is to issue a DNS query: $ dig my-app.example.com @controller Cleanup \u00b6 Now that we have verified that ExternalDNS created all DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on OpenStack Designate"},{"location":"tutorials/designate/#setting-up-externaldns-for-services-on-openstack-designate","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OpenStack Designate DNS.","title":"Setting up ExternalDNS for Services on OpenStack Designate"},{"location":"tutorials/designate/#authenticating-with-openstack","text":"We are going to use OpenStack CLI - openstack utility, which is an umbrella application for most of OpenStack clients including designate . All OpenStack CLIs require authentication parameters to be provided. These parameters include: * URL of the OpenStack identity service ( keystone ) which is responsible for user authentication and also served as a registry for other OpenStack services. Designate endpoints must be registered in keystone in order to ExternalDNS and OpenStack CLI be able to find them. * OpenStack region name * User login name. * User project (tenant) name. * User domain (only when using keystone API v3) Although these parameters can be passed explicitly through the CLI flags, traditionally it is done by sourcing openrc file ( source ~/openrc ) that is a shell snippet that sets environment variables that all OpenStack CLI understand by convention. Recent versions of OpenStack Dashboard have a nice UI to download openrc file for both v2 and v3 auth protocols. Both protocols can be used with ExternalDNS. v3 is generally preferred over v2, but might not be available in some OpenStack installations.","title":"Authenticating with OpenStack"},{"location":"tutorials/designate/#installing-openstack-designate","text":"Please refer to the Designate deployment tutorial for instructions on how to install and test Designate with BIND backend. You will be required to have admin rights in existing OpenStack installation to do this. One convenient way to get yourself an OpenStack installation to play with is to use DevStack .","title":"Installing OpenStack Designate"},{"location":"tutorials/designate/#creating-dns-zones","text":"All domain names that are ExternalDNS is going to create must belong to one of DNS zones created in advance. Here is an example of how to create example.com DNS zone: $ openstack zone create --email dnsmaster@example.com example.com. It is important to manually create all the zones that are going to be used for kubernetes entities (ExternalDNS sources) before starting ExternalDNS.","title":"Creating DNS zones"},{"location":"tutorials/designate/#deploy-externaldns","text":"Create a deployment file called externaldns.yaml with the following contents:","title":"Deploy ExternalDNS"},{"location":"tutorials/designate/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=designate env : # values from openrc file - name : OS_AUTH_URL value : https://controller/identity/v3 - name : OS_REGION_NAME value : RegionOne - name : OS_USERNAME value : admin - name : OS_PASSWORD value : p@ssw0rd - name : OS_PROJECT_NAME value : demo - name : OS_USER_DOMAIN_NAME value : Default","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/designate/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=designate env : # values from openrc file - name : OS_AUTH_URL value : https://controller/identity/v3 - name : OS_REGION_NAME value : RegionOne - name : OS_USERNAME value : admin - name : OS_PASSWORD value : p@ssw0rd - name : OS_PROJECT_NAME value : demo - name : OS_USER_DOMAIN_NAME value : Default Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/designate/#optional-trust-self-sign-certificates","text":"If your OpenStack-Installation is configured with a self-sign certificate, you could extend the pod.spec with following secret-mount: volumeMounts : - mountPath : /etc/ssl/certs/ name : cacerts volumes : - name : cacerts secret : defaultMode : 420 secretName : self-sign-certs content of the secret self-sign-certs must be the certificate/chain in PEM format.","title":"Optional: Trust self-sign certificates"},{"location":"tutorials/designate/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and notify Designate, which in turn synchronize DNS records with underlying DNS server backend.","title":"Deploying an Nginx Service"},{"location":"tutorials/designate/#verifying-dns-records","text":"To verify that DNS record was indeed created, you can use the following command: $ openstack recordset list example.com. There should be a record for my-app.example.com having ACTIVE status. And of course, the ultimate method to verify is to issue a DNS query: $ dig my-app.example.com @controller","title":"Verifying DNS records"},{"location":"tutorials/designate/#cleanup","text":"Now that we have verified that ExternalDNS created all DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/digitalocean/","text":"Setting up ExternalDNS for Services on DigitalOcean \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using DigitalOcean DNS. Make sure to use >=0.4.2 version of ExternalDNS for this tutorial. Creating a DigitalOcean DNS zone \u00b6 If you want to learn about how to use DigitalOcean\u2019s DNS service read the following tutorial series: An Introduction to Managing DNS , and specifically How To Set Up a Host Name with DigitalOcean DNS Create a new DNS zone where you want to create your records in. Let\u2019s use example.com as an example here. Creating DigitalOcean Credentials \u00b6 Generate a new personal token by going to the API settings or follow How To Use the DigitalOcean API v2 if you need more information. Give the token a name and choose read and write access. The token needs to be passed to ExternalDNS so make a note of it for later use. The environment variable DO_TOKEN will be needed to run ExternalDNS with DigitalOcean. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=digitalocean env : - name : DO_TOKEN value : \"YOUR_DIGITALOCEAN_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=digitalocean env : - name : DO_TOKEN value : \"YOUR_DIGITALOCEAN_API_KEY\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DigitalOcean DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DigitalOcean DNS records. Verifying DigitalOcean DNS records \u00b6 Check your DigitalOcean UI to view the records for your DigitalOcean DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage DigitalOcean DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml Advanced Usage \u00b6 API Page Size \u00b6 If you have a large number of domains and/or records within a domain, you may encounter API rate limiting because of the number of API calls that external-dns must make to the DigitalOcean API to retrieve the current DNS configuration during every reconciliation loop. If this is the case, use the --digitalocean-api-page-size option to increase the size of the pages used when querying the DigitalOcean API. (Note: external-dns uses a default of 50.)","title":"Setting up ExternalDNS for Services on DigitalOcean"},{"location":"tutorials/digitalocean/#setting-up-externaldns-for-services-on-digitalocean","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using DigitalOcean DNS. Make sure to use >=0.4.2 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on DigitalOcean"},{"location":"tutorials/digitalocean/#creating-a-digitalocean-dns-zone","text":"If you want to learn about how to use DigitalOcean\u2019s DNS service read the following tutorial series: An Introduction to Managing DNS , and specifically How To Set Up a Host Name with DigitalOcean DNS Create a new DNS zone where you want to create your records in. Let\u2019s use example.com as an example here.","title":"Creating a DigitalOcean DNS zone"},{"location":"tutorials/digitalocean/#creating-digitalocean-credentials","text":"Generate a new personal token by going to the API settings or follow How To Use the DigitalOcean API v2 if you need more information. Give the token a name and choose read and write access. The token needs to be passed to ExternalDNS so make a note of it for later use. The environment variable DO_TOKEN will be needed to run ExternalDNS with DigitalOcean.","title":"Creating DigitalOcean Credentials"},{"location":"tutorials/digitalocean/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/digitalocean/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=digitalocean env : - name : DO_TOKEN value : \"YOUR_DIGITALOCEAN_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/digitalocean/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=digitalocean env : - name : DO_TOKEN value : \"YOUR_DIGITALOCEAN_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/digitalocean/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DigitalOcean DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DigitalOcean DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/digitalocean/#verifying-digitalocean-dns-records","text":"Check your DigitalOcean UI to view the records for your DigitalOcean DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying DigitalOcean DNS records"},{"location":"tutorials/digitalocean/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage DigitalOcean DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/digitalocean/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"tutorials/digitalocean/#api-page-size","text":"If you have a large number of domains and/or records within a domain, you may encounter API rate limiting because of the number of API calls that external-dns must make to the DigitalOcean API to retrieve the current DNS configuration during every reconciliation loop. If this is the case, use the --digitalocean-api-page-size option to increase the size of the pages used when querying the DigitalOcean API. (Note: external-dns uses a default of 50.)","title":"API Page Size"},{"location":"tutorials/dnsimple/","text":"Setting up ExternalDNS for Services on DNSimple \u00b6 This tutorial describes how to setup ExternalDNS for usage with DNSimple. Make sure to use >=0.4.6 version of ExternalDNS for this tutorial. Created a DNSimple API Access Token \u00b6 A DNSimple API access token can be acquired by following the provided documentation from DNSimple The environment variable DNSIMPLE_OAUTH must be set to the API token generated for to run ExternalDNS with DNSimple. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple. - --provider=dnsimple - --registry=txt env : - name : DNSIMPLE_OAUTH value : \"YOUR_DNSIMPLE_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple. - --provider=dnsimple - --registry=txt env : - name : DNSIMPLE_OAUTH value : \"YOUR_DNSIMPLE_API_KEY\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : validate-external-dns.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DNSimple DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Check the status by running kubectl get services nginx . If the EXTERNAL-IP field shows an address, the service is ready to be accessed externally. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DNSimple DNS records. Verifying DNSimple DNS records \u00b6 Getting your DNSimple Account ID \u00b6 If you do not know your DNSimple account ID it can be acquired using the whoami endpoint from the DNSimple Identity API curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN \" \\ -H 'Accept: application/json' \\ https://api.dnsimple.com/v2/whoami { \"data\" : { \"user\" : null, \"account\" : { \"id\" : 1 , \"email\" : \"example-account@example.com\" , \"plan_identifier\" : \"dnsimple-professional\" , \"created_at\" : \"2015-09-18T23:04:37Z\" , \"updated_at\" : \"2016-06-09T20:03:39Z\" } } } Looking at the DNSimple Dashboard \u00b6 You can view your DNSimple Record Editor at https://dnsimple.com/a/YOUR_ACCOUNT_ID/domains/example.com/records. Ensure you substitute the value YOUR_ACCOUNT_ID with the ID of your DNSimple account and example.com with the correct domain that you used during validation. Using the DNSimple Zone Records API \u00b6 This approach allows for you to use the DNSimple List records for a zone endpoint to verify the creation of the A and TXT record. Ensure you substitute the value YOUR_ACCOUNT_ID with the ID of your DNSimple account and example.com with the correct domain that you used during validation. curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN \" \\ -H 'Accept: application/json' \\ 'https://api.dnsimple.com/v2/YOUR_ACCOUNT_ID/zones/example.com/records&name=validate-external-dns' Clean up \u00b6 Now that we have verified that ExternalDNS will automatically manage DNSimple DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml Deleting Created Records \u00b6 The created records can be deleted using the record IDs from the verification step and the Delete a zone record endpoint.","title":"Setting up ExternalDNS for Services on DNSimple"},{"location":"tutorials/dnsimple/#setting-up-externaldns-for-services-on-dnsimple","text":"This tutorial describes how to setup ExternalDNS for usage with DNSimple. Make sure to use >=0.4.6 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on DNSimple"},{"location":"tutorials/dnsimple/#created-a-dnsimple-api-access-token","text":"A DNSimple API access token can be acquired by following the provided documentation from DNSimple The environment variable DNSIMPLE_OAUTH must be set to the API token generated for to run ExternalDNS with DNSimple.","title":"Created a DNSimple API Access Token"},{"location":"tutorials/dnsimple/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/dnsimple/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple. - --provider=dnsimple - --registry=txt env : - name : DNSIMPLE_OAUTH value : \"YOUR_DNSIMPLE_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/dnsimple/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple. - --provider=dnsimple - --registry=txt env : - name : DNSIMPLE_OAUTH value : \"YOUR_DNSIMPLE_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/dnsimple/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : validate-external-dns.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the DNSimple DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Check the status by running kubectl get services nginx . If the EXTERNAL-IP field shows an address, the service is ready to be accessed externally. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DNSimple DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/dnsimple/#verifying-dnsimple-dns-records","text":"","title":"Verifying DNSimple DNS records"},{"location":"tutorials/dnsimple/#getting-your-dnsimple-account-id","text":"If you do not know your DNSimple account ID it can be acquired using the whoami endpoint from the DNSimple Identity API curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN \" \\ -H 'Accept: application/json' \\ https://api.dnsimple.com/v2/whoami { \"data\" : { \"user\" : null, \"account\" : { \"id\" : 1 , \"email\" : \"example-account@example.com\" , \"plan_identifier\" : \"dnsimple-professional\" , \"created_at\" : \"2015-09-18T23:04:37Z\" , \"updated_at\" : \"2016-06-09T20:03:39Z\" } } }","title":"Getting your DNSimple Account ID"},{"location":"tutorials/dnsimple/#looking-at-the-dnsimple-dashboard","text":"You can view your DNSimple Record Editor at https://dnsimple.com/a/YOUR_ACCOUNT_ID/domains/example.com/records. Ensure you substitute the value YOUR_ACCOUNT_ID with the ID of your DNSimple account and example.com with the correct domain that you used during validation.","title":"Looking at the DNSimple Dashboard"},{"location":"tutorials/dnsimple/#using-the-dnsimple-zone-records-api","text":"This approach allows for you to use the DNSimple List records for a zone endpoint to verify the creation of the A and TXT record. Ensure you substitute the value YOUR_ACCOUNT_ID with the ID of your DNSimple account and example.com with the correct domain that you used during validation. curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN \" \\ -H 'Accept: application/json' \\ 'https://api.dnsimple.com/v2/YOUR_ACCOUNT_ID/zones/example.com/records&name=validate-external-dns'","title":"Using the DNSimple Zone Records API"},{"location":"tutorials/dnsimple/#clean-up","text":"Now that we have verified that ExternalDNS will automatically manage DNSimple DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Clean up"},{"location":"tutorials/dnsimple/#deleting-created-records","text":"The created records can be deleted using the record IDs from the verification step and the Delete a zone record endpoint.","title":"Deleting Created Records"},{"location":"tutorials/dyn/","text":"Setting up ExternalDNS for Dyn \u00b6 Creating a Dyn Configuration Secret \u00b6 For ExternalDNS to access the Dyn API, create a Kubernetes secret. To create the secret: $ kubectl create secret generic external-dns \\ --from-literal=EXTERNAL_DNS_DYN_CUSTOMER_NAME=${DYN_CUSTOMER_NAME} \\ --from-literal=EXTERNAL_DNS_DYN_USERNAME=${DYN_USERNAME} \\ --from-literal=EXTERNAL_DNS_DYN_PASSWORD=${DYN_PASSWORD} The credentials are the same ones created during account registration. As best practise, you are advised to create an API-only user that is entitled to only the zones intended to be changed by ExternalDNS Deploy ExternalDNS \u00b6 The rest of this tutorial assumes you own example.com domain and your DNS provider is Dyn. Change example.com with a domain/zone that you really own. In case of the dyn provider, the flag --zone-id-filter is mandatory as it specifies which zones to scan for records. Without it Create a deployment file called externaldns.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --txt-prefix=_d - --namespace=example - --zone-id-filter=example.com - --domain-filter=example.com - --provider=dyn env : - name : EXTERNAL_DNS_DYN_CUSTOMER_NAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_CUSTOMER_NAME - name : EXTERNAL_DNS_DYN_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_USERNAME - name : EXTERNAL_DNS_DYN_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_PASSWORD EOF As we\u2019ll be creating an Ingress resource, you need --txt-prefix=_d as a CNAME cannot coexist with a TXT record. You can change the prefix to any valid start of a FQDN. Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml Running a locally build version \u00b6 If you just want to test ExternalDNS in dry-run mode locally without doing the above deployment you can also do it. Make sure your kubectl is configured correctly . Assuming you have the sources, build and run it like so: make # output skipped ./build/external-dns \\ --provider = dyn \\ --dyn-customer-name = ${ DYN_CUSTOMER_NAME } \\ --dyn-username = ${ DYN_USERNAME } \\ --dyn-password = ${ DYN_PASSWORD } \\ --domain-filter = example.com \\ --zone-id-filter = example.com \\ --namespace = example \\ --log-level = debug \\ --txt-prefix = _ \\ --dry-run = true INFO [ 0000 ] running in dry-run mode. No changes to DNS records will be made. INFO [ 0000 ] Connected to cluster at https://some-k8s-cluster.example.com INFO [ 0001 ] Zones: [ example.com ] # output skipped Having --dry-run=true and --log-level=debug is a great way to see exactly what DynamicDNS is doing or is about to do. Deploying an Ingress Resource \u00b6 Create a file called \u2018test-ingress.yaml\u2019 with the following contents: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : test-ingress namespace : example spec : rules : - host : test-ingress.example.com http : paths : - backend : service : name : my-awesome-service port : number : 8080 pathType : Prefix As the DNS name test-ingress.example.com matches the filter, external-dns will create two records: a CNAME for test-ingress.example.com and TXT for _dtest-ingress.example.com. Create the Ingress: $ kubectl create -f test-ingress.yaml By default external-dns scans for changes every minute so give it some time to catch up with the Verifying Dyn DNS records \u00b6 Login to the console at https://portal.dynect.net/login/ and verify records are created Clean up \u00b6 Login to the console at https://portal.dynect.net/login/ and delete the records created. Alternatively, just delete the sample Ingress resources and external-dns will delete the records.","title":"Setting up ExternalDNS for Dyn"},{"location":"tutorials/dyn/#setting-up-externaldns-for-dyn","text":"","title":"Setting up ExternalDNS for Dyn"},{"location":"tutorials/dyn/#creating-a-dyn-configuration-secret","text":"For ExternalDNS to access the Dyn API, create a Kubernetes secret. To create the secret: $ kubectl create secret generic external-dns \\ --from-literal=EXTERNAL_DNS_DYN_CUSTOMER_NAME=${DYN_CUSTOMER_NAME} \\ --from-literal=EXTERNAL_DNS_DYN_USERNAME=${DYN_USERNAME} \\ --from-literal=EXTERNAL_DNS_DYN_PASSWORD=${DYN_PASSWORD} The credentials are the same ones created during account registration. As best practise, you are advised to create an API-only user that is entitled to only the zones intended to be changed by ExternalDNS","title":"Creating a Dyn Configuration Secret"},{"location":"tutorials/dyn/#deploy-externaldns","text":"The rest of this tutorial assumes you own example.com domain and your DNS provider is Dyn. Change example.com with a domain/zone that you really own. In case of the dyn provider, the flag --zone-id-filter is mandatory as it specifies which zones to scan for records. Without it Create a deployment file called externaldns.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --txt-prefix=_d - --namespace=example - --zone-id-filter=example.com - --domain-filter=example.com - --provider=dyn env : - name : EXTERNAL_DNS_DYN_CUSTOMER_NAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_CUSTOMER_NAME - name : EXTERNAL_DNS_DYN_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_USERNAME - name : EXTERNAL_DNS_DYN_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_DYN_PASSWORD EOF As we\u2019ll be creating an Ingress resource, you need --txt-prefix=_d as a CNAME cannot coexist with a TXT record. You can change the prefix to any valid start of a FQDN. Create the deployment for ExternalDNS: $ kubectl create -f externaldns.yaml","title":"Deploy ExternalDNS"},{"location":"tutorials/dyn/#running-a-locally-build-version","text":"If you just want to test ExternalDNS in dry-run mode locally without doing the above deployment you can also do it. Make sure your kubectl is configured correctly . Assuming you have the sources, build and run it like so: make # output skipped ./build/external-dns \\ --provider = dyn \\ --dyn-customer-name = ${ DYN_CUSTOMER_NAME } \\ --dyn-username = ${ DYN_USERNAME } \\ --dyn-password = ${ DYN_PASSWORD } \\ --domain-filter = example.com \\ --zone-id-filter = example.com \\ --namespace = example \\ --log-level = debug \\ --txt-prefix = _ \\ --dry-run = true INFO [ 0000 ] running in dry-run mode. No changes to DNS records will be made. INFO [ 0000 ] Connected to cluster at https://some-k8s-cluster.example.com INFO [ 0001 ] Zones: [ example.com ] # output skipped Having --dry-run=true and --log-level=debug is a great way to see exactly what DynamicDNS is doing or is about to do.","title":"Running a locally build version"},{"location":"tutorials/dyn/#deploying-an-ingress-resource","text":"Create a file called \u2018test-ingress.yaml\u2019 with the following contents: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : test-ingress namespace : example spec : rules : - host : test-ingress.example.com http : paths : - backend : service : name : my-awesome-service port : number : 8080 pathType : Prefix As the DNS name test-ingress.example.com matches the filter, external-dns will create two records: a CNAME for test-ingress.example.com and TXT for _dtest-ingress.example.com. Create the Ingress: $ kubectl create -f test-ingress.yaml By default external-dns scans for changes every minute so give it some time to catch up with the","title":"Deploying an Ingress Resource"},{"location":"tutorials/dyn/#verifying-dyn-dns-records","text":"Login to the console at https://portal.dynect.net/login/ and verify records are created","title":"Verifying Dyn DNS records"},{"location":"tutorials/dyn/#clean-up","text":"Login to the console at https://portal.dynect.net/login/ and delete the records created. Alternatively, just delete the sample Ingress resources and external-dns will delete the records.","title":"Clean up"},{"location":"tutorials/exoscale/","text":"Setting up ExternalDNS for Exoscale \u00b6 Prerequisites \u00b6 Exoscale provider support was added via this PR , thus you need to use external-dns v0.5.5. The Exoscale provider expects that your Exoscale zones, you wish to add records to, already exists and are configured correctly. It does not add, remove or configure new zones in anyway. To do this please refer to the Exoscale DNS documentation . Additionally you will have to provide the Exoscale\u2026: API Key API Secret API Endpoint Elastic IP address, to access the workers Deployment \u00b6 Deploying external DNS for Exoscale is actually nearly identical to deploying it for other providers. This is what a sample deployment.yaml looks like: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : # Only use if you're also using RBAC # serviceAccountName: external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress # or service or both - --provider=exoscale - --domain-filter={{ my-domain }} - --policy=sync # if you want DNS entries to get deleted as well - --txt-owner-id={{ owner-id-for-this-external-dns }} - --exoscale-endpoint={{ endpoint }} # usually https://api.exoscale.ch/dns - --exoscale-apikey={{ api-key}} - --exoscale-apisecret={{ api-secret }} RBAC \u00b6 If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns: apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default Testing and Verification \u00b6 Important! : Remember to change example.com with your own domain throughout the following text. Spin up a simple nginx HTTP server with the following spec ( kubectl apply -f ): apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx external-dns.alpha.kubernetes.io/target : {{ Elastic-IP-address }} spec : rules : - host : via-ingress.example.com http : paths : - backend : service : name : \"nginx\" port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 Important! : Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching , which is hard to flush. Wait about 30s-1m (interval for external-dns to kick in), then check Exoscales portal \u2026 via-ingress.example.com should appear as a A and TXT record with your Elastic-IP-address.","title":"Setting up ExternalDNS for Exoscale"},{"location":"tutorials/exoscale/#setting-up-externaldns-for-exoscale","text":"","title":"Setting up ExternalDNS for Exoscale"},{"location":"tutorials/exoscale/#prerequisites","text":"Exoscale provider support was added via this PR , thus you need to use external-dns v0.5.5. The Exoscale provider expects that your Exoscale zones, you wish to add records to, already exists and are configured correctly. It does not add, remove or configure new zones in anyway. To do this please refer to the Exoscale DNS documentation . Additionally you will have to provide the Exoscale\u2026: API Key API Secret API Endpoint Elastic IP address, to access the workers","title":"Prerequisites"},{"location":"tutorials/exoscale/#deployment","text":"Deploying external DNS for Exoscale is actually nearly identical to deploying it for other providers. This is what a sample deployment.yaml looks like: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : # Only use if you're also using RBAC # serviceAccountName: external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress # or service or both - --provider=exoscale - --domain-filter={{ my-domain }} - --policy=sync # if you want DNS entries to get deleted as well - --txt-owner-id={{ owner-id-for-this-external-dns }} - --exoscale-endpoint={{ endpoint }} # usually https://api.exoscale.ch/dns - --exoscale-apikey={{ api-key}} - --exoscale-apisecret={{ api-secret }}","title":"Deployment"},{"location":"tutorials/exoscale/#rbac","text":"If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns: apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default","title":"RBAC"},{"location":"tutorials/exoscale/#testing-and-verification","text":"Important! : Remember to change example.com with your own domain throughout the following text. Spin up a simple nginx HTTP server with the following spec ( kubectl apply -f ): apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx external-dns.alpha.kubernetes.io/target : {{ Elastic-IP-address }} spec : rules : - host : via-ingress.example.com http : paths : - backend : service : name : \"nginx\" port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 Important! : Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching , which is hard to flush. Wait about 30s-1m (interval for external-dns to kick in), then check Exoscales portal \u2026 via-ingress.example.com should appear as a A and TXT record with your Elastic-IP-address.","title":"Testing and Verification"},{"location":"tutorials/externalname/","text":"Setting up ExternalDNS for ExternalName Services \u00b6 This tutorial describes how to setup ExternalDNS for usage in conjunction with an ExternalName service. Use cases \u00b6 The main use cases that inspired this feature is the necessity for having a subdomain pointing to an external domain. In this scenario, it makes sense for the subdomain to have a CNAME record pointing to the external domain. Setup \u00b6 External DNS \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org ExternalName Service \u00b6 kind : Service apiVersion : v1 metadata : name : aws-service annotations : external-dns.alpha.kubernetes.io/hostname : tenant1.example.org,tenant2.example.org spec : type : ExternalName externalName : aws.example.org This will create 2 CNAME records pointing to aws.example.org : tenant1.example.org tenant2.example.org ExternalName Service with an IP address \u00b6 If externalName is an IP address, External DNS will create A records instead of CNAME. kind : Service apiVersion : v1 metadata : name : aws-service annotations : external-dns.alpha.kubernetes.io/hostname : tenant1.example.org,tenant2.example.org spec : type : ExternalName externalName : 111.111.111.111 This will create 2 A records pointing to 111.111.111.111 : tenant1.example.org tenant2.example.org","title":"Setting up ExternalDNS for ExternalName Services"},{"location":"tutorials/externalname/#setting-up-externaldns-for-externalname-services","text":"This tutorial describes how to setup ExternalDNS for usage in conjunction with an ExternalName service.","title":"Setting up ExternalDNS for ExternalName Services"},{"location":"tutorials/externalname/#use-cases","text":"The main use cases that inspired this feature is the necessity for having a subdomain pointing to an external domain. In this scenario, it makes sense for the subdomain to have a CNAME record pointing to the external domain.","title":"Use cases"},{"location":"tutorials/externalname/#setup","text":"","title":"Setup"},{"location":"tutorials/externalname/#external-dns","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org","title":"External DNS"},{"location":"tutorials/externalname/#externalname-service","text":"kind : Service apiVersion : v1 metadata : name : aws-service annotations : external-dns.alpha.kubernetes.io/hostname : tenant1.example.org,tenant2.example.org spec : type : ExternalName externalName : aws.example.org This will create 2 CNAME records pointing to aws.example.org : tenant1.example.org tenant2.example.org","title":"ExternalName Service"},{"location":"tutorials/externalname/#externalname-service-with-an-ip-address","text":"If externalName is an IP address, External DNS will create A records instead of CNAME. kind : Service apiVersion : v1 metadata : name : aws-service annotations : external-dns.alpha.kubernetes.io/hostname : tenant1.example.org,tenant2.example.org spec : type : ExternalName externalName : 111.111.111.111 This will create 2 A records pointing to 111.111.111.111 : tenant1.example.org tenant2.example.org","title":"ExternalName Service with an IP address"},{"location":"tutorials/gandi/","text":"Setting up ExternalDNS for Services on Gandi \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Gandi. Make sure to use >=0.7.7 version of ExternalDNS for this tutorial. Creating a Gandi DNS zone (domain) \u00b6 Create a new DNS zone where you want to create your records in. Let\u2019s use example.com as an example here. Make sure the zone uses Creating Gandi API Key \u00b6 Generate an API key on your account (click on \u201cSecurity\u201d). The environment variable GANDI_KEY will be needed to run ExternalDNS with Gandi. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=gandi env : - name : GANDI_KEY value : \"YOUR_GANDI_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=gandi env : - name : GANDI_KEY value : \"YOUR_GANDI_API_KEY\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Gandi Domain. Make sure that your Domain is configured to use Live-DNS. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Gandi DNS records. Verifying Gandi DNS records \u00b6 Check your Gandi Dashboard to view the records for your Gandi DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Gandi DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml Additional options \u00b6 If you\u2019re using organizations to separate your domains, you can pass the organization\u2019s ID in an environment variable called GANDI_SHARING_ID to get access to it.","title":"Setting up ExternalDNS for Services on Gandi"},{"location":"tutorials/gandi/#setting-up-externaldns-for-services-on-gandi","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Gandi. Make sure to use >=0.7.7 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Gandi"},{"location":"tutorials/gandi/#creating-a-gandi-dns-zone-domain","text":"Create a new DNS zone where you want to create your records in. Let\u2019s use example.com as an example here. Make sure the zone uses","title":"Creating a Gandi DNS zone (domain)"},{"location":"tutorials/gandi/#creating-gandi-api-key","text":"Generate an API key on your account (click on \u201cSecurity\u201d). The environment variable GANDI_KEY will be needed to run ExternalDNS with Gandi.","title":"Creating Gandi API Key"},{"location":"tutorials/gandi/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/gandi/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=gandi env : - name : GANDI_KEY value : \"YOUR_GANDI_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/gandi/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1beta1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=gandi env : - name : GANDI_KEY value : \"YOUR_GANDI_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/gandi/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Gandi Domain. Make sure that your Domain is configured to use Live-DNS. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Gandi DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/gandi/#verifying-gandi-dns-records","text":"Check your Gandi Dashboard to view the records for your Gandi DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Gandi DNS records"},{"location":"tutorials/gandi/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Gandi DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/gandi/#additional-options","text":"If you\u2019re using organizations to separate your domains, you can pass the organization\u2019s ID in an environment variable called GANDI_SHARING_ID to get access to it.","title":"Additional options"},{"location":"tutorials/gateway-api/","text":"Configuring ExternalDNS to use Gateway API Route Sources \u00b6 This describes how to configure ExternalDNS to use Gateway API Route sources. It is meant to supplement the other provider-specific setup tutorials. Supported API Versions \u00b6 As the Gateway API is still in an experimental phase, ExternalDNS makes no backwards compatibilty guarantees regarding its support. However, it currently supports a mixture of v1alpha2 and v1beta1 APIs. Gateways and HTTPRoutes are supported using the v1beta1 API. GRPCRoutes, TLSRoutes, TCPRoutes, and UDPRoutes are supported using the v1alpha2 API. Hostnames \u00b6 HTTPRoute and TLSRoute specs, along with their associated Gateway Listeners, contain hostnames that will be used by ExternalDNS. However, no such hostnames may be specified in TCPRoute or UDPRoute specs. For TCPRoutes and UDPRoutes, the external-dns.alpha.kubernetes.io/hostname annotation is the recommended way to provide their hostnames to ExternalDNS. This annotation is also supported for HTTPRoutes and TLSRoutes by ExternalDNS, but it\u2019s strongly recommended that they use their specs to provide all intended hostnames, since the Gateway that ultimately routes their requests/connections won\u2019t recognize additional hostnames from the annotation. Manifest with RBAC \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"gateway.networking.k8s.io\" ] resources : [ \"gateways\" , \"httproutes\" , \"grpcroutes\" , \"tlsroutes\" , \"tcproutes\" , \"udproutes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : default spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : # Add desired Gateway API Route sources. - --source=gateway-httproute - --source=gateway-grpcroute - --source=gateway-tlsroute - --source=gateway-tcproute - --source=gateway-udproute # Optionally, limit Routes to those in the given namespace. - --namespace=my-route-namespace # Optionally, limit Routes to those matching the given label selector. - --label-filter=my-route-label==my-route-value # Optionally, limit Route endpoints to those Gateways in the given namespace. - --gateway-namespace=my-gateway-namespace # Optionally, limit Route endpoints to those Gateways matching the given label selector. - --gateway-label-filter=my-gateway-label==my-gateway-value # Add provider-specific flags... - --domain-filter=external-dns-test.my-org.com - --provider=google - --registry=txt - --txt-owner-id=my-identifier","title":"Configuring ExternalDNS to use Gateway API Route Sources"},{"location":"tutorials/gateway-api/#configuring-externaldns-to-use-gateway-api-route-sources","text":"This describes how to configure ExternalDNS to use Gateway API Route sources. It is meant to supplement the other provider-specific setup tutorials.","title":"Configuring ExternalDNS to use Gateway API Route Sources"},{"location":"tutorials/gateway-api/#supported-api-versions","text":"As the Gateway API is still in an experimental phase, ExternalDNS makes no backwards compatibilty guarantees regarding its support. However, it currently supports a mixture of v1alpha2 and v1beta1 APIs. Gateways and HTTPRoutes are supported using the v1beta1 API. GRPCRoutes, TLSRoutes, TCPRoutes, and UDPRoutes are supported using the v1alpha2 API.","title":"Supported API Versions"},{"location":"tutorials/gateway-api/#hostnames","text":"HTTPRoute and TLSRoute specs, along with their associated Gateway Listeners, contain hostnames that will be used by ExternalDNS. However, no such hostnames may be specified in TCPRoute or UDPRoute specs. For TCPRoutes and UDPRoutes, the external-dns.alpha.kubernetes.io/hostname annotation is the recommended way to provide their hostnames to ExternalDNS. This annotation is also supported for HTTPRoutes and TLSRoutes by ExternalDNS, but it\u2019s strongly recommended that they use their specs to provide all intended hostnames, since the Gateway that ultimately routes their requests/connections won\u2019t recognize additional hostnames from the annotation.","title":"Hostnames"},{"location":"tutorials/gateway-api/#manifest-with-rbac","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"namespaces\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"gateway.networking.k8s.io\" ] resources : [ \"gateways\" , \"httproutes\" , \"grpcroutes\" , \"tlsroutes\" , \"tcproutes\" , \"udproutes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : default spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : # Add desired Gateway API Route sources. - --source=gateway-httproute - --source=gateway-grpcroute - --source=gateway-tlsroute - --source=gateway-tcproute - --source=gateway-udproute # Optionally, limit Routes to those in the given namespace. - --namespace=my-route-namespace # Optionally, limit Routes to those matching the given label selector. - --label-filter=my-route-label==my-route-value # Optionally, limit Route endpoints to those Gateways in the given namespace. - --gateway-namespace=my-gateway-namespace # Optionally, limit Route endpoints to those Gateways matching the given label selector. - --gateway-label-filter=my-gateway-label==my-gateway-value # Add provider-specific flags... - --domain-filter=external-dns-test.my-org.com - --provider=google - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest with RBAC"},{"location":"tutorials/gke/","text":"Setting up ExternalDNS on Google Kubernetes Engine \u00b6 This tutorial describes how to setup ExternalDNS for usage within a GKE ( Google Kuberentes Engine ) cluster. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial Single project test scenario using access scopes \u00b6 If you prefer to try-out ExternalDNS in one of the existing environments you can skip this step The following instructions use access scopes to provide ExternalDNS with the permissions it needs to manage DNS records within a single project , the organizing entity to allocate resources. Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments. This solution will only work when both CloudDNS and GKE are provisioned in the same project. If the CloudDNS zone is in a different project, this solution will not work. Configure Project Environment \u00b6 Setup your environment to work with Google Cloud Platform. Fill in your variables as needed, e.g. target project. # set variables to the appropriate desired values PROJECT_ID = \"my-external-dns-test\" REGION = \"europe-west1\" ZONE = \"europe-west1-d\" ClOUD_BILLING_ACCOUNT = \"<my-cloud-billing-account>\" # set default settings for project gcloud config set project $PROJECT_ID gcloud config set compute/region $REGION gcloud config set compute/zone $ZONE # enable billing and APIs if not done already gcloud beta billing projects link $PROJECT_ID \\ --billing-account $BILLING_ACCOUNT gcloud services enable \"dns.googleapis.com\" gcloud services enable \"container.googleapis.com\" Create GKE Cluster \u00b6 gcloud container clusters create $GKE_CLUSTER_NAME \\ --num-nodes 1 \\ --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" WARNING : Note that this cluster will use the default compute engine GSA that contians the overly permissive project editor ( roles/editor ) role. So essentially, anything on the cluster could potentially grant escalated privileges. Also, as mentioned earlier, the access scope ndev.clouddns.readwrite will allow anything running on the cluster to have read/write permissions on all Cloud DNS zones within the same project. Cloud DNS Zone \u00b6 Create a DNS zone which will contain the managed DNS records. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the nameServers key. Please consult your registrar\u2019s documentation on how to do that. This tutorial will use example domain of example.com . gcloud dns managed-zones create \"example-com\" --dns-name \"example.com.\" \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone \"example-com\" --name \"example.com.\" --type NS Outputs: NAME TYPE TTL DATA example.com. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc. Cross project access scenario using Google Service Account \u00b6 More often, following best practices in regards to security and operations, Cloud DNS zones will be managed in a separate project from the Kubernetes cluster. This section shows how setup ExternalDNS to access Cloud DNS from a different project. These steps will also work for single project scenarios as well. ExternalDNS will need permissions to make changes to the Cloud DNS zone. There are three ways to configure the access needed: Worker Node Service Account Static Credentials Work Load Identity Setup Cloud DNS and GKE \u00b6 Below are examples on how you can configure Cloud DNS and GKE in separate projects, and then use one of the three methods to grant access to ExternalDNS. Replace the environment variables to values that make sense in your environment. Configure Projects \u00b6 For this process, create projects with the appropriate APIs enabled. # set variables to appropriate desired values GKE_PROJECT_ID = \"my-workload-project\" DNS_PROJECT_ID = \"my-cloud-dns-project\" ClOUD_BILLING_ACCOUNT = \"<my-cloud-billing-account>\" # enable billing and APIs for DNS project if not done already gcloud config set project $DNS_PROJECT_ID gcloud beta billing projects link $CLOUD_DNS_PROJECT \\ --billing-account $ClOUD_BILLING_ACCOUNT gcloud services enable \"dns.googleapis.com\" # enable billing and APIs for GKE project if not done already gcloud config set project $GKE_PROJECT_ID gcloud beta billing projects link $CLOUD_DNS_PROJECT \\ --billing-account $ClOUD_BILLING_ACCOUNT gcloud services enable \"container.googleapis.com\" Provisioning Cloud DNS \u00b6 Create a Cloud DNS zone in the designated DNS project. gcloud dns managed-zones create \"example-com\" --project $DNS_PROJECT_ID \\ --description \"example.com\" --dns-name = \"example.com.\" --visibility = public If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the nameServers key. Please consult your registrar\u2019s documentation on how to do that. The example domain of example.com will be used for this tutorial. Provisioning a GKE cluster for cross project access \u00b6 Create a GSA (Google Service Account) and grant it the minimal set of privileges required for GKE nodes: GKE_CLUSTER_NAME = \"my-external-dns-cluster\" GKE_REGION = \"us-central1\" GKE_SA_NAME = \"worker-nodes-sa\" GKE_SA_EMAIL = \" $GKE_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" ROLES =( roles/logging.logWriter roles/monitoring.metricWriter roles/monitoring.viewer roles/stackdriver.resourceMetadata.writer ) gcloud iam service-accounts create $GKE_SA_NAME \\ --display-name $GKE_SA_NAME --project $GKE_PROJECT_ID # assign google service account to roles in GKE project for ROLE in ${ ROLES [*] } ; do gcloud projects add-iam-policy-binding $GKE_PROJECT_ID \\ --member \"serviceAccount: $GKE_SA_EMAIL \" \\ --role $ROLE done Create a cluster using this service account and enable workload identity : gcloud container clusters create $GKE_CLUSTER_NAME \\ --project $GKE_PROJECT_ID --region $GKE_REGION --num-nodes 1 \\ --service-account \" $GKE_SA_EMAIL \" \\ --workload-pool \" $GKE_PROJECT_ID .svc.id.goog\" Worker Node Service Account method \u00b6 In this method, the GSA (Google Service Account) that is associated with GKE worker nodes will be configured to have access to Cloud DNS. WARNING : This will grant access to modify the Cloud DNS zone records for all containers running on cluster, not just ExternalDNS, so use this option with caution. This is not recommended for production environments. GKE_SA_EMAIL = \" $GKE_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" # assign google service account to dns.admin role in the cloud dns project gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $GKE_SA_EMAIL \\ --role roles/dns.admin After this, follow the steps in Deploy ExternalDNS . Make sure to set the --google-project flag to match the Cloud DNS project name. Static Credentials \u00b6 In this scenario, a new GSA (Google Service Account) is created that has access to the CloudDNS zone. The credentials for this GSA are saved and installed as a Kubernetes secret that will be used by ExternalDNS. This allows only containers that have access to the secret, such as ExternalDNS to update records on the Cloud DNS Zone. Create GSA for use with static credentials \u00b6 DNS_SA_NAME = \"external-dns-sa\" DNS_SA_EMAIL = \" $DNS_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" # create GSA used to access the Cloud DNS zone gcloud iam service-accounts create $DNS_SA_NAME --display-name $DNS_SA_NAME # assign google service account to dns.admin role in cloud-dns project gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $DNS_SA_EMAIL --role \"roles/dns.admin\" Create Kubernetes secret using static credentials \u00b6 Generate static credentials from the ExternalDNS GSA. # download static credentials gcloud iam service-accounts keys create /local/path/to/credentials.json \\ --iam-account $DNS_SA_EMAIL Create a Kubernetes secret with the credentials in the same namespace of ExternalDNS. kubectl create secret generic \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --from-file /local/path/to/credentials.json After this, follow the steps in Deploy ExternalDNS . Make sure to set the --google-project flag to match Cloud DNS project name. Make sure to uncomment out the section that mounts the secret to the ExternalDNS pods. Workload Identity \u00b6 Workload Identity allows workloads in your GKE cluster to impersonate GSA (Google Service Accounts) using KSA (Kubernetes Service Accounts) configured during deployemnt. These are the steps to use this feature with ExternalDNS. Create GSA for use with Workload Identity \u00b6 DNS_SA_NAME = \"external-dns-sa\" DNS_SA_EMAIL = \" $DNS_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" gcloud iam service-accounts create $DNS_SA_NAME --display-name $DNS_SA_NAME gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $DNS_SA_EMAIL --role \"roles/dns.admin\" Link KSA to GSA \u00b6 Add an IAM policy binding bewtween the workload identity GSA and ExternalDNS GSA. This will link the ExternalDNS KSA to ExternalDNS GSA. gcloud iam service-accounts add-iam-policy-binding $DNS_SA_EMAIL \\ --role \"roles/iam.workloadIdentityUser\" \\ --member \"serviceAccount: $GKE_PROJECT_ID .svc.id.goog[ ${ EXTERNALDNS_NS :- \"default\" } /external-dns]\" Deploy External DNS \u00b6 Deploy ExternalDNS with the following steps below, documented under Deploy ExternalDNS . Set the --google-project flag to the Cloud DNS project name. Link KSA to GSA in Kubernetes \u00b6 Add the proper workload identity annotation to the ExternalDNS KSA. kubectl annotate serviceaccount \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ \"iam.gke.io/gcp-service-account= $DNS_SA_EMAIL \" Update ExternalDNS pods \u00b6 Update the Pod spec to schedule the workloads on nodes that use Workload Identity and to use the annotated Kubernetes service account. kubectl patch deployment \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --patch \\ '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"iam.gke.io/gke-metadata-server-enabled\": \"true\"}}}}}' After all of these steps you may see several messages with googleapi: Error 403: Forbidden, forbidden . After several minutes when the token is refreshed, these error messages will go away, and you should see info messages, such as: All records are already up to date . Deploy ExternalDNS \u00b6 Then apply the following manifests file to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns labels : app.kubernetes.io/name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns labels : app.kubernetes.io/name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer labels : app.kubernetes.io/name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default # change if namespace is not 'default' --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --log-format=json # google cloud logs parses severity of the \"text\" log format incorrectly # - --google-project=my-cloud-dns-project # Use this to specify a project different from the one external-dns is running inside - --google-zone-visibility=public # Use this to filter to only zones with this visibility. Set to either 'public' or 'private'. Omitting will match public and private zones - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier # # uncomment below if static credentials are used # env: # - name: GOOGLE_APPLICATION_CREDENTIALS # value: /etc/secrets/service-account/credentials.json # volumeMounts: # - name: google-service-account # mountPath: /etc/secrets/service-account/ # volumes: # - name: google-service-account # secret: # secretName: external-dns Create the deployment for ExternalDNS: kubectl create --namespace \"default\" --filename externaldns.yaml Verify ExternalDNS works \u00b6 The following will deploy a small nginx server that will be used to demonstrate that ExternalDNS is working. Verify using an external load balancer \u00b6 Create the following sample application to test that ExternalDNS works. This example will provision a L4 load balancer. apiVersion : v1 kind : Service metadata : name : nginx annotations : # change nginx.example.com to match an appropriate value external-dns.alpha.kubernetes.io/hostname : nginx.example.com spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 Create the deployment and service objects: kubectl create --namespace \"default\" --filename nginx.yaml After roughly two minutes check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone \"example-com\" --name \"nginx.example.com.\" Example output: NAME TYPE TTL DATA nginx.example.com. A 300 104.155.60.49 nginx.example.com. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier\" Note created TXT record alongside A record. TXT record signifies that the corresponding A record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. dig +short @ns-cloud-e1.googledomains.com. nginx.example.com. 104 .155.60.49 Given you hooked up your DNS zone with its parent zone you can use curl to access your site. curl nginx.example.com Verify using an ingress \u00b6 Let\u2019s check that Ingress works as well. Create the following Ingress. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx spec : rules : - host : server.example.com http : paths : - path : / pathType : Prefix backend : service : name : nginx port : number : 80 Create the ingress objects with: kubectl create --namespace \"default\" --filename ingress.yaml Note that this will ingress object will use the default ingress controller that comes with GKE to create a L7 load balancer in addition to the L4 load balancer previously with the service object. To use only the L7 load balancer, update the service manafest to change the Service type to NodePort and remove the ExternalDNS annotation. After roughly two minutes check that a corresponding DNS record for your Ingress was created. gcloud dns record-sets list \\ --zone \"example-com\" \\ --name \"server.example.com.\" \\ Output: NAME TYPE TTL DATA server.example.com. A 300 130.211.46.224 server.example.com. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier\" Let\u2019s check that we can resolve this DNS name as well. dig +short @ns-cloud-e1.googledomains.com. server.example.com. 130 .211.46.224 Try with curl as well. curl server.example.com Clean up \u00b6 Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers get cleaned up correctly. kubectl delete service nginx kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. gcloud dns managed-zones delete \"example-com\" gcloud container clusters delete \"external-dns\"","title":"Setting up ExternalDNS on Google Kubernetes Engine"},{"location":"tutorials/gke/#setting-up-externaldns-on-google-kubernetes-engine","text":"This tutorial describes how to setup ExternalDNS for usage within a GKE ( Google Kuberentes Engine ) cluster. Make sure to use >=0.11.0 version of ExternalDNS for this tutorial","title":"Setting up ExternalDNS on Google Kubernetes Engine"},{"location":"tutorials/gke/#single-project-test-scenario-using-access-scopes","text":"If you prefer to try-out ExternalDNS in one of the existing environments you can skip this step The following instructions use access scopes to provide ExternalDNS with the permissions it needs to manage DNS records within a single project , the organizing entity to allocate resources. Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments. This solution will only work when both CloudDNS and GKE are provisioned in the same project. If the CloudDNS zone is in a different project, this solution will not work.","title":"Single project test scenario using access scopes"},{"location":"tutorials/gke/#configure-project-environment","text":"Setup your environment to work with Google Cloud Platform. Fill in your variables as needed, e.g. target project. # set variables to the appropriate desired values PROJECT_ID = \"my-external-dns-test\" REGION = \"europe-west1\" ZONE = \"europe-west1-d\" ClOUD_BILLING_ACCOUNT = \"<my-cloud-billing-account>\" # set default settings for project gcloud config set project $PROJECT_ID gcloud config set compute/region $REGION gcloud config set compute/zone $ZONE # enable billing and APIs if not done already gcloud beta billing projects link $PROJECT_ID \\ --billing-account $BILLING_ACCOUNT gcloud services enable \"dns.googleapis.com\" gcloud services enable \"container.googleapis.com\"","title":"Configure Project Environment"},{"location":"tutorials/gke/#create-gke-cluster","text":"gcloud container clusters create $GKE_CLUSTER_NAME \\ --num-nodes 1 \\ --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" WARNING : Note that this cluster will use the default compute engine GSA that contians the overly permissive project editor ( roles/editor ) role. So essentially, anything on the cluster could potentially grant escalated privileges. Also, as mentioned earlier, the access scope ndev.clouddns.readwrite will allow anything running on the cluster to have read/write permissions on all Cloud DNS zones within the same project.","title":"Create GKE Cluster"},{"location":"tutorials/gke/#cloud-dns-zone","text":"Create a DNS zone which will contain the managed DNS records. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the nameServers key. Please consult your registrar\u2019s documentation on how to do that. This tutorial will use example domain of example.com . gcloud dns managed-zones create \"example-com\" --dns-name \"example.com.\" \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone \"example-com\" --name \"example.com.\" --type NS Outputs: NAME TYPE TTL DATA example.com. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc.","title":"Cloud DNS Zone"},{"location":"tutorials/gke/#cross-project-access-scenario-using-google-service-account","text":"More often, following best practices in regards to security and operations, Cloud DNS zones will be managed in a separate project from the Kubernetes cluster. This section shows how setup ExternalDNS to access Cloud DNS from a different project. These steps will also work for single project scenarios as well. ExternalDNS will need permissions to make changes to the Cloud DNS zone. There are three ways to configure the access needed: Worker Node Service Account Static Credentials Work Load Identity","title":"Cross project access scenario using Google Service Account"},{"location":"tutorials/gke/#setup-cloud-dns-and-gke","text":"Below are examples on how you can configure Cloud DNS and GKE in separate projects, and then use one of the three methods to grant access to ExternalDNS. Replace the environment variables to values that make sense in your environment.","title":"Setup Cloud DNS and GKE"},{"location":"tutorials/gke/#configure-projects","text":"For this process, create projects with the appropriate APIs enabled. # set variables to appropriate desired values GKE_PROJECT_ID = \"my-workload-project\" DNS_PROJECT_ID = \"my-cloud-dns-project\" ClOUD_BILLING_ACCOUNT = \"<my-cloud-billing-account>\" # enable billing and APIs for DNS project if not done already gcloud config set project $DNS_PROJECT_ID gcloud beta billing projects link $CLOUD_DNS_PROJECT \\ --billing-account $ClOUD_BILLING_ACCOUNT gcloud services enable \"dns.googleapis.com\" # enable billing and APIs for GKE project if not done already gcloud config set project $GKE_PROJECT_ID gcloud beta billing projects link $CLOUD_DNS_PROJECT \\ --billing-account $ClOUD_BILLING_ACCOUNT gcloud services enable \"container.googleapis.com\"","title":"Configure Projects"},{"location":"tutorials/gke/#provisioning-cloud-dns","text":"Create a Cloud DNS zone in the designated DNS project. gcloud dns managed-zones create \"example-com\" --project $DNS_PROJECT_ID \\ --description \"example.com\" --dns-name = \"example.com.\" --visibility = public If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the nameServers key. Please consult your registrar\u2019s documentation on how to do that. The example domain of example.com will be used for this tutorial.","title":"Provisioning Cloud DNS"},{"location":"tutorials/gke/#provisioning-a-gke-cluster-for-cross-project-access","text":"Create a GSA (Google Service Account) and grant it the minimal set of privileges required for GKE nodes: GKE_CLUSTER_NAME = \"my-external-dns-cluster\" GKE_REGION = \"us-central1\" GKE_SA_NAME = \"worker-nodes-sa\" GKE_SA_EMAIL = \" $GKE_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" ROLES =( roles/logging.logWriter roles/monitoring.metricWriter roles/monitoring.viewer roles/stackdriver.resourceMetadata.writer ) gcloud iam service-accounts create $GKE_SA_NAME \\ --display-name $GKE_SA_NAME --project $GKE_PROJECT_ID # assign google service account to roles in GKE project for ROLE in ${ ROLES [*] } ; do gcloud projects add-iam-policy-binding $GKE_PROJECT_ID \\ --member \"serviceAccount: $GKE_SA_EMAIL \" \\ --role $ROLE done Create a cluster using this service account and enable workload identity : gcloud container clusters create $GKE_CLUSTER_NAME \\ --project $GKE_PROJECT_ID --region $GKE_REGION --num-nodes 1 \\ --service-account \" $GKE_SA_EMAIL \" \\ --workload-pool \" $GKE_PROJECT_ID .svc.id.goog\"","title":"Provisioning a GKE cluster for cross project access"},{"location":"tutorials/gke/#worker-node-service-account-method","text":"In this method, the GSA (Google Service Account) that is associated with GKE worker nodes will be configured to have access to Cloud DNS. WARNING : This will grant access to modify the Cloud DNS zone records for all containers running on cluster, not just ExternalDNS, so use this option with caution. This is not recommended for production environments. GKE_SA_EMAIL = \" $GKE_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" # assign google service account to dns.admin role in the cloud dns project gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $GKE_SA_EMAIL \\ --role roles/dns.admin After this, follow the steps in Deploy ExternalDNS . Make sure to set the --google-project flag to match the Cloud DNS project name.","title":"Worker Node Service Account method"},{"location":"tutorials/gke/#static-credentials","text":"In this scenario, a new GSA (Google Service Account) is created that has access to the CloudDNS zone. The credentials for this GSA are saved and installed as a Kubernetes secret that will be used by ExternalDNS. This allows only containers that have access to the secret, such as ExternalDNS to update records on the Cloud DNS Zone.","title":"Static Credentials"},{"location":"tutorials/gke/#create-gsa-for-use-with-static-credentials","text":"DNS_SA_NAME = \"external-dns-sa\" DNS_SA_EMAIL = \" $DNS_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" # create GSA used to access the Cloud DNS zone gcloud iam service-accounts create $DNS_SA_NAME --display-name $DNS_SA_NAME # assign google service account to dns.admin role in cloud-dns project gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $DNS_SA_EMAIL --role \"roles/dns.admin\"","title":"Create GSA for use with static credentials"},{"location":"tutorials/gke/#create-kubernetes-secret-using-static-credentials","text":"Generate static credentials from the ExternalDNS GSA. # download static credentials gcloud iam service-accounts keys create /local/path/to/credentials.json \\ --iam-account $DNS_SA_EMAIL Create a Kubernetes secret with the credentials in the same namespace of ExternalDNS. kubectl create secret generic \"external-dns\" --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --from-file /local/path/to/credentials.json After this, follow the steps in Deploy ExternalDNS . Make sure to set the --google-project flag to match Cloud DNS project name. Make sure to uncomment out the section that mounts the secret to the ExternalDNS pods.","title":"Create Kubernetes secret using static credentials"},{"location":"tutorials/gke/#workload-identity","text":"Workload Identity allows workloads in your GKE cluster to impersonate GSA (Google Service Accounts) using KSA (Kubernetes Service Accounts) configured during deployemnt. These are the steps to use this feature with ExternalDNS.","title":"Workload Identity"},{"location":"tutorials/gke/#create-gsa-for-use-with-workload-identity","text":"DNS_SA_NAME = \"external-dns-sa\" DNS_SA_EMAIL = \" $DNS_SA_NAME @ ${ GKE_PROJECT_ID } .iam.gserviceaccount.com\" gcloud iam service-accounts create $DNS_SA_NAME --display-name $DNS_SA_NAME gcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\ --member serviceAccount: $DNS_SA_EMAIL --role \"roles/dns.admin\"","title":"Create GSA for use with Workload Identity"},{"location":"tutorials/gke/#link-ksa-to-gsa","text":"Add an IAM policy binding bewtween the workload identity GSA and ExternalDNS GSA. This will link the ExternalDNS KSA to ExternalDNS GSA. gcloud iam service-accounts add-iam-policy-binding $DNS_SA_EMAIL \\ --role \"roles/iam.workloadIdentityUser\" \\ --member \"serviceAccount: $GKE_PROJECT_ID .svc.id.goog[ ${ EXTERNALDNS_NS :- \"default\" } /external-dns]\"","title":"Link KSA to GSA"},{"location":"tutorials/gke/#deploy-external-dns","text":"Deploy ExternalDNS with the following steps below, documented under Deploy ExternalDNS . Set the --google-project flag to the Cloud DNS project name.","title":"Deploy External DNS"},{"location":"tutorials/gke/#link-ksa-to-gsa-in-kubernetes","text":"Add the proper workload identity annotation to the ExternalDNS KSA. kubectl annotate serviceaccount \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ \"iam.gke.io/gcp-service-account= $DNS_SA_EMAIL \"","title":"Link KSA to GSA in Kubernetes"},{"location":"tutorials/gke/#update-externaldns-pods","text":"Update the Pod spec to schedule the workloads on nodes that use Workload Identity and to use the annotated Kubernetes service account. kubectl patch deployment \"external-dns\" \\ --namespace ${ EXTERNALDNS_NS :- \"default\" } \\ --patch \\ '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"iam.gke.io/gke-metadata-server-enabled\": \"true\"}}}}}' After all of these steps you may see several messages with googleapi: Error 403: Forbidden, forbidden . After several minutes when the token is refreshed, these error messages will go away, and you should see info messages, such as: All records are already up to date .","title":"Update ExternalDNS pods"},{"location":"tutorials/gke/#deploy-externaldns","text":"Then apply the following manifests file to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns labels : app.kubernetes.io/name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns labels : app.kubernetes.io/name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" , \"nodes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer labels : app.kubernetes.io/name : external-dns roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default # change if namespace is not 'default' --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns labels : app.kubernetes.io/name : external-dns spec : strategy : type : Recreate selector : matchLabels : app.kubernetes.io/name : external-dns template : metadata : labels : app.kubernetes.io/name : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --log-format=json # google cloud logs parses severity of the \"text\" log format incorrectly # - --google-project=my-cloud-dns-project # Use this to specify a project different from the one external-dns is running inside - --google-zone-visibility=public # Use this to filter to only zones with this visibility. Set to either 'public' or 'private'. Omitting will match public and private zones - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier # # uncomment below if static credentials are used # env: # - name: GOOGLE_APPLICATION_CREDENTIALS # value: /etc/secrets/service-account/credentials.json # volumeMounts: # - name: google-service-account # mountPath: /etc/secrets/service-account/ # volumes: # - name: google-service-account # secret: # secretName: external-dns Create the deployment for ExternalDNS: kubectl create --namespace \"default\" --filename externaldns.yaml","title":"Deploy ExternalDNS"},{"location":"tutorials/gke/#verify-externaldns-works","text":"The following will deploy a small nginx server that will be used to demonstrate that ExternalDNS is working.","title":"Verify ExternalDNS works"},{"location":"tutorials/gke/#verify-using-an-external-load-balancer","text":"Create the following sample application to test that ExternalDNS works. This example will provision a L4 load balancer. apiVersion : v1 kind : Service metadata : name : nginx annotations : # change nginx.example.com to match an appropriate value external-dns.alpha.kubernetes.io/hostname : nginx.example.com spec : type : LoadBalancer ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 Create the deployment and service objects: kubectl create --namespace \"default\" --filename nginx.yaml After roughly two minutes check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone \"example-com\" --name \"nginx.example.com.\" Example output: NAME TYPE TTL DATA nginx.example.com. A 300 104.155.60.49 nginx.example.com. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier\" Note created TXT record alongside A record. TXT record signifies that the corresponding A record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means. Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first. dig +short @ns-cloud-e1.googledomains.com. nginx.example.com. 104 .155.60.49 Given you hooked up your DNS zone with its parent zone you can use curl to access your site. curl nginx.example.com","title":"Verify using an external load balancer"},{"location":"tutorials/gke/#verify-using-an-ingress","text":"Let\u2019s check that Ingress works as well. Create the following Ingress. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx spec : rules : - host : server.example.com http : paths : - path : / pathType : Prefix backend : service : name : nginx port : number : 80 Create the ingress objects with: kubectl create --namespace \"default\" --filename ingress.yaml Note that this will ingress object will use the default ingress controller that comes with GKE to create a L7 load balancer in addition to the L4 load balancer previously with the service object. To use only the L7 load balancer, update the service manafest to change the Service type to NodePort and remove the ExternalDNS annotation. After roughly two minutes check that a corresponding DNS record for your Ingress was created. gcloud dns record-sets list \\ --zone \"example-com\" \\ --name \"server.example.com.\" \\ Output: NAME TYPE TTL DATA server.example.com. A 300 130.211.46.224 server.example.com. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier\" Let\u2019s check that we can resolve this DNS name as well. dig +short @ns-cloud-e1.googledomains.com. server.example.com. 130 .211.46.224 Try with curl as well. curl server.example.com","title":"Verify using an ingress"},{"location":"tutorials/gke/#clean-up","text":"Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers get cleaned up correctly. kubectl delete service nginx kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. gcloud dns managed-zones delete \"example-com\" gcloud container clusters delete \"external-dns\"","title":"Clean up"},{"location":"tutorials/gloo-proxy/","text":"Configuring ExternalDNS to use the Gloo Proxy Source \u00b6 This tutorial describes how to configure ExternalDNS to use the Gloo Proxy source. It is meant to supplement the other provider-specific setup tutorials. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=gloo-proxy - --gloo-namespace=custom-gloo-system # gloo system namespace. Omit to use the default (gloo-system) - --provider=aws - --registry=txt - --txt-owner-id=my-identifier Manifest (for clusters with RBAC enabled) \u00b6 Could be change if you have mulitple sources apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"gloo.solo.io\" ] resources : [ \"proxies\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"gateway.solo.io\" ] resources : [ \"virtualservices\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=gloo-proxy - --gloo-namespace=custom-gloo-system # gloo system namespace. Omit to use the default (gloo-system) - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Configuring ExternalDNS to use the Gloo Proxy Source"},{"location":"tutorials/gloo-proxy/#configuring-externaldns-to-use-the-gloo-proxy-source","text":"This tutorial describes how to configure ExternalDNS to use the Gloo Proxy source. It is meant to supplement the other provider-specific setup tutorials.","title":"Configuring ExternalDNS to use the Gloo Proxy Source"},{"location":"tutorials/gloo-proxy/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=gloo-proxy - --gloo-namespace=custom-gloo-system # gloo system namespace. Omit to use the default (gloo-system) - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/gloo-proxy/#manifest-for-clusters-with-rbac-enabled","text":"Could be change if you have mulitple sources apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"gloo.solo.io\" ] resources : [ \"proxies\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"gateway.solo.io\" ] resources : [ \"virtualservices\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=gloo-proxy - --gloo-namespace=custom-gloo-system # gloo system namespace. Omit to use the default (gloo-system) - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/godaddy/","text":"Setting up ExternalDNS for Services on GoDaddy \u00b6 This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using GoDaddy DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial. Creating a zone with GoDaddy DNS \u00b6 If you are new to GoDaddy, we recommend you first read the following instructions for creating a zone. Creating a zone using the GoDaddy web console Creating a zone using the GoDaddy API Creating GoDaddy API key \u00b6 You first need to create an API Key. Using the GoDaddy documentation you will have your API key and API secret Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=godaddy - --txt-prefix=external-dns. # In case of multiple k8s cluster - --txt-owner-id=owner-id # In case of multiple k8s cluster - --godaddy-api-key=<Your API Key> - --godaddy-api-secret=<Your API secret> Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"endpoints\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=godaddy - --txt-prefix=external-dns. # In case of multiple k8s cluster - --txt-owner-id=owner-id # In case of multiple k8s cluster - --godaddy-api-key=<Your API Key> - --godaddy-api-secret=<Your API secret> Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the GoDaddy DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service \u00b6 $ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the GoDaddy DNS records. Verifying GoDaddy DNS records \u00b6 Use the GoDaddy web console or API to verify that the A record for your domain shows the external IP address of the services. Cleanup \u00b6 Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Setting up ExternalDNS for Services on GoDaddy"},{"location":"tutorials/godaddy/#setting-up-externaldns-for-services-on-godaddy","text":"This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using GoDaddy DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on GoDaddy"},{"location":"tutorials/godaddy/#creating-a-zone-with-godaddy-dns","text":"If you are new to GoDaddy, we recommend you first read the following instructions for creating a zone. Creating a zone using the GoDaddy web console Creating a zone using the GoDaddy API","title":"Creating a zone with GoDaddy DNS"},{"location":"tutorials/godaddy/#creating-godaddy-api-key","text":"You first need to create an API Key. Using the GoDaddy documentation you will have your API key and API secret","title":"Creating GoDaddy API key"},{"location":"tutorials/godaddy/#deploy-externaldns","text":"Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:","title":"Deploy ExternalDNS"},{"location":"tutorials/godaddy/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=godaddy - --txt-prefix=external-dns. # In case of multiple k8s cluster - --txt-owner-id=owner-id # In case of multiple k8s cluster - --godaddy-api-key=<Your API Key> - --godaddy-api-secret=<Your API secret>","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/godaddy/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"\" ] resources : [ \"endpoints\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=godaddy - --txt-prefix=external-dns. # In case of multiple k8s cluster - --txt-owner-id=owner-id # In case of multiple k8s cluster - --godaddy-api-key=<Your API Key> - --godaddy-api-secret=<Your API secret>","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/godaddy/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the GoDaddy DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/godaddy/#create-the-deployment-and-service","text":"$ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the GoDaddy DNS records.","title":"Create the deployment and service"},{"location":"tutorials/godaddy/#verifying-godaddy-dns-records","text":"Use the GoDaddy web console or API to verify that the A record for your domain shows the external IP address of the services.","title":"Verifying GoDaddy DNS records"},{"location":"tutorials/godaddy/#cleanup","text":"Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/hostport/","text":"Setting up ExternalDNS for Headless Services \u00b6 This tutorial describes how to setup ExternalDNS for usage in conjunction with a Headless service. Use cases \u00b6 The main use cases that inspired this feature is the necessity for fixed addressable hostnames with services, such as Kafka when trying to access them from outside the cluster. In this scenario, quite often, only the Node IP addresses are actually routable and as in systems like Kafka more direct connections are preferable. Setup \u00b6 We will go through a small example of deploying a simple Kafka with use of a headless service. External DNS \u00b6 A simple deploy could look like this: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org Kafka Stateful Set \u00b6 First lets deploy a Kafka Stateful set, a simple example(a lot of stuff is missing) with a headless service called ksvc apiVersion : apps/v1beta1 kind : StatefulSet metadata : name : kafka spec : serviceName : ksvc replicas : 3 template : metadata : labels : component : kafka spec : containers : - name : kafka image : confluent/kafka ports : - containerPort : 9092 hostPort : 9092 name : external command : - bash - -c - \" export DOMAIN=$(hostname -d) && \\ export KAFKA_BROKER_ID=$(echo $HOSTNAME|rev|cut -d '-' -f 1|rev) && \\ export KAFKA_ZOOKEEPER_CONNECT=$ZK_CSVC_SERVICE_HOST:$ZK_CSVC_SERVICE_PORT && \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://$HOSTNAME.example.org:9092 && \\ /etc/confluent/docker/run\" volumeMounts : - name : datadir mountPath : /var/lib/kafka volumeClaimTemplates : - metadata : name : datadir annotations : volume.beta.kubernetes.io/storage-class : st1 spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 500Gi Very important here, is to set the hostPort (only works if the PodSecurityPolicy allows it)! and in case your app requires an actual hostname inside the container, unlike Kafka, which can advertise on another address, you have to set the hostname yourself. Headless Service \u00b6 Now we need to define a headless service to use to expose the Kafka pods. There are generally two approaches to use expose the nodeport of a Headless service: Add --fqdn-template={{name}}.example.org Use a full annotation If you go with #1, you just need to define the headless service, here is an example of the case #2: apiVersion : v1 kind : Service metadata : name : ksvc annotations : external-dns.alpha.kubernetes.io/hostname : example.org spec : ports : - port : 9092 name : external clusterIP : None selector : component : kafka This will create 3 dns records: kafka-0.example.org kafka-1.example.org kafka-2.example.org If you set --fqdn-template={{name}}.example.org you can omit the annotation. Generally it is a better approach to use --fqdn-template={{name}}.example.org , because then you would get the service name inside the generated A records: kafka-0.ksvc.example.org kafka-1.ksvc.example.org kafka-2.ksvc.example.org Using pods\u2019 HostIPs as targets \u00b6 Add the following annotation to your Service : external-dns.alpha.kubernetes.io/endpoints-type : HostIP external-dns will now publish the value of the .status.hostIP field of the pods backing your Service . Using node external IPs as targets \u00b6 Add the following annotation to your Service : external-dns.alpha.kubernetes.io/endpoints-type : NodeExternalIP external-dns will now publish the node external IP ( .status.addresses entries of with type: NodeExternalIP ) of the nodes on which the pods backing your Service are running. Using pod annotations to specify target IPs \u00b6 Add the following annotation to the pods backing your Service : external-dns.alpha.kubernetes.io/target : \"1.2.3.4\" external-dns will publish the IP specified in the annotation of each pod instead of using the podIP advertised by Kubernetes. This can be useful e.g. if you are NATing public IPs onto your pod IPs and want to publish these in DNS.","title":"Setting up ExternalDNS for Headless Services"},{"location":"tutorials/hostport/#setting-up-externaldns-for-headless-services","text":"This tutorial describes how to setup ExternalDNS for usage in conjunction with a Headless service.","title":"Setting up ExternalDNS for Headless Services"},{"location":"tutorials/hostport/#use-cases","text":"The main use cases that inspired this feature is the necessity for fixed addressable hostnames with services, such as Kafka when trying to access them from outside the cluster. In this scenario, quite often, only the Node IP addresses are actually routable and as in systems like Kafka more direct connections are preferable.","title":"Use cases"},{"location":"tutorials/hostport/#setup","text":"We will go through a small example of deploying a simple Kafka with use of a headless service.","title":"Setup"},{"location":"tutorials/hostport/#external-dns","text":"A simple deploy could look like this:","title":"External DNS"},{"location":"tutorials/hostport/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/hostport/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --log-level=debug - --source=service - --source=ingress - --namespace=dev - --domain-filter=example.org. - --provider=aws - --registry=txt - --txt-owner-id=dev.example.org","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/hostport/#kafka-stateful-set","text":"First lets deploy a Kafka Stateful set, a simple example(a lot of stuff is missing) with a headless service called ksvc apiVersion : apps/v1beta1 kind : StatefulSet metadata : name : kafka spec : serviceName : ksvc replicas : 3 template : metadata : labels : component : kafka spec : containers : - name : kafka image : confluent/kafka ports : - containerPort : 9092 hostPort : 9092 name : external command : - bash - -c - \" export DOMAIN=$(hostname -d) && \\ export KAFKA_BROKER_ID=$(echo $HOSTNAME|rev|cut -d '-' -f 1|rev) && \\ export KAFKA_ZOOKEEPER_CONNECT=$ZK_CSVC_SERVICE_HOST:$ZK_CSVC_SERVICE_PORT && \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://$HOSTNAME.example.org:9092 && \\ /etc/confluent/docker/run\" volumeMounts : - name : datadir mountPath : /var/lib/kafka volumeClaimTemplates : - metadata : name : datadir annotations : volume.beta.kubernetes.io/storage-class : st1 spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 500Gi Very important here, is to set the hostPort (only works if the PodSecurityPolicy allows it)! and in case your app requires an actual hostname inside the container, unlike Kafka, which can advertise on another address, you have to set the hostname yourself.","title":"Kafka Stateful Set"},{"location":"tutorials/hostport/#headless-service","text":"Now we need to define a headless service to use to expose the Kafka pods. There are generally two approaches to use expose the nodeport of a Headless service: Add --fqdn-template={{name}}.example.org Use a full annotation If you go with #1, you just need to define the headless service, here is an example of the case #2: apiVersion : v1 kind : Service metadata : name : ksvc annotations : external-dns.alpha.kubernetes.io/hostname : example.org spec : ports : - port : 9092 name : external clusterIP : None selector : component : kafka This will create 3 dns records: kafka-0.example.org kafka-1.example.org kafka-2.example.org If you set --fqdn-template={{name}}.example.org you can omit the annotation. Generally it is a better approach to use --fqdn-template={{name}}.example.org , because then you would get the service name inside the generated A records: kafka-0.ksvc.example.org kafka-1.ksvc.example.org kafka-2.ksvc.example.org","title":"Headless Service"},{"location":"tutorials/hostport/#using-pods-hostips-as-targets","text":"Add the following annotation to your Service : external-dns.alpha.kubernetes.io/endpoints-type : HostIP external-dns will now publish the value of the .status.hostIP field of the pods backing your Service .","title":"Using pods' HostIPs as targets"},{"location":"tutorials/hostport/#using-node-external-ips-as-targets","text":"Add the following annotation to your Service : external-dns.alpha.kubernetes.io/endpoints-type : NodeExternalIP external-dns will now publish the node external IP ( .status.addresses entries of with type: NodeExternalIP ) of the nodes on which the pods backing your Service are running.","title":"Using node external IPs as targets"},{"location":"tutorials/hostport/#using-pod-annotations-to-specify-target-ips","text":"Add the following annotation to the pods backing your Service : external-dns.alpha.kubernetes.io/target : \"1.2.3.4\" external-dns will publish the IP specified in the annotation of each pod instead of using the podIP advertised by Kubernetes. This can be useful e.g. if you are NATing public IPs onto your pod IPs and want to publish these in DNS.","title":"Using pod annotations to specify target IPs"},{"location":"tutorials/ibmcloud/","text":"Setting up ExternalDNS for Services on IBMCloud \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using IBMCloud DNS. This tutorial uses IBMCloud CLI for all IBM Cloud commands and assumes that the Kubernetes cluster was created via IBM Cloud Kubernetes Service and kubectl commands are being run on an orchestration node. Creating a IBMCloud DNS zone \u00b6 The IBMCloud provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. For public zone, This tutorial assume that the IBMCloud Internet Services was provisioned and the cis cli plugin was installed with IBMCloud CLI For private zone, This tutorial assume that the IBMCloud DNS Services was provisioned and the dns cli plugin was installed with IBMCloud CLI Public Zone \u00b6 For this tutorial, we create public zone named example.com on IBMCloud Internet Services instance external-dns-public $ ibmcloud cis domain-add example.com -i external-dns-public Follow step to active your zone Private Zone \u00b6 For this tutorial, we create private zone named example.com on IBMCloud DNS Services instance external-dns-private $ ibmcloud dns zone-create example.com -i external-dns-private Creating configuration file \u00b6 The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named azure.json with content similar to this: { \"apiKey\": \"1234567890abcdefghijklmnopqrstuvwxyz\", \"instanceCrn\": \"crn:v1:bluemix:public:internet-svcs:global:a/bcf1865e99742d38d2d5fc3fb80a5496:b950da8a-5be6-4691-810e-36388c77b0a3::\" } You can create or find the apiKey in your ibmcloud IAM \u2192 API Keys page You can find the instanceCrn in your service instance details Now you can create a file named \u2018ibmcloud.json\u2019 with values gathered above and with the structure of the example above. Use this file to create a Kubernetes secret: $ kubectl create secret generic ibmcloud-config-file --from-file=/local/path/to/ibmcloud.json Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ibmcloud - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud volumeMounts : - name : ibmcloud-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : ibmcloud-config-file secret : secretName : ibmcloud-config-file items : - key : externaldns-config.json path : ibmcloud.json Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ibmcloud - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud public zone volumeMounts : - name : ibmcloud-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : ibmcloud-config-file secret : secretName : ibmcloud-config-file items : - key : externaldns-config.json path : ibmcloud.json Deploying an Nginx Service \u00b6 Create a service file called nginx.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : www.example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the hostname as the IBMCloud DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the IBMCloud DNS records. Verifying IBMCloud DNS records \u00b6 Run the following command to view the A records: Public Zone \u00b6 # Get the domain ID with below command on IBMCloud Internet Services instance `external-dns-public` $ ibmcloud cis domains -i external-dns-public # Get the records with domain ID $ ibmcloud cis dns-records DOMAIN_ID -i external-dns-public Private Zone \u00b6 # Get the domain ID with below command on IBMCloud DNS Services instance `external-dns-private` $ ibmcloud dns zones -i external-dns-private # Get the records with domain ID $ ibmcloud dns resource-records ZONE_ID -i external-dns-public This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage IBMCloud DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml Setting proxied records on public zone \u00b6 Using the external-dns.alpha.kubernetes.io/ibmcloud-proxied: \"true\" annotation on your ingress or service, you can specify if the proxy feature of IBMCloud public DNS should be enabled for that record. This setting will override the global --ibmcloud-proxied setting. Active priviate zone with VPC allocated \u00b6 By default, IBMCloud DNS Services don\u2019t active your private zone with new zone added, with externale DNS, you can use external-dns.alpha.kubernetes.io/ibmcloud-vpc: \"crn:v1:bluemix:public:is:us-south:a/bcf1865e99742d38d2d5fc3fb80a5496::vpc:r006-74353823-a60d-42e4-97c5-5e2551278435\" annotation on your ingress or service, it will active your private zone with in specific VPC for that record created in. this setting won\u2019t work if the private zone was active already. Note: the annotaion value is the VPC CRN, every IBM Cloud service have a valid CRN.","title":"Setting up ExternalDNS for Services on IBMCloud"},{"location":"tutorials/ibmcloud/#setting-up-externaldns-for-services-on-ibmcloud","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using IBMCloud DNS. This tutorial uses IBMCloud CLI for all IBM Cloud commands and assumes that the Kubernetes cluster was created via IBM Cloud Kubernetes Service and kubectl commands are being run on an orchestration node.","title":"Setting up ExternalDNS for Services on IBMCloud"},{"location":"tutorials/ibmcloud/#creating-a-ibmcloud-dns-zone","text":"The IBMCloud provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. For public zone, This tutorial assume that the IBMCloud Internet Services was provisioned and the cis cli plugin was installed with IBMCloud CLI For private zone, This tutorial assume that the IBMCloud DNS Services was provisioned and the dns cli plugin was installed with IBMCloud CLI","title":"Creating a IBMCloud DNS zone"},{"location":"tutorials/ibmcloud/#public-zone","text":"For this tutorial, we create public zone named example.com on IBMCloud Internet Services instance external-dns-public $ ibmcloud cis domain-add example.com -i external-dns-public Follow step to active your zone","title":"Public Zone"},{"location":"tutorials/ibmcloud/#private-zone","text":"For this tutorial, we create private zone named example.com on IBMCloud DNS Services instance external-dns-private $ ibmcloud dns zone-create example.com -i external-dns-private","title":"Private Zone"},{"location":"tutorials/ibmcloud/#creating-configuration-file","text":"The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named azure.json with content similar to this: { \"apiKey\": \"1234567890abcdefghijklmnopqrstuvwxyz\", \"instanceCrn\": \"crn:v1:bluemix:public:internet-svcs:global:a/bcf1865e99742d38d2d5fc3fb80a5496:b950da8a-5be6-4691-810e-36388c77b0a3::\" } You can create or find the apiKey in your ibmcloud IAM \u2192 API Keys page You can find the instanceCrn in your service instance details Now you can create a file named \u2018ibmcloud.json\u2019 with values gathered above and with the structure of the example above. Use this file to create a Kubernetes secret: $ kubectl create secret generic ibmcloud-config-file --from-file=/local/path/to/ibmcloud.json","title":"Creating configuration file"},{"location":"tutorials/ibmcloud/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/ibmcloud/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ibmcloud - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud volumeMounts : - name : ibmcloud-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : ibmcloud-config-file secret : secretName : ibmcloud-config-file items : - key : externaldns-config.json path : ibmcloud.json","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/ibmcloud/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ibmcloud - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud public zone volumeMounts : - name : ibmcloud-config-file mountPath : /etc/kubernetes readOnly : true volumes : - name : ibmcloud-config-file secret : secretName : ibmcloud-config-file items : - key : externaldns-config.json path : ibmcloud.json","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/ibmcloud/#deploying-an-nginx-service","text":"Create a service file called nginx.yaml with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : www.example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the hostname as the IBMCloud DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the IBMCloud DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/ibmcloud/#verifying-ibmcloud-dns-records","text":"Run the following command to view the A records:","title":"Verifying IBMCloud DNS records"},{"location":"tutorials/ibmcloud/#public-zone_1","text":"# Get the domain ID with below command on IBMCloud Internet Services instance `external-dns-public` $ ibmcloud cis domains -i external-dns-public # Get the records with domain ID $ ibmcloud cis dns-records DOMAIN_ID -i external-dns-public","title":"Public Zone"},{"location":"tutorials/ibmcloud/#private-zone_1","text":"# Get the domain ID with below command on IBMCloud DNS Services instance `external-dns-private` $ ibmcloud dns zones -i external-dns-private # Get the records with domain ID $ ibmcloud dns resource-records ZONE_ID -i external-dns-public This should show the external IP address of the service as the A record for your domain.","title":"Private Zone"},{"location":"tutorials/ibmcloud/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage IBMCloud DNS records, we can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/ibmcloud/#setting-proxied-records-on-public-zone","text":"Using the external-dns.alpha.kubernetes.io/ibmcloud-proxied: \"true\" annotation on your ingress or service, you can specify if the proxy feature of IBMCloud public DNS should be enabled for that record. This setting will override the global --ibmcloud-proxied setting.","title":"Setting proxied records on public zone"},{"location":"tutorials/ibmcloud/#active-priviate-zone-with-vpc-allocated","text":"By default, IBMCloud DNS Services don\u2019t active your private zone with new zone added, with externale DNS, you can use external-dns.alpha.kubernetes.io/ibmcloud-vpc: \"crn:v1:bluemix:public:is:us-south:a/bcf1865e99742d38d2d5fc3fb80a5496::vpc:r006-74353823-a60d-42e4-97c5-5e2551278435\" annotation on your ingress or service, it will active your private zone with in specific VPC for that record created in. this setting won\u2019t work if the private zone was active already. Note: the annotaion value is the VPC CRN, every IBM Cloud service have a valid CRN.","title":"Active priviate zone with VPC allocated"},{"location":"tutorials/infoblox/","text":"Setting up ExternalDNS for Infoblox \u00b6 This tutorial describes how to setup ExternalDNS for usage with Infoblox. Make sure to use >=0.4.6 version of ExternalDNS for this tutorial. The only WAPI version that has been validated is v2.3.1 . It is assumed that the API user has rights to create objects of the following types: zone_auth , record:a , record:cname , record:txt . This tutorial assumes you have substituted the correct values for the following environment variables: export GRID_HOST=127.0.0.1 export WAPI_PORT=443 export WAPI_VERSION=2.3.1 export WAPI_USERNAME=admin export WAPI_PASSWORD=infoblox Creating an Infoblox DNS zone \u00b6 The Infoblox provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. Create an Infoblox DNS zone for \u201cexample.com\u201d: $ curl -kl \\ -X POST \\ -d fqdn=example.com \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/zone_auth Substitute a domain you own for \u201cexample.com\u201d if desired. Creating an Infoblox Configuration Secret \u00b6 For ExternalDNS to access the Infoblox API, create a Kubernetes secret. To create the secret: $ kubectl create secret generic external-dns \\ --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME=${WAPI_USERNAME} \\ --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD=${WAPI_PASSWORD} Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains. - --provider=infoblox - --infoblox-grid-host=${GRID_HOST} # (required) IP of the Infoblox Grid host. - --infoblox-wapi-port=443 # (optional) Infoblox WAPI port. The default is \"443\". - --infoblox-wapi-version=2.3.1 # (optional) Infoblox WAPI version. The default is \"2.3.1\" - --infoblox-ssl-verify # (optional) Use --no-infoblox-ssl-verify to skip server certificate verification. - --infoblox-create-ptr # (optional) Use --infoblox-create-ptr to create a ptr entry in addition to an entry. env : - name : EXTERNAL_DNS_INFOBLOX_HTTP_POOL_CONNECTIONS value : \"10\" # (optional) Infoblox WAPI request connection pool size. The default is \"10\". - name : EXTERNAL_DNS_INFOBLOX_HTTP_REQUEST_TIMEOUT value : \"60\" # (optional) Infoblox WAPI request timeout in seconds. The default is \"60\". - name : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME - name : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains. - --provider=infoblox - --infoblox-grid-host=${GRID_HOST} # (required) IP of the Infoblox Grid host. - --infoblox-wapi-port=443 # (optional) Infoblox WAPI port. The default is \"443\". - --infoblox-wapi-version=2.3.1 # (optional) Infoblox WAPI version. The default is \"2.3.1\" - --infoblox-ssl-verify # (optional) Use --no-infoblox-ssl-verify to skip server certificate verification. - --infoblox-create-ptr # (optional) Use --infoblox-create-ptr to create a ptr entry in addition to an entry. env : - name : EXTERNAL_DNS_INFOBLOX_HTTP_POOL_CONNECTIONS value : \"10\" # (optional) Infoblox WAPI request connection pool size. The default is \"10\". - name : EXTERNAL_DNS_INFOBLOX_HTTP_REQUEST_TIMEOUT value : \"60\" # (optional) Infoblox WAPI request timeout in seconds. The default is \"60\". - name : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME - name : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Infoblox DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml It takes a little while for the Infoblox cloud provider to create an external IP for the service. Check the status by running kubectl get services nginx . If the EXTERNAL-IP field shows an address, the service is ready to be accessed externally. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Infoblox DNS records. Verifying Infoblox DNS records \u00b6 Run the following command to view the A records for your Infoblox DNS zone: $ curl -kl \\ -X GET \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/record:a?zone=example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself). Clean up \u00b6 Now that we have verified that ExternalDNS will automatically manage Infoblox DNS records, we can delete the tutorial\u2019s DNS zone: $ curl -kl \\ -X DELETE \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/zone_auth?fqdn=example.com Ability to filter results from the zone auth API using a regular expression \u00b6 There is also the ability to filter results from the Infoblox zone_auth service based upon a regular expression. See the Infoblox API document for examples. To use this feature for the zone_auth service, set the parameter infoblox-fqdn-regex for external-dns to a regular expression that makes sense for you. For instance, to only return hosted zones that start with staging in the test.com domain (like staging.beta.test.com, or staging.test.com), use the following command line option when starting external-dns --infoblox-fqdn-regex=^staging.*test.com$ Infoblox PTR record support \u00b6 There is an option to enable PTR records support for infoblox provider. PTR records allow to do reverse dns search. To enable PTR records support, add following into arguments for external-dns: --infoblox-create-ptr to allow management of PTR records. You can also add a filter for reverse dns zone to limit PTR records to specific zones only: --domain-filter=10.196.0.0/16 change this to the reverse zone(s) as defined in your infoblox. Now external-dns will manage PTR records for you.","title":"Setting up ExternalDNS for Infoblox"},{"location":"tutorials/infoblox/#setting-up-externaldns-for-infoblox","text":"This tutorial describes how to setup ExternalDNS for usage with Infoblox. Make sure to use >=0.4.6 version of ExternalDNS for this tutorial. The only WAPI version that has been validated is v2.3.1 . It is assumed that the API user has rights to create objects of the following types: zone_auth , record:a , record:cname , record:txt . This tutorial assumes you have substituted the correct values for the following environment variables: export GRID_HOST=127.0.0.1 export WAPI_PORT=443 export WAPI_VERSION=2.3.1 export WAPI_USERNAME=admin export WAPI_PASSWORD=infoblox","title":"Setting up ExternalDNS for Infoblox"},{"location":"tutorials/infoblox/#creating-an-infoblox-dns-zone","text":"The Infoblox provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. Create an Infoblox DNS zone for \u201cexample.com\u201d: $ curl -kl \\ -X POST \\ -d fqdn=example.com \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/zone_auth Substitute a domain you own for \u201cexample.com\u201d if desired.","title":"Creating an Infoblox DNS zone"},{"location":"tutorials/infoblox/#creating-an-infoblox-configuration-secret","text":"For ExternalDNS to access the Infoblox API, create a Kubernetes secret. To create the secret: $ kubectl create secret generic external-dns \\ --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME=${WAPI_USERNAME} \\ --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD=${WAPI_PASSWORD}","title":"Creating an Infoblox Configuration Secret"},{"location":"tutorials/infoblox/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/infoblox/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains. - --provider=infoblox - --infoblox-grid-host=${GRID_HOST} # (required) IP of the Infoblox Grid host. - --infoblox-wapi-port=443 # (optional) Infoblox WAPI port. The default is \"443\". - --infoblox-wapi-version=2.3.1 # (optional) Infoblox WAPI version. The default is \"2.3.1\" - --infoblox-ssl-verify # (optional) Use --no-infoblox-ssl-verify to skip server certificate verification. - --infoblox-create-ptr # (optional) Use --infoblox-create-ptr to create a ptr entry in addition to an entry. env : - name : EXTERNAL_DNS_INFOBLOX_HTTP_POOL_CONNECTIONS value : \"10\" # (optional) Infoblox WAPI request connection pool size. The default is \"10\". - name : EXTERNAL_DNS_INFOBLOX_HTTP_REQUEST_TIMEOUT value : \"60\" # (optional) Infoblox WAPI request timeout in seconds. The default is \"60\". - name : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME - name : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/infoblox/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains. - --provider=infoblox - --infoblox-grid-host=${GRID_HOST} # (required) IP of the Infoblox Grid host. - --infoblox-wapi-port=443 # (optional) Infoblox WAPI port. The default is \"443\". - --infoblox-wapi-version=2.3.1 # (optional) Infoblox WAPI version. The default is \"2.3.1\" - --infoblox-ssl-verify # (optional) Use --no-infoblox-ssl-verify to skip server certificate verification. - --infoblox-create-ptr # (optional) Use --infoblox-create-ptr to create a ptr entry in addition to an entry. env : - name : EXTERNAL_DNS_INFOBLOX_HTTP_POOL_CONNECTIONS value : \"10\" # (optional) Infoblox WAPI request connection pool size. The default is \"10\". - name : EXTERNAL_DNS_INFOBLOX_HTTP_REQUEST_TIMEOUT value : \"60\" # (optional) Infoblox WAPI request timeout in seconds. The default is \"60\". - name : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME - name : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD valueFrom : secretKeyRef : name : external-dns key : EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/infoblox/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Infoblox DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml It takes a little while for the Infoblox cloud provider to create an external IP for the service. Check the status by running kubectl get services nginx . If the EXTERNAL-IP field shows an address, the service is ready to be accessed externally. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Infoblox DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/infoblox/#verifying-infoblox-dns-records","text":"Run the following command to view the A records for your Infoblox DNS zone: $ curl -kl \\ -X GET \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/record:a?zone=example.com Substitute the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).","title":"Verifying Infoblox DNS records"},{"location":"tutorials/infoblox/#clean-up","text":"Now that we have verified that ExternalDNS will automatically manage Infoblox DNS records, we can delete the tutorial\u2019s DNS zone: $ curl -kl \\ -X DELETE \\ -u ${WAPI_USERNAME}:${WAPI_PASSWORD} \\ https://${GRID_HOST}:${WAPI_PORT}/wapi/v${WAPI_VERSION}/zone_auth?fqdn=example.com","title":"Clean up"},{"location":"tutorials/infoblox/#ability-to-filter-results-from-the-zone-auth-api-using-a-regular-expression","text":"There is also the ability to filter results from the Infoblox zone_auth service based upon a regular expression. See the Infoblox API document for examples. To use this feature for the zone_auth service, set the parameter infoblox-fqdn-regex for external-dns to a regular expression that makes sense for you. For instance, to only return hosted zones that start with staging in the test.com domain (like staging.beta.test.com, or staging.test.com), use the following command line option when starting external-dns --infoblox-fqdn-regex=^staging.*test.com$","title":"Ability to filter results from the zone auth API using a regular expression"},{"location":"tutorials/infoblox/#infoblox-ptr-record-support","text":"There is an option to enable PTR records support for infoblox provider. PTR records allow to do reverse dns search. To enable PTR records support, add following into arguments for external-dns: --infoblox-create-ptr to allow management of PTR records. You can also add a filter for reverse dns zone to limit PTR records to specific zones only: --domain-filter=10.196.0.0/16 change this to the reverse zone(s) as defined in your infoblox. Now external-dns will manage PTR records for you.","title":"Infoblox PTR record support"},{"location":"tutorials/istio/","text":"Configuring ExternalDNS to use the Istio Gateway and/or Istio Virtual Service Source \u00b6 This tutorial describes how to configure ExternalDNS to use the Istio Gateway source. It is meant to supplement the other provider-specific setup tutorials. Note: Using the Istio Gateway source requires Istio >=1.0.0. Manifest (for clusters without RBAC enabled) Manifest (for clusters with RBAC enabled) Update existing ExternalDNS Deployment Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=istio-gateway # choose one - --source=istio-virtualservice # or both - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"networking.istio.io\" ] resources : [ \"gateways\" , \"virtualservices\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=istio-gateway - --source=istio-virtualservice - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier Update existing ExternalDNS Deployment \u00b6 For clusters with running external-dns , you can just update the deployment. With access to the kube-system namespace, update the existing external-dns deployment. Add a parameter to the arguments of the container to create dns entries with --source=istio-gateway . Execute the following command or update the argument. kubectl patch deployment external-dns --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/2\", \"value\": \"--source=istio-gateway\" }]' In case the setup uses a clusterrole , just append a new value to the enable the istio group. kubectl patch clusterrole external-dns --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/rules/4\", \"value\": { \"apiGroups\": [ \"networking.istio.io\"], \"resources\": [\"gateways\"],\"verbs\": [\"get\", \"watch\", \"list\" ]} }]' Verify that Istio Gateway/VirtualService Source works \u00b6 Follow the Istio ingress traffic tutorial to deploy a sample service that will be exposed outside of the service mesh. The following are relevant snippets from that tutorial. Install a sample service \u00b6 With automatic sidecar injection: $ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml Otherwise: $ kubectl apply -f < ( istioctl kube-inject -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml ) Using a Gateway as a source \u00b6 Create an Istio Gateway: \u00b6 $ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway namespace: istio-system spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"httpbin.example.com\" # this is used by external-dns to extract DNS names EOF Configure routes for traffic entering via the Gateway: \u00b6 $ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \"httpbin.example.com\" gateways: - istio-system/httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin EOF Using a VirtualService as a source \u00b6 Create an Istio Gateway: \u00b6 $ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway namespace: istio-system spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" EOF Configure routes for traffic entering via the Gateway: \u00b6 $ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \"httpbin.example.com\" # this is used by external-dns to extract DNS names gateways: - istio-system/httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin EOF Access the sample service using curl \u00b6 $ curl -I http://httpbin.example.com/status/200 HTTP/1.1 200 OK server: envoy date: Tue, 28 Aug 2018 15 :26:47 GMT content-type: text/html ; charset = utf-8 access-control-allow-origin: * access-control-allow-credentials: true content-length: 0 x-envoy-upstream-service-time: 5 Accessing any other URL that has not been explicitly exposed should return an HTTP 404 error: $ curl -I http://httpbin.example.com/headers HTTP/1.1 404 Not Found date: Tue, 28 Aug 2018 15 :27:48 GMT server: envoy transfer-encoding: chunked Note: The -H flag in the original Istio tutorial is no longer necessary in the curl commands. Debug ExternalDNS \u00b6 Look for the deployment pod to see the status ```console$ kubectl get pods | grep external-dns external-dns-6b84999479-4knv9 1/1 Running 0 3h29m * Watch for the logs as follows ```console $ kubectl logs -f external-dns-6b84999479-4knv9 At this point, you can create or update any Istio Gateway object with hosts entries array. ATTENTION : Make sure to specify those whose account is related to the DNS record. Successful executions will print the following time=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com A\" time=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com TXT\" time=\"2020-01-17T06:08:08Z\" level=info msg=\"2 record(s) in zone example.com. were successfully updated\" time=\"2020-01-17T06:09:08Z\" level=info msg=\"All records are already up to date, there are no changes for the matching hosted zones\" If there\u2019s any problem around clusterrole , you would see the errors showing wrong permissions: source \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\" time=\"2020-01-17T06:07:08Z\" level=error msg=\"gateways.networking.istio.io is forbidden: User \\\"system:serviceaccount:kube-system:external-dns\\\" cannot list resource \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\"","title":"Configuring ExternalDNS to use the Istio Gateway and/or Istio Virtual Service Source"},{"location":"tutorials/istio/#configuring-externaldns-to-use-the-istio-gateway-andor-istio-virtual-service-source","text":"This tutorial describes how to configure ExternalDNS to use the Istio Gateway source. It is meant to supplement the other provider-specific setup tutorials. Note: Using the Istio Gateway source requires Istio >=1.0.0. Manifest (for clusters without RBAC enabled) Manifest (for clusters with RBAC enabled) Update existing ExternalDNS Deployment","title":"Configuring ExternalDNS to use the Istio Gateway and/or Istio Virtual Service Source"},{"location":"tutorials/istio/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=istio-gateway # choose one - --source=istio-virtualservice # or both - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/istio/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"networking.istio.io\" ] resources : [ \"gateways\" , \"virtualservices\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --source=istio-gateway - --source=istio-virtualservice - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/istio/#update-existing-externaldns-deployment","text":"For clusters with running external-dns , you can just update the deployment. With access to the kube-system namespace, update the existing external-dns deployment. Add a parameter to the arguments of the container to create dns entries with --source=istio-gateway . Execute the following command or update the argument. kubectl patch deployment external-dns --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/2\", \"value\": \"--source=istio-gateway\" }]' In case the setup uses a clusterrole , just append a new value to the enable the istio group. kubectl patch clusterrole external-dns --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/rules/4\", \"value\": { \"apiGroups\": [ \"networking.istio.io\"], \"resources\": [\"gateways\"],\"verbs\": [\"get\", \"watch\", \"list\" ]} }]'","title":"Update existing ExternalDNS Deployment"},{"location":"tutorials/istio/#verify-that-istio-gatewayvirtualservice-source-works","text":"Follow the Istio ingress traffic tutorial to deploy a sample service that will be exposed outside of the service mesh. The following are relevant snippets from that tutorial.","title":"Verify that Istio Gateway/VirtualService Source works"},{"location":"tutorials/istio/#install-a-sample-service","text":"With automatic sidecar injection: $ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml Otherwise: $ kubectl apply -f < ( istioctl kube-inject -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml )","title":"Install a sample service"},{"location":"tutorials/istio/#using-a-gateway-as-a-source","text":"","title":"Using a Gateway as a source"},{"location":"tutorials/istio/#create-an-istio-gateway","text":"$ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway namespace: istio-system spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"httpbin.example.com\" # this is used by external-dns to extract DNS names EOF","title":"Create an Istio Gateway:"},{"location":"tutorials/istio/#configure-routes-for-traffic-entering-via-the-gateway","text":"$ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \"httpbin.example.com\" gateways: - istio-system/httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin EOF","title":"Configure routes for traffic entering via the Gateway:"},{"location":"tutorials/istio/#using-a-virtualservice-as-a-source","text":"","title":"Using a VirtualService as a source"},{"location":"tutorials/istio/#create-an-istio-gateway_1","text":"$ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway namespace: istio-system spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" EOF","title":"Create an Istio Gateway:"},{"location":"tutorials/istio/#configure-routes-for-traffic-entering-via-the-gateway_1","text":"$ cat <<EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \"httpbin.example.com\" # this is used by external-dns to extract DNS names gateways: - istio-system/httpbin-gateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin EOF","title":"Configure routes for traffic entering via the Gateway:"},{"location":"tutorials/istio/#access-the-sample-service-using-curl","text":"$ curl -I http://httpbin.example.com/status/200 HTTP/1.1 200 OK server: envoy date: Tue, 28 Aug 2018 15 :26:47 GMT content-type: text/html ; charset = utf-8 access-control-allow-origin: * access-control-allow-credentials: true content-length: 0 x-envoy-upstream-service-time: 5 Accessing any other URL that has not been explicitly exposed should return an HTTP 404 error: $ curl -I http://httpbin.example.com/headers HTTP/1.1 404 Not Found date: Tue, 28 Aug 2018 15 :27:48 GMT server: envoy transfer-encoding: chunked Note: The -H flag in the original Istio tutorial is no longer necessary in the curl commands.","title":"Access the sample service using curl"},{"location":"tutorials/istio/#debug-externaldns","text":"Look for the deployment pod to see the status ```console$ kubectl get pods | grep external-dns external-dns-6b84999479-4knv9 1/1 Running 0 3h29m * Watch for the logs as follows ```console $ kubectl logs -f external-dns-6b84999479-4knv9 At this point, you can create or update any Istio Gateway object with hosts entries array. ATTENTION : Make sure to specify those whose account is related to the DNS record. Successful executions will print the following time=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com A\" time=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com TXT\" time=\"2020-01-17T06:08:08Z\" level=info msg=\"2 record(s) in zone example.com. were successfully updated\" time=\"2020-01-17T06:09:08Z\" level=info msg=\"All records are already up to date, there are no changes for the matching hosted zones\" If there\u2019s any problem around clusterrole , you would see the errors showing wrong permissions: source \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\" time=\"2020-01-17T06:07:08Z\" level=error msg=\"gateways.networking.istio.io is forbidden: User \\\"system:serviceaccount:kube-system:external-dns\\\" cannot list resource \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\"","title":"Debug ExternalDNS"},{"location":"tutorials/kong/","text":"Configuring ExternalDNS to use the Kong TCPIngress Source \u00b6 This tutorial describes how to configure ExternalDNS to use the Kong TCPIngress source. It is meant to supplement the other provider-specific setup tutorials. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=kong-tcpingress - --provider=aws - --registry=txt - --txt-owner-id=my-identifier Manifest (for clusters with RBAC enabled) \u00b6 Could be changed if you have mulitple sources apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"configuration.konghq.com\" ] resources : [ \"tcpingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=kong-tcpingress - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Configuring ExternalDNS to use the Kong TCPIngress Source"},{"location":"tutorials/kong/#configuring-externaldns-to-use-the-kong-tcpingress-source","text":"This tutorial describes how to configure ExternalDNS to use the Kong TCPIngress source. It is meant to supplement the other provider-specific setup tutorials.","title":"Configuring ExternalDNS to use the Kong TCPIngress Source"},{"location":"tutorials/kong/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=kong-tcpingress - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/kong/#manifest-for-clusters-with-rbac-enabled","text":"Could be changed if you have mulitple sources apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] - apiGroups : [ \"configuration.konghq.com\" ] resources : [ \"tcpingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns # update this to the desired external-dns version image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=kong-tcpingress - --provider=aws - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/kops-dns-controller/","text":"kOps dns-controller compatibility mode \u00b6 kOps includes a dns-controller that is primarily used to bootstrap the cluster, but can also be used for provisioning DNS entries for Services and Ingress. ExternalDNS can be used as a drop-in replacement for dns-controller if you are running a non-gossip cluster. The flag --compatibility kops-dns-controller enables the dns-controller behaviour. Annotations \u00b6 In kops-dns-controller compatibility mode, ExternalDNS supports two additional annotations: dns.alpha.kubernetes.io/external which is used to define a DNS record for accessing the resource publicly (i.e. public IPs) dns.alpha.kubernetes.io/internal which is used to define a DNS record for accessing the resource from outside the cluster but inside the cloud, i.e. it will typically use internal IPs for instances. These annotations may both be comma-separated lists of names. DNS record mappings \u00b6 The DNS record mappings try to \u201cdo the right thing\u201d, but what this means is different for each resource type. Pods \u00b6 For the external annotation, ExternalDNS will map a HostNetwork=true Pod to the external IPs of the Node. For the internal annotation, ExternalDNS will map a HostNetwork=true Pod to the internal IPs of the Node. ExternalDNS ignore Pods that are not HostNetwork=true Annotations added to Pods will always result in an A record being created. Services \u00b6 For a Service of Type=LoadBalancer, ExternalDNS looks at Status.LoadBalancer.Ingress. It will create CNAMEs to hostnames, and A records for IP addresses. It will do this for both internal and external names For a Service of Type=NodePort, ExternalDNS will create A records for the Node\u2019s internal/external IP addresses, as appropriate.","title":"kOps dns-controller compatibility mode"},{"location":"tutorials/kops-dns-controller/#kops-dns-controller-compatibility-mode","text":"kOps includes a dns-controller that is primarily used to bootstrap the cluster, but can also be used for provisioning DNS entries for Services and Ingress. ExternalDNS can be used as a drop-in replacement for dns-controller if you are running a non-gossip cluster. The flag --compatibility kops-dns-controller enables the dns-controller behaviour.","title":"kOps dns-controller compatibility mode"},{"location":"tutorials/kops-dns-controller/#annotations","text":"In kops-dns-controller compatibility mode, ExternalDNS supports two additional annotations: dns.alpha.kubernetes.io/external which is used to define a DNS record for accessing the resource publicly (i.e. public IPs) dns.alpha.kubernetes.io/internal which is used to define a DNS record for accessing the resource from outside the cluster but inside the cloud, i.e. it will typically use internal IPs for instances. These annotations may both be comma-separated lists of names.","title":"Annotations"},{"location":"tutorials/kops-dns-controller/#dns-record-mappings","text":"The DNS record mappings try to \u201cdo the right thing\u201d, but what this means is different for each resource type.","title":"DNS record mappings"},{"location":"tutorials/kops-dns-controller/#pods","text":"For the external annotation, ExternalDNS will map a HostNetwork=true Pod to the external IPs of the Node. For the internal annotation, ExternalDNS will map a HostNetwork=true Pod to the internal IPs of the Node. ExternalDNS ignore Pods that are not HostNetwork=true Annotations added to Pods will always result in an A record being created.","title":"Pods"},{"location":"tutorials/kops-dns-controller/#services","text":"For a Service of Type=LoadBalancer, ExternalDNS looks at Status.LoadBalancer.Ingress. It will create CNAMEs to hostnames, and A records for IP addresses. It will do this for both internal and external names For a Service of Type=NodePort, ExternalDNS will create A records for the Node\u2019s internal/external IP addresses, as appropriate.","title":"Services"},{"location":"tutorials/kube-ingress-aws/","text":"Using ExternalDNS with kube-ingress-aws-controller \u00b6 This tutorial describes how to use ExternalDNS with the kube-ingress-aws-controller . Setting up ExternalDNS and kube-ingress-aws-controller \u00b6 Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the source=ingress argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the ingress-class argument, but this is not required. For help setting up the Kubernetes Ingress AWS Controller, that can create ALBs and NLBs, follow the Setup Guide . Optional RouteGroup \u00b6 RouteGroup is a CRD, that enables you to do complex routing with Skipper . First, you have to apply the RouteGroup CRD to your cluster: kubectl apply -f https://github.com/zalando/skipper/blob/HEAD/dataclients/kubernetes/deploy/apply/routegroups_crd.yaml You have to grant all controllers: Skipper , kube-ingress-aws-controller and external-dns to read the routegroup resource and kube-ingress-aws-controller to update the status field of a routegroup. This depends on your RBAC policies, in case you use RBAC, you can use this for all 3 controllers: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kube-ingress-aws-controller rules : - apiGroups : - extensions - networking.k8s.io resources : - ingresses verbs : - get - list - watch - apiGroups : - extensions - networking.k8s.io resources : - ingresses/status verbs : - patch - update - apiGroups : - zalando.org resources : - routegroups verbs : - get - list - watch - apiGroups : - zalando.org resources : - routegroups/status verbs : - patch - update See also current RBAC yaml files: - kube-ingress-aws-controller - skipper - external-dns Deploy an example application \u00b6 Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ingress objects, that were created by kube-ingress-aws-controller . apiVersion : apps/v1 kind : Deployment metadata : name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver template : metadata : labels : app : echoserver spec : containers : - image : gcr.io/google_containers/echoserver:1.4 imagePullPolicy : Always name : echoserver ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : echoserver spec : ports : - port : 80 targetPort : 8080 protocol : TCP type : ClusterIP selector : app : echoserver Note that the Service object is of type ClusterIP , because we will target Skipper and do the HTTP routing in Skipper. We don\u2019t need a Service of type LoadBalancer here, since we will be using a shared skipper-ingress for all Ingress. Skipper use hostNetwork to be able to get traffic from AWS LoadBalancers EC2 network. ALBs or NLBs, will be created based on need and will be shared across all ingress as default. Ingress examples \u00b6 Create the following Ingress to expose the echoserver application to the Internet. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.mycluster.example.org http : &echoserver_root paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix - host : echoserver.example.org http : *echoserver_root The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to skipper which will forward to the echoserver application. If the --source=ingress argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records being created, echoserver.mycluster.example.org and echoserver.example.org , which both alias the ALB that is associated with the Ingress object. Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : external-dns.alpha.kubernetes.io/hostname : echoserver.mycluster.example.org, echoserver.example.org kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix In the above example we create a default path that works for any hostname, and make use of the external-dns.alpha.kubernetes.io/hostname annotation to create multiple aliases for the resulting ALB. Dualstack ALBs \u00b6 AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs. The Kubernetes Ingress AWS controller supports the alb.ingress.kubernetes.io/ip-address-type annotation (which defaults to ipv4 ) to determine this. If this annotation is set to dualstack then ExternalDNS will create two alias records (one A record and one AAAA record) for each hostname associated with the Ingress object. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/ip-address-type : dualstack kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an ALB with a dualstack interface. ExternalDNS will create both an A echoserver.example.org record and an AAAA record of the same name, that each are aliases for the same ALB. NLBs \u00b6 AWS has NLBs and kube-ingress-aws-controller is able to create NLBs instead of ALBs. The Kubernetes Ingress AWS controller supports the zalando.org/aws-load-balancer-type annotation (which defaults to alb ) to determine this. If this annotation is set to nlb then ExternalDNS will create an NLB instead of an ALB. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : zalando.org/aws-load-balancer-type : nlb kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an NLB. A successful create, you can observe in the ingress status field, that is written by kube-ingress-aws-controller : status : loadBalancer : ingress : - hostname : kube-ing-lb-atedkrlml7iu-1681027139.$region.elb.amazonaws.com ExternalDNS will create a A-records echoserver.example.org , that use AWS ALIAS record to automatically maintain IP addresses of the NLB. RouteGroup (optional) \u00b6 Kube-ingress-aws-controller , Skipper and external-dns support RouteGroups . External-dns needs to be started with --source=skipper-routegroup parameter in order to work on RouteGroup objects. Here we can not show all RouteGroup capabilities , but we show one simple example with an application and a custom https redirect. apiVersion : zalando.org/v1 kind : RouteGroup metadata : name : my-route-group spec : backends : - name : my-backend type : service serviceName : my-service servicePort : 80 - name : redirectShunt type : shunt defaultBackends : - backendName : my-service routes : - pathSubtree : / - pathSubtree : / predicates : - Header(\"X-Forwarded-Proto\", \"http\") filters : - redirectTo(302, \"https:\") backends : - redirectShunt","title":"Using ExternalDNS with kube-ingress-aws-controller"},{"location":"tutorials/kube-ingress-aws/#using-externaldns-with-kube-ingress-aws-controller","text":"This tutorial describes how to use ExternalDNS with the kube-ingress-aws-controller .","title":"Using ExternalDNS with kube-ingress-aws-controller"},{"location":"tutorials/kube-ingress-aws/#setting-up-externaldns-and-kube-ingress-aws-controller","text":"Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the source=ingress argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the ingress-class argument, but this is not required. For help setting up the Kubernetes Ingress AWS Controller, that can create ALBs and NLBs, follow the Setup Guide .","title":"Setting up ExternalDNS and kube-ingress-aws-controller"},{"location":"tutorials/kube-ingress-aws/#optional-routegroup","text":"RouteGroup is a CRD, that enables you to do complex routing with Skipper . First, you have to apply the RouteGroup CRD to your cluster: kubectl apply -f https://github.com/zalando/skipper/blob/HEAD/dataclients/kubernetes/deploy/apply/routegroups_crd.yaml You have to grant all controllers: Skipper , kube-ingress-aws-controller and external-dns to read the routegroup resource and kube-ingress-aws-controller to update the status field of a routegroup. This depends on your RBAC policies, in case you use RBAC, you can use this for all 3 controllers: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : kube-ingress-aws-controller rules : - apiGroups : - extensions - networking.k8s.io resources : - ingresses verbs : - get - list - watch - apiGroups : - extensions - networking.k8s.io resources : - ingresses/status verbs : - patch - update - apiGroups : - zalando.org resources : - routegroups verbs : - get - list - watch - apiGroups : - zalando.org resources : - routegroups/status verbs : - patch - update See also current RBAC yaml files: - kube-ingress-aws-controller - skipper - external-dns","title":"Optional RouteGroup"},{"location":"tutorials/kube-ingress-aws/#deploy-an-example-application","text":"Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ingress objects, that were created by kube-ingress-aws-controller . apiVersion : apps/v1 kind : Deployment metadata : name : echoserver spec : replicas : 1 selector : matchLabels : app : echoserver template : metadata : labels : app : echoserver spec : containers : - image : gcr.io/google_containers/echoserver:1.4 imagePullPolicy : Always name : echoserver ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : echoserver spec : ports : - port : 80 targetPort : 8080 protocol : TCP type : ClusterIP selector : app : echoserver Note that the Service object is of type ClusterIP , because we will target Skipper and do the HTTP routing in Skipper. We don\u2019t need a Service of type LoadBalancer here, since we will be using a shared skipper-ingress for all Ingress. Skipper use hostNetwork to be able to get traffic from AWS LoadBalancers EC2 network. ALBs or NLBs, will be created based on need and will be shared across all ingress as default.","title":"Deploy an example application"},{"location":"tutorials/kube-ingress-aws/#ingress-examples","text":"Create the following Ingress to expose the echoserver application to the Internet. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.mycluster.example.org http : &echoserver_root paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix - host : echoserver.example.org http : *echoserver_root The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to skipper which will forward to the echoserver application. If the --source=ingress argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records being created, echoserver.mycluster.example.org and echoserver.example.org , which both alias the ALB that is associated with the Ingress object. Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : external-dns.alpha.kubernetes.io/hostname : echoserver.mycluster.example.org, echoserver.example.org kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix In the above example we create a default path that works for any hostname, and make use of the external-dns.alpha.kubernetes.io/hostname annotation to create multiple aliases for the resulting ALB.","title":"Ingress examples"},{"location":"tutorials/kube-ingress-aws/#dualstack-albs","text":"AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs. The Kubernetes Ingress AWS controller supports the alb.ingress.kubernetes.io/ip-address-type annotation (which defaults to ipv4 ) to determine this. If this annotation is set to dualstack then ExternalDNS will create two alias records (one A record and one AAAA record) for each hostname associated with the Ingress object. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : alb.ingress.kubernetes.io/ip-address-type : dualstack kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an ALB with a dualstack interface. ExternalDNS will create both an A echoserver.example.org record and an AAAA record of the same name, that each are aliases for the same ALB.","title":"Dualstack ALBs"},{"location":"tutorials/kube-ingress-aws/#nlbs","text":"AWS has NLBs and kube-ingress-aws-controller is able to create NLBs instead of ALBs. The Kubernetes Ingress AWS controller supports the zalando.org/aws-load-balancer-type annotation (which defaults to alb ) to determine this. If this annotation is set to nlb then ExternalDNS will create an NLB instead of an ALB. Example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : zalando.org/aws-load-balancer-type : nlb kubernetes.io/ingress.class : skipper name : echoserver spec : ingressClassName : skipper rules : - host : echoserver.example.org http : paths : - path : / backend : service : name : echoserver port : number : 80 pathType : Prefix The above Ingress object will result in the creation of an NLB. A successful create, you can observe in the ingress status field, that is written by kube-ingress-aws-controller : status : loadBalancer : ingress : - hostname : kube-ing-lb-atedkrlml7iu-1681027139.$region.elb.amazonaws.com ExternalDNS will create a A-records echoserver.example.org , that use AWS ALIAS record to automatically maintain IP addresses of the NLB.","title":"NLBs"},{"location":"tutorials/kube-ingress-aws/#routegroup-optional","text":"Kube-ingress-aws-controller , Skipper and external-dns support RouteGroups . External-dns needs to be started with --source=skipper-routegroup parameter in order to work on RouteGroup objects. Here we can not show all RouteGroup capabilities , but we show one simple example with an application and a custom https redirect. apiVersion : zalando.org/v1 kind : RouteGroup metadata : name : my-route-group spec : backends : - name : my-backend type : service serviceName : my-service servicePort : 80 - name : redirectShunt type : shunt defaultBackends : - backendName : my-service routes : - pathSubtree : / - pathSubtree : / predicates : - Header(\"X-Forwarded-Proto\", \"http\") filters : - redirectTo(302, \"https:\") backends : - redirectShunt","title":"RouteGroup (optional)"},{"location":"tutorials/linode/","text":"Setting up ExternalDNS for Services on Linode \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Linode DNS Manager. Make sure to use >=0.5.5 version of ExternalDNS for this tutorial. Managing DNS with Linode \u00b6 If you want to learn about how to use Linode DNS Manager read the following tutorials: An Introduction to Managing DNS , and general documentation Creating Linode Credentials \u00b6 Generate a new oauth token by following the instructions at Access-and-Authentication The environment variable LINODE_TOKEN will be needed to run ExternalDNS with Linode. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=linode env : - name : LINODE_TOKEN value : \"YOUR_LINODE_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=linode env : - name : LINODE_TOKEN value : \"YOUR_LINODE_API_KEY\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Linode DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Linode DNS records. Verifying Linode DNS records \u00b6 Check your Linode UI to view the records for your Linode DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Linode DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on Linode"},{"location":"tutorials/linode/#setting-up-externaldns-for-services-on-linode","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Linode DNS Manager. Make sure to use >=0.5.5 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Linode"},{"location":"tutorials/linode/#managing-dns-with-linode","text":"If you want to learn about how to use Linode DNS Manager read the following tutorials: An Introduction to Managing DNS , and general documentation","title":"Managing DNS with Linode"},{"location":"tutorials/linode/#creating-linode-credentials","text":"Generate a new oauth token by following the instructions at Access-and-Authentication The environment variable LINODE_TOKEN will be needed to run ExternalDNS with Linode.","title":"Creating Linode Credentials"},{"location":"tutorials/linode/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/linode/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=linode env : - name : LINODE_TOKEN value : \"YOUR_LINODE_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/linode/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=linode env : - name : LINODE_TOKEN value : \"YOUR_LINODE_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/linode/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Linode DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Linode DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/linode/#verifying-linode-dns-records","text":"Check your Linode UI to view the records for your Linode DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Linode DNS records"},{"location":"tutorials/linode/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Linode DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/nginx-ingress/","text":"Setting up ExternalDNS on GKE with nginx-ingress-controller \u00b6 This tutorial describes how to setup ExternalDNS for usage within a GKE cluster that doesn\u2019t make use of Google\u2019s default ingress controller but rather uses nginx-ingress-controller for that task. Set up your environment \u00b6 Setup your environment to work with Google Cloud Platform. Fill in your values as needed, e.g. target project. $ gcloud config set project \"zalando-external-dns-test\" $ gcloud config set compute/region \"europe-west1\" $ gcloud config set compute/zone \"europe-west1-d\" GKE Node Scopes \u00b6 The following instructions use instance scopes to provide ExternalDNS with the permissions it needs to manage DNS records. Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments. Create a GKE cluster without using the default ingress controller. $ gcloud container clusters create \"external-dns\" \\ --num-nodes 1 \\ --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" Create a DNS zone which will contain the managed DNS records. $ gcloud dns managed-zones create \"external-dns-test-gcp-zalan-do\" \\ --dns-name \"external-dns-test.gcp.zalan.do.\" \\ --description \"Automatically managed zone by ExternalDNS\" Make a note of the nameservers that were assigned to your new zone. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"external-dns-test.gcp.zalan.do.\" \\ --type NS NAME TYPE TTL DATA external-dns-test.gcp.zalan.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc. Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following. $ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\" Connect your kubectl client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes. $ gcloud container clusters get-credentials \"external-dns\" $ kubectl create clusterrolebinding cluster-admin-me \\ --clusterrole = cluster-admin --user = \" $( gcloud config get-value account ) \" Deploy the nginx ingress controller \u00b6 First, you need to deploy the nginx-based ingress controller. It can be deployed in at least two modes: Leveraging a Layer 4 load balancer in front of the nginx proxies or directly targeting pods with hostPorts on your worker nodes. ExternalDNS doesn\u2019t really care and supports both modes. Default Backend \u00b6 The nginx controller uses a default backend that it serves when no Ingress rule matches. This is a separate Service that can be picked by you. We\u2019ll use the default backend that\u2019s used by other ingress controllers for that matter. Apply the following manifests to your cluster to deploy the default backend. apiVersion : v1 kind : Service metadata : name : default-http-backend spec : ports : - port : 80 targetPort : 8080 selector : app : default-http-backend --- apiVersion : apps/v1 kind : Deployment metadata : name : default-http-backend spec : selector : matchLabels : app : default-http-backend template : metadata : labels : app : default-http-backend spec : containers : - name : default-http-backend image : gcr.io/google_containers/defaultbackend:1.3 Without a separate TCP load balancer \u00b6 By default, the controller will update your Ingress objects with the public IPs of the nodes running your nginx controller instances. You should run multiple instances in case of pod or node failure. The controller will do leader election and will put multiple IPs as targets in your Ingress objects in that case. It could also make sense to run it as a DaemonSet. However, we\u2019ll just run a single replica. You have to open the respective ports on all of your worker nodes to allow nginx to receive traffic. $ gcloud compute firewall-rules create \"allow-http\" --allow tcp:80 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\" $ gcloud compute firewall-rules create \"allow-https\" --allow tcp:443 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\" Change --target-tags to the corresponding tags of your nodes. You can find them by describing your instances or by looking at the default firewall rules created by GKE for your cluster. Apply the following manifests to your cluster to deploy the nginx-based ingress controller. Note, how it receives a reference to the default backend\u2019s Service and that it listens on hostPorts. (You may have to use hostNetwork: true as well.) apiVersion : apps/v1 kind : Deployment metadata : name : nginx-ingress-controller spec : selector : matchLabels : app : nginx-ingress-controller template : metadata : labels : app : nginx-ingress-controller spec : containers : - name : nginx-ingress-controller image : gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 args : - /nginx-ingress-controller - --default-backend-service=default/default-http-backend env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace ports : - containerPort : 80 hostPort : 80 - containerPort : 443 hostPort : 443 With a separate TCP load balancer \u00b6 However, you can also have the ingress controller proxied by a Kubernetes Service. This will instruct the controller to populate this Service\u2019s external IP as the external IP of the Ingress. This exposes the nginx proxies via a Layer 4 load balancer ( type=LoadBalancer ) which is more reliable than the other method. With that approach, you can run as many nginx proxy instances on your cluster as you like or have them autoscaled. This is the preferred way of running the nginx controller. Apply the following manifests to your cluster. Note, how the controller is receiving an additional flag telling it which Service it should treat as its public endpoint and how it doesn\u2019t need hostPorts anymore. Apply the following manifests to run the controller in this mode. apiVersion : v1 kind : Service metadata : name : nginx-ingress-controller spec : type : LoadBalancer ports : - name : http port : 80 targetPort : 80 - name : https port : 443 targetPort : 443 selector : app : nginx-ingress-controller --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-ingress-controller spec : selector : matchLabels : app : nginx-ingress-controller template : metadata : labels : app : nginx-ingress-controller spec : containers : - name : nginx-ingress-controller image : gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 args : - /nginx-ingress-controller - --default-backend-service=default/default-http-backend - --publish-service=default/nginx-ingress-controller env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace ports : - containerPort : 80 - containerPort : 443 Deploy ExternalDNS \u00b6 Apply the following manifest file to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --domain-filter=external-dns-test.gcp.zalan.do - --provider=google - --google-project=zalando-external-dns-test - --registry=txt - --txt-owner-id=my-identifier Use --dry-run if you want to be extra careful on the first run. Note, that you will not see any records created when you are running in dry-run mode. You can, however, inspect the logs and watch what would have been done. Deploy a sample application \u00b6 Create the following sample application to test that ExternalDNS works. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : ingressClassName : nginx rules : - host : via-ingress.external-dns-test.gcp.zalan.do http : paths : - path : / backend : service : name : nginx port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 After roughly two minutes check that a corresponding DNS record for your Ingress was created. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\ --type A NAME TYPE TTL DATA via-ingress.external-dns-test.gcp.zalan.do. A 300 35.187.1.246 Let\u2019s check that we can resolve this DNS name as well. dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do. 35.187.1.246 Try with curl as well. $ curl via-ingress.external-dns-test.gcp.zalan.do <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html> Clean up \u00b6 Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly. $ kubectl delete service nginx-ingress-controller $ kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. $ gcloud dns managed-zones delete \"external-dns-test-gcp-zalan-do\" $ gcloud container clusters delete \"external-dns\" Also delete the NS records for your removed zone from the parent zone. $ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction remove ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\" GKE with Workload Identity \u00b6 The following instructions use GKE workload identity to provide ExternalDNS with the permissions it needs to manage DNS records. Workload identity is the Google-recommended way to provide GKE workloads access to GCP APIs. Create a GKE cluster with workload identity enabled and without the HttpLoadBalancing add-on. $ gcloud container clusters create external-dns \\ --workload-metadata-from-node = GKE_METADATA_SERVER \\ --identity-namespace = zalando-external-dns-test.svc.id.goog \\ --addons = HorizontalPodAutoscaling Create a GCP service account (GSA) for ExternalDNS and save its email address. $ sa_name = \"Kubernetes external-dns\" $ gcloud iam service-accounts create sa-edns --display-name = \" $sa_name \" $ sa_email = $( gcloud iam service-accounts list --format = 'value(email)' \\ --filter = \"displayName: $sa_name \" ) Bind the ExternalDNS GSA to the DNS admin role. $ gcloud projects add-iam-policy-binding zalando-external-dns-test \\ --member = \"serviceAccount: $sa_email \" --role = roles/dns.admin Link the ExternalDNS GSA to the Kubernetes service account (KSA) that external-dns will run under, i.e., the external-dns KSA in the external-dns namespaces. $ gcloud iam service-accounts add-iam-policy-binding \" $sa_email \" \\ --member = \"serviceAccount:zalando-external-dns-test.svc.id.goog[external-dns/external-dns]\" \\ --role = roles/iam.workloadIdentityUser Create a DNS zone which will contain the managed DNS records. $ gcloud dns managed-zones create external-dns-test-gcp-zalan-do \\ --dns-name = external-dns-test.gcp.zalan.do. \\ --description = \"Automatically managed zone by ExternalDNS\" Make a note of the nameservers that were assigned to your new zone. $ gcloud dns record-sets list \\ --zone = external-dns-test-gcp-zalan-do \\ --name = external-dns-test.gcp.zalan.do. \\ --type NS NAME TYPE TTL DATA external-dns-test.gcp.zalan.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc. Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following. $ gcloud dns record-sets transaction start --zone = gcp-zalan-do $ gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name = external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone = gcp-zalan-do $ gcloud dns record-sets transaction execute --zone = gcp-zalan-do Connect your kubectl client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes. $ gcloud container clusters get-credentials external-dns $ kubectl create clusterrolebinding cluster-admin-me \\ --clusterrole = cluster-admin --user = \" $( gcloud config get-value account ) \" Deploy ingress-nginx \u00b6 Follow the ingress-nginx GKE installation instructions to deploy it to the cluster. $ kubectl apply -f \\ https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/cloud/deploy.yaml Deploy ExternalDNS \u00b6 Apply the following manifest file to deploy external-dns. apiVersion : v1 kind : Namespace metadata : name : external-dns --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : external-dns --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - args : - --source=ingress - --domain-filter=external-dns-test.gcp.zalan.do - --provider=google - --google-project=zalando-external-dns-test - --registry=txt - --txt-owner-id=my-identifier image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns securityContext : fsGroup : 65534 runAsUser : 65534 serviceAccountName : external-dns Then add the proper workload identity annotation to the cert-manager service account. $ kubectl annotate serviceaccount --namespace = external-dns external-dns \\ \"iam.gke.io/gcp-service-account= $sa_email \" Deploy a sample application \u00b6 Create the following sample application to test that ExternalDNS works. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : ingressClassName : nginx rules : - host : via-ingress.external-dns-test.gcp.zalan.do http : paths : - path : / backend : service : name : nginx port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 After roughly two minutes check that a corresponding DNS record for your ingress was created. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\ --type A NAME TYPE TTL DATA via-ingress.external-dns-test.gcp.zalan.do. A 300 35.187.1.246 Let\u2019s check that we can resolve this DNS name as well. $ dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do. 35.187.1.246 Try with curl as well. $ curl via-ingress.external-dns-test.gcp.zalan.do <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html> Clean up \u00b6 Make sure to delete all service and ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly. $ kubectl delete service --namespace = ingress-nginx ingress-nginx-controller $ kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. $ gcloud dns managed-zones delete external-dns-test-gcp-zalan-do $ gcloud container clusters delete external-dns Also delete the NS records for your removed zone from the parent zone. $ gcloud dns record-sets transaction start --zone gcp-zalan-do $ gcloud dns record-sets transaction remove ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name = external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone = gcp-zalan-do $ gcloud dns record-sets transaction execute --zone = gcp-zalan-do User Demo How-To Blogs and Examples \u00b6 Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns","title":"Setting up ExternalDNS on GKE with nginx-ingress-controller"},{"location":"tutorials/nginx-ingress/#setting-up-externaldns-on-gke-with-nginx-ingress-controller","text":"This tutorial describes how to setup ExternalDNS for usage within a GKE cluster that doesn\u2019t make use of Google\u2019s default ingress controller but rather uses nginx-ingress-controller for that task.","title":"Setting up ExternalDNS on GKE with nginx-ingress-controller"},{"location":"tutorials/nginx-ingress/#set-up-your-environment","text":"Setup your environment to work with Google Cloud Platform. Fill in your values as needed, e.g. target project. $ gcloud config set project \"zalando-external-dns-test\" $ gcloud config set compute/region \"europe-west1\" $ gcloud config set compute/zone \"europe-west1-d\"","title":"Set up your environment"},{"location":"tutorials/nginx-ingress/#gke-node-scopes","text":"The following instructions use instance scopes to provide ExternalDNS with the permissions it needs to manage DNS records. Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments. Create a GKE cluster without using the default ingress controller. $ gcloud container clusters create \"external-dns\" \\ --num-nodes 1 \\ --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" Create a DNS zone which will contain the managed DNS records. $ gcloud dns managed-zones create \"external-dns-test-gcp-zalan-do\" \\ --dns-name \"external-dns-test.gcp.zalan.do.\" \\ --description \"Automatically managed zone by ExternalDNS\" Make a note of the nameservers that were assigned to your new zone. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"external-dns-test.gcp.zalan.do.\" \\ --type NS NAME TYPE TTL DATA external-dns-test.gcp.zalan.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc. Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following. $ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\" Connect your kubectl client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes. $ gcloud container clusters get-credentials \"external-dns\" $ kubectl create clusterrolebinding cluster-admin-me \\ --clusterrole = cluster-admin --user = \" $( gcloud config get-value account ) \"","title":"GKE Node Scopes"},{"location":"tutorials/nginx-ingress/#deploy-the-nginx-ingress-controller","text":"First, you need to deploy the nginx-based ingress controller. It can be deployed in at least two modes: Leveraging a Layer 4 load balancer in front of the nginx proxies or directly targeting pods with hostPorts on your worker nodes. ExternalDNS doesn\u2019t really care and supports both modes.","title":"Deploy the nginx ingress controller"},{"location":"tutorials/nginx-ingress/#default-backend","text":"The nginx controller uses a default backend that it serves when no Ingress rule matches. This is a separate Service that can be picked by you. We\u2019ll use the default backend that\u2019s used by other ingress controllers for that matter. Apply the following manifests to your cluster to deploy the default backend. apiVersion : v1 kind : Service metadata : name : default-http-backend spec : ports : - port : 80 targetPort : 8080 selector : app : default-http-backend --- apiVersion : apps/v1 kind : Deployment metadata : name : default-http-backend spec : selector : matchLabels : app : default-http-backend template : metadata : labels : app : default-http-backend spec : containers : - name : default-http-backend image : gcr.io/google_containers/defaultbackend:1.3","title":"Default Backend"},{"location":"tutorials/nginx-ingress/#without-a-separate-tcp-load-balancer","text":"By default, the controller will update your Ingress objects with the public IPs of the nodes running your nginx controller instances. You should run multiple instances in case of pod or node failure. The controller will do leader election and will put multiple IPs as targets in your Ingress objects in that case. It could also make sense to run it as a DaemonSet. However, we\u2019ll just run a single replica. You have to open the respective ports on all of your worker nodes to allow nginx to receive traffic. $ gcloud compute firewall-rules create \"allow-http\" --allow tcp:80 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\" $ gcloud compute firewall-rules create \"allow-https\" --allow tcp:443 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\" Change --target-tags to the corresponding tags of your nodes. You can find them by describing your instances or by looking at the default firewall rules created by GKE for your cluster. Apply the following manifests to your cluster to deploy the nginx-based ingress controller. Note, how it receives a reference to the default backend\u2019s Service and that it listens on hostPorts. (You may have to use hostNetwork: true as well.) apiVersion : apps/v1 kind : Deployment metadata : name : nginx-ingress-controller spec : selector : matchLabels : app : nginx-ingress-controller template : metadata : labels : app : nginx-ingress-controller spec : containers : - name : nginx-ingress-controller image : gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 args : - /nginx-ingress-controller - --default-backend-service=default/default-http-backend env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace ports : - containerPort : 80 hostPort : 80 - containerPort : 443 hostPort : 443","title":"Without a separate TCP load balancer"},{"location":"tutorials/nginx-ingress/#with-a-separate-tcp-load-balancer","text":"However, you can also have the ingress controller proxied by a Kubernetes Service. This will instruct the controller to populate this Service\u2019s external IP as the external IP of the Ingress. This exposes the nginx proxies via a Layer 4 load balancer ( type=LoadBalancer ) which is more reliable than the other method. With that approach, you can run as many nginx proxy instances on your cluster as you like or have them autoscaled. This is the preferred way of running the nginx controller. Apply the following manifests to your cluster. Note, how the controller is receiving an additional flag telling it which Service it should treat as its public endpoint and how it doesn\u2019t need hostPorts anymore. Apply the following manifests to run the controller in this mode. apiVersion : v1 kind : Service metadata : name : nginx-ingress-controller spec : type : LoadBalancer ports : - name : http port : 80 targetPort : 80 - name : https port : 443 targetPort : 443 selector : app : nginx-ingress-controller --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx-ingress-controller spec : selector : matchLabels : app : nginx-ingress-controller template : metadata : labels : app : nginx-ingress-controller spec : containers : - name : nginx-ingress-controller image : gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 args : - /nginx-ingress-controller - --default-backend-service=default/default-http-backend - --publish-service=default/nginx-ingress-controller env : - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace ports : - containerPort : 80 - containerPort : 443","title":"With a separate TCP load balancer"},{"location":"tutorials/nginx-ingress/#deploy-externaldns","text":"Apply the following manifest file to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --domain-filter=external-dns-test.gcp.zalan.do - --provider=google - --google-project=zalando-external-dns-test - --registry=txt - --txt-owner-id=my-identifier Use --dry-run if you want to be extra careful on the first run. Note, that you will not see any records created when you are running in dry-run mode. You can, however, inspect the logs and watch what would have been done.","title":"Deploy ExternalDNS"},{"location":"tutorials/nginx-ingress/#deploy-a-sample-application","text":"Create the following sample application to test that ExternalDNS works. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : ingressClassName : nginx rules : - host : via-ingress.external-dns-test.gcp.zalan.do http : paths : - path : / backend : service : name : nginx port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 After roughly two minutes check that a corresponding DNS record for your Ingress was created. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\ --type A NAME TYPE TTL DATA via-ingress.external-dns-test.gcp.zalan.do. A 300 35.187.1.246 Let\u2019s check that we can resolve this DNS name as well. dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do. 35.187.1.246 Try with curl as well. $ curl via-ingress.external-dns-test.gcp.zalan.do <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html>","title":"Deploy a sample application"},{"location":"tutorials/nginx-ingress/#clean-up","text":"Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly. $ kubectl delete service nginx-ingress-controller $ kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. $ gcloud dns managed-zones delete \"external-dns-test-gcp-zalan-do\" $ gcloud container clusters delete \"external-dns\" Also delete the NS records for your removed zone from the parent zone. $ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction remove ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\" $ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\"","title":"Clean up"},{"location":"tutorials/nginx-ingress/#gke-with-workload-identity","text":"The following instructions use GKE workload identity to provide ExternalDNS with the permissions it needs to manage DNS records. Workload identity is the Google-recommended way to provide GKE workloads access to GCP APIs. Create a GKE cluster with workload identity enabled and without the HttpLoadBalancing add-on. $ gcloud container clusters create external-dns \\ --workload-metadata-from-node = GKE_METADATA_SERVER \\ --identity-namespace = zalando-external-dns-test.svc.id.goog \\ --addons = HorizontalPodAutoscaling Create a GCP service account (GSA) for ExternalDNS and save its email address. $ sa_name = \"Kubernetes external-dns\" $ gcloud iam service-accounts create sa-edns --display-name = \" $sa_name \" $ sa_email = $( gcloud iam service-accounts list --format = 'value(email)' \\ --filter = \"displayName: $sa_name \" ) Bind the ExternalDNS GSA to the DNS admin role. $ gcloud projects add-iam-policy-binding zalando-external-dns-test \\ --member = \"serviceAccount: $sa_email \" --role = roles/dns.admin Link the ExternalDNS GSA to the Kubernetes service account (KSA) that external-dns will run under, i.e., the external-dns KSA in the external-dns namespaces. $ gcloud iam service-accounts add-iam-policy-binding \" $sa_email \" \\ --member = \"serviceAccount:zalando-external-dns-test.svc.id.goog[external-dns/external-dns]\" \\ --role = roles/iam.workloadIdentityUser Create a DNS zone which will contain the managed DNS records. $ gcloud dns managed-zones create external-dns-test-gcp-zalan-do \\ --dns-name = external-dns-test.gcp.zalan.do. \\ --description = \"Automatically managed zone by ExternalDNS\" Make a note of the nameservers that were assigned to your new zone. $ gcloud dns record-sets list \\ --zone = external-dns-test-gcp-zalan-do \\ --name = external-dns-test.gcp.zalan.do. \\ --type NS NAME TYPE TTL DATA external-dns-test.gcp.zalan.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case it\u2019s ns-cloud-{e1-e4}.googledomains.com. but your\u2019s could slightly differ, e.g. {a1-a4} , {b1-b4} etc. Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following. $ gcloud dns record-sets transaction start --zone = gcp-zalan-do $ gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name = external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone = gcp-zalan-do $ gcloud dns record-sets transaction execute --zone = gcp-zalan-do Connect your kubectl client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes. $ gcloud container clusters get-credentials external-dns $ kubectl create clusterrolebinding cluster-admin-me \\ --clusterrole = cluster-admin --user = \" $( gcloud config get-value account ) \"","title":"GKE with Workload Identity"},{"location":"tutorials/nginx-ingress/#deploy-ingress-nginx","text":"Follow the ingress-nginx GKE installation instructions to deploy it to the cluster. $ kubectl apply -f \\ https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/cloud/deploy.yaml","title":"Deploy ingress-nginx"},{"location":"tutorials/nginx-ingress/#deploy-externaldns_1","text":"Apply the following manifest file to deploy external-dns. apiVersion : v1 kind : Namespace metadata : name : external-dns --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : external-dns --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - args : - --source=ingress - --domain-filter=external-dns-test.gcp.zalan.do - --provider=google - --google-project=zalando-external-dns-test - --registry=txt - --txt-owner-id=my-identifier image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns securityContext : fsGroup : 65534 runAsUser : 65534 serviceAccountName : external-dns Then add the proper workload identity annotation to the cert-manager service account. $ kubectl annotate serviceaccount --namespace = external-dns external-dns \\ \"iam.gke.io/gcp-service-account= $sa_email \"","title":"Deploy ExternalDNS"},{"location":"tutorials/nginx-ingress/#deploy-a-sample-application_1","text":"Create the following sample application to test that ExternalDNS works. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : nginx annotations : kubernetes.io/ingress.class : nginx spec : ingressClassName : nginx rules : - host : via-ingress.external-dns-test.gcp.zalan.do http : paths : - path : / backend : service : name : nginx port : number : 80 pathType : Prefix --- apiVersion : v1 kind : Service metadata : name : nginx spec : ports : - port : 80 targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 After roughly two minutes check that a corresponding DNS record for your ingress was created. $ gcloud dns record-sets list \\ --zone \"external-dns-test-gcp-zalan-do\" \\ --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\ --type A NAME TYPE TTL DATA via-ingress.external-dns-test.gcp.zalan.do. A 300 35.187.1.246 Let\u2019s check that we can resolve this DNS name as well. $ dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do. 35.187.1.246 Try with curl as well. $ curl via-ingress.external-dns-test.gcp.zalan.do <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> ... </head> <body> ... </body> </html>","title":"Deploy a sample application"},{"location":"tutorials/nginx-ingress/#clean-up_1","text":"Make sure to delete all service and ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly. $ kubectl delete service --namespace = ingress-nginx ingress-nginx-controller $ kubectl delete ingress nginx Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster. $ gcloud dns managed-zones delete external-dns-test-gcp-zalan-do $ gcloud container clusters delete external-dns Also delete the NS records for your removed zone from the parent zone. $ gcloud dns record-sets transaction start --zone gcp-zalan-do $ gcloud dns record-sets transaction remove ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name = external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone = gcp-zalan-do $ gcloud dns record-sets transaction execute --zone = gcp-zalan-do","title":"Clean up"},{"location":"tutorials/nginx-ingress/#user-demo-how-to-blogs-and-examples","text":"Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager & external-dns","title":"User Demo How-To Blogs and Examples"},{"location":"tutorials/nodes/","text":"Configuring ExternalDNS to use Cluster Nodes as Source \u00b6 This tutorial describes how to configure ExternalDNS to use the cluster nodes as source. Using nodes ( --source=node ) as source is possible to synchronize a DNS zone with the nodes of a cluster. The node source adds an A record per each node externalIP (if not found, node\u2019s internalIP is used). The TTL record can be set with the external-dns.alpha.kubernetes.io/ttl node annotation. Manifest (for cluster without RBAC enabled) \u00b6 --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --source=node # will use nodes as source - --provider=aws - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --domain-filter=external-dns-test.my-org.com - --aws-zone-type=public - --registry=txt - --fqdn-template={{.Name}}.external-dns-test.my-org.com - --txt-owner-id=my-identifier - --policy=sync - --log-level=debug Manifest (for cluster with RBAC enabled) \u00b6 apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"route.openshift.io\"] resources: [\"routes\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --source=node # will use nodes as source - --provider=aws - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --domain-filter=external-dns-test.my-org.com - --aws-zone-type=public - --registry=txt - --fqdn-template={{.Name}}.external-dns-test.my-org.com - --txt-owner-id=my-identifier - --policy=sync - --log-level=debug","title":"Configuring ExternalDNS to use Cluster Nodes as Source"},{"location":"tutorials/nodes/#configuring-externaldns-to-use-cluster-nodes-as-source","text":"This tutorial describes how to configure ExternalDNS to use the cluster nodes as source. Using nodes ( --source=node ) as source is possible to synchronize a DNS zone with the nodes of a cluster. The node source adds an A record per each node externalIP (if not found, node\u2019s internalIP is used). The TTL record can be set with the external-dns.alpha.kubernetes.io/ttl node annotation.","title":"Configuring ExternalDNS to use Cluster Nodes as Source"},{"location":"tutorials/nodes/#manifest-for-cluster-without-rbac-enabled","text":"--- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --source=node # will use nodes as source - --provider=aws - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --domain-filter=external-dns-test.my-org.com - --aws-zone-type=public - --registry=txt - --fqdn-template={{.Name}}.external-dns-test.my-org.com - --txt-owner-id=my-identifier - --policy=sync - --log-level=debug","title":"Manifest (for cluster without RBAC enabled)"},{"location":"tutorials/nodes/#manifest-for-cluster-with-rbac-enabled","text":"apiVersion: v1 kind: ServiceAccount metadata: name: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"route.openshift.io\"] resources: [\"routes\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"services\",\"endpoints\",\"pods\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"extensions\",\"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\",\"watch\",\"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns spec: strategy: type: Recreate selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --source=node # will use nodes as source - --provider=aws - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --domain-filter=external-dns-test.my-org.com - --aws-zone-type=public - --registry=txt - --fqdn-template={{.Name}}.external-dns-test.my-org.com - --txt-owner-id=my-identifier - --policy=sync - --log-level=debug","title":"Manifest (for cluster with RBAC enabled)"},{"location":"tutorials/ns-record/","text":"Creating NS record with CRD source \u00b6 You can create NS records with the help of CRD source and DNSEndpoint CRD. Consider the following example apiVersion : externaldns.k8s.io/v1alpha1 kind : DNSEndpoint metadata : name : ns-record spec : endpoints : - dnsName : zone.example.com recordTTL : 300 recordType : NS targets : - ns1.example.com - ns2.example.com After instantiation of this Custom Resource external-dns will create NS record with the help of configured provider, e.g. aws","title":"Creating NS record with CRD source"},{"location":"tutorials/ns-record/#creating-ns-record-with-crd-source","text":"You can create NS records with the help of CRD source and DNSEndpoint CRD. Consider the following example apiVersion : externaldns.k8s.io/v1alpha1 kind : DNSEndpoint metadata : name : ns-record spec : endpoints : - dnsName : zone.example.com recordTTL : 300 recordType : NS targets : - ns1.example.com - ns2.example.com After instantiation of this Custom Resource external-dns will create NS record with the help of configured provider, e.g. aws","title":"Creating NS record with CRD source"},{"location":"tutorials/ns1/","text":"Setting up ExternalDNS for Services on NS1 \u00b6 This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using NS1 DNS. Make sure to use >=0.5 version of ExternalDNS for this tutorial. Creating a zone with NS1 DNS \u00b6 If you are new to NS1, we recommend you first read the following instructions for creating a zone. Creating a zone using the NS1 portal Creating a zone using the NS1 API Creating NS1 Credentials \u00b6 All NS1 products are API-first, meaning everything that can be done on the portal\u2014including managing zones and records, data sources and feeds, and account settings and users\u2014can be done via API. The NS1 API is a standard REST API with JSON responses. The environment var NS1_APIKEY will be needed to run ExternalDNS with NS1. To add or delete an API key \u00b6 Log into the NS1 portal at my.nsone.net . Click your username in the upper-right corner, and navigate to Account Settings > Users & Teams . Navigate to the API Keys tab, and click Add Key . Enter the name of the application and modify permissions and settings as desired. Once complete, click Create Key . The new API key appears in the list. Note: Set the permissions for your API keys just as you would for a user or team associated with your organization\u2019s NS1 account. For more information, refer to the article Creating and Managing API Keys in the NS1 Knowledge Base. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ns1 env : - name : NS1_APIKEY value : \"YOUR_NS1_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ns1 env : - name : NS1_APIKEY value : \"YOUR_NS1_API_KEY\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the NS1 DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service \u00b6 $ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the NS1 DNS records. Verifying NS1 DNS records \u00b6 Use the NS1 portal or API to verify that the A record for your domain shows the external IP address of the services. Cleanup \u00b6 Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Setting up ExternalDNS for Services on NS1"},{"location":"tutorials/ns1/#setting-up-externaldns-for-services-on-ns1","text":"This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using NS1 DNS. Make sure to use >=0.5 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on NS1"},{"location":"tutorials/ns1/#creating-a-zone-with-ns1-dns","text":"If you are new to NS1, we recommend you first read the following instructions for creating a zone. Creating a zone using the NS1 portal Creating a zone using the NS1 API","title":"Creating a zone with NS1 DNS"},{"location":"tutorials/ns1/#creating-ns1-credentials","text":"All NS1 products are API-first, meaning everything that can be done on the portal\u2014including managing zones and records, data sources and feeds, and account settings and users\u2014can be done via API. The NS1 API is a standard REST API with JSON responses. The environment var NS1_APIKEY will be needed to run ExternalDNS with NS1.","title":"Creating NS1 Credentials"},{"location":"tutorials/ns1/#to-add-or-delete-an-api-key","text":"Log into the NS1 portal at my.nsone.net . Click your username in the upper-right corner, and navigate to Account Settings > Users & Teams . Navigate to the API Keys tab, and click Add Key . Enter the name of the application and modify permissions and settings as desired. Once complete, click Create Key . The new API key appears in the list. Note: Set the permissions for your API keys just as you would for a user or team associated with your organization\u2019s NS1 account. For more information, refer to the article Creating and Managing API Keys in the NS1 Knowledge Base.","title":"To add or delete an API key"},{"location":"tutorials/ns1/#deploy-externaldns","text":"Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:","title":"Deploy ExternalDNS"},{"location":"tutorials/ns1/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ns1 env : - name : NS1_APIKEY value : \"YOUR_NS1_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/ns1/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ns1 env : - name : NS1_APIKEY value : \"YOUR_NS1_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/ns1/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the NS1 DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/ns1/#create-the-deployment-and-service","text":"$ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the NS1 DNS records.","title":"Create the deployment and service"},{"location":"tutorials/ns1/#verifying-ns1-dns-records","text":"Use the NS1 portal or API to verify that the A record for your domain shows the external IP address of the services.","title":"Verifying NS1 DNS records"},{"location":"tutorials/ns1/#cleanup","text":"Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/openshift/","text":"Configuring ExternalDNS to use the OpenShift Route Source \u00b6 This tutorial describes how to configure ExternalDNS to use the OpenShift Route source. It is meant to supplement the other provider-specific setup tutorials. For OCP 4.x \u00b6 In OCP 4.x, if you have multiple OpenShift ingress controllers then you must specify an ingress controller name (also called router name), you can get it from the route\u2019s status.ingress[*].routerName field. If you don\u2019t specify a router name when you have multiple ingress controllers in your cluster then the first router from the route\u2019s status.ingress will be used. Note that the router must have admitted the route in order to be selected. Once the router is known, ExternalDNS will use this router\u2019s canonical hostname as the target for the CNAME record. Starting from OCP 4.10 you can use ExternalDNS Operator to manage ExternalDNS instances. Example of its custom resource for AWS provider: apiVersion : externaldns.olm.openshift.io/v1alpha1 kind : ExternalDNS metadata : name : sample spec : provider : type : AWS source : openshiftRouteOptions : routerName : default type : OpenShiftRoute zones : - Z05387772BD5723IZFRX3 This will create an ExternalDNS POD with the following container args in external-dns namespace: spec: containers: - args: - --metrics-address=127.0.0.1:7979 - --txt-owner-id=external-dns-sample - --provider=aws - --source=openshift-route - --policy=sync - --registry=txt - --log-level=debug - --zone-id-filter=Z05387772BD5723IZFRX3 - --openshift-router-name=default - --txt-prefix=external-dns- For OCP 3.11 environment \u00b6 Prepare ROUTER_CANONICAL_HOSTNAME in default/router deployment \u00b6 Read and go through Finding the Host Name of the Router . If no ROUTER_CANONICAL_HOSTNAME is set, you must annotate each route with external-dns.alpha.kubernetes.io/target! Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=openshift-route - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"route.openshift.io\" ] resources : [ \"routes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=openshift-route - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier Verify External DNS works (OpenShift Route example) \u00b6 The following instructions are based on the Hello Openshift . Install a sample service and expose it \u00b6 $ oc apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: app: hello-openshift name: hello-openshift spec: replicas: 1 selector: matchLabels: app: hello-openshift template: metadata: labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift name: hello-openshift --- apiVersion: v1 kind: Service metadata: labels: app: hello-openshift name: hello-openshift spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: hello-openshift sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-openshift spec: host: hello-openshift.example.com to: kind: Service name: hello-openshift weight: 100 wildcardPolicy: None EOF Access the sample route using curl \u00b6 $ curl -i http://hello-openshift.example.com HTTP/1.1 200 OK Date: Fri, 10 Apr 2020 09 :36:41 GMT Content-Length: 17 Content-Type: text/plain ; charset = utf-8 Hello OpenShift!","title":"Configuring ExternalDNS to use the OpenShift Route Source"},{"location":"tutorials/openshift/#configuring-externaldns-to-use-the-openshift-route-source","text":"This tutorial describes how to configure ExternalDNS to use the OpenShift Route source. It is meant to supplement the other provider-specific setup tutorials.","title":"Configuring ExternalDNS to use the OpenShift Route Source"},{"location":"tutorials/openshift/#for-ocp-4x","text":"In OCP 4.x, if you have multiple OpenShift ingress controllers then you must specify an ingress controller name (also called router name), you can get it from the route\u2019s status.ingress[*].routerName field. If you don\u2019t specify a router name when you have multiple ingress controllers in your cluster then the first router from the route\u2019s status.ingress will be used. Note that the router must have admitted the route in order to be selected. Once the router is known, ExternalDNS will use this router\u2019s canonical hostname as the target for the CNAME record. Starting from OCP 4.10 you can use ExternalDNS Operator to manage ExternalDNS instances. Example of its custom resource for AWS provider: apiVersion : externaldns.olm.openshift.io/v1alpha1 kind : ExternalDNS metadata : name : sample spec : provider : type : AWS source : openshiftRouteOptions : routerName : default type : OpenShiftRoute zones : - Z05387772BD5723IZFRX3 This will create an ExternalDNS POD with the following container args in external-dns namespace: spec: containers: - args: - --metrics-address=127.0.0.1:7979 - --txt-owner-id=external-dns-sample - --provider=aws - --source=openshift-route - --policy=sync - --registry=txt - --log-level=debug - --zone-id-filter=Z05387772BD5723IZFRX3 - --openshift-router-name=default - --txt-prefix=external-dns-","title":"For OCP 4.x"},{"location":"tutorials/openshift/#for-ocp-311-environment","text":"","title":"For OCP 3.11 environment"},{"location":"tutorials/openshift/#prepare-router_canonical_hostname-in-defaultrouter-deployment","text":"Read and go through Finding the Host Name of the Router . If no ROUTER_CANONICAL_HOSTNAME is set, you must annotate each route with external-dns.alpha.kubernetes.io/target!","title":"Prepare ROUTER_CANONICAL_HOSTNAME in default/router deployment"},{"location":"tutorials/openshift/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=openshift-route - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/openshift/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"route.openshift.io\" ] resources : [ \"routes\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=openshift-route - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/openshift/#verify-external-dns-works-openshift-route-example","text":"The following instructions are based on the Hello Openshift .","title":"Verify External DNS works (OpenShift Route example)"},{"location":"tutorials/openshift/#install-a-sample-service-and-expose-it","text":"$ oc apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: app: hello-openshift name: hello-openshift spec: replicas: 1 selector: matchLabels: app: hello-openshift template: metadata: labels: app: hello-openshift spec: containers: - image: openshift/hello-openshift name: hello-openshift --- apiVersion: v1 kind: Service metadata: labels: app: hello-openshift name: hello-openshift spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: hello-openshift sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-openshift spec: host: hello-openshift.example.com to: kind: Service name: hello-openshift weight: 100 wildcardPolicy: None EOF","title":"Install a sample service and expose it"},{"location":"tutorials/openshift/#access-the-sample-route-using-curl","text":"$ curl -i http://hello-openshift.example.com HTTP/1.1 200 OK Date: Fri, 10 Apr 2020 09 :36:41 GMT Content-Length: 17 Content-Type: text/plain ; charset = utf-8 Hello OpenShift!","title":"Access the sample route using curl"},{"location":"tutorials/oracle/","text":"Setting up ExternalDNS for Oracle Cloud Infrastructure (OCI) \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OCI DNS. Make sure to use the latest version of ExternalDNS for this tutorial. Creating an OCI DNS Zone \u00b6 Create a DNS zone which will contain the managed DNS records. Let\u2019s use example.com as an reference here. For more information about OCI DNS see the documentation here . Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. We first need to create a config file containing the information needed to connect with the OCI API. Create a new file (oci.yaml) and modify the contents to match the example below. Be sure to adjust the values to match your own credentials: auth : region : us-phoenix-1 tenancy : ocid1.tenancy.oc1... user : ocid1.user.oc1... key : | -----BEGIN RSA PRIVATE KEY----- -----END RSA PRIVATE KEY----- fingerprint : af:81:71:8e... # Omit if there is not a password for the key passphrase : Tx1jRk... compartment : ocid1.compartment.oc1... Create a secret using the config file above: $ kubectl create secret generic external-dns-config --from-file = oci.yaml Manifest (for clusters with RBAC enabled) \u00b6 Apply the following manifest to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --provider=oci - --policy=upsert-only # prevent ExternalDNS from deleting any records, omit to enable full synchronization - --txt-owner-id=my-identifier volumeMounts : - name : config mountPath : /etc/kubernetes/ volumes : - name : config secret : secretName : external-dns-config Verify ExternalDNS works (Service example) \u00b6 Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http Apply the manifest above and wait roughly two minutes and check that a corresponding DNS record for your service was created. $ kubectl apply -f nginx.yaml","title":"Setting up ExternalDNS for Oracle Cloud Infrastructure (OCI)"},{"location":"tutorials/oracle/#setting-up-externaldns-for-oracle-cloud-infrastructure-oci","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OCI DNS. Make sure to use the latest version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Oracle Cloud Infrastructure (OCI)"},{"location":"tutorials/oracle/#creating-an-oci-dns-zone","text":"Create a DNS zone which will contain the managed DNS records. Let\u2019s use example.com as an reference here. For more information about OCI DNS see the documentation here .","title":"Creating an OCI DNS Zone"},{"location":"tutorials/oracle/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. We first need to create a config file containing the information needed to connect with the OCI API. Create a new file (oci.yaml) and modify the contents to match the example below. Be sure to adjust the values to match your own credentials: auth : region : us-phoenix-1 tenancy : ocid1.tenancy.oc1... user : ocid1.user.oc1... key : | -----BEGIN RSA PRIVATE KEY----- -----END RSA PRIVATE KEY----- fingerprint : af:81:71:8e... # Omit if there is not a password for the key passphrase : Tx1jRk... compartment : ocid1.compartment.oc1... Create a secret using the config file above: $ kubectl create secret generic external-dns-config --from-file = oci.yaml","title":"Deploy ExternalDNS"},{"location":"tutorials/oracle/#manifest-for-clusters-with-rbac-enabled","text":"Apply the following manifest to deploy ExternalDNS. apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --provider=oci - --policy=upsert-only # prevent ExternalDNS from deleting any records, omit to enable full synchronization - --txt-owner-id=my-identifier volumeMounts : - name : config mountPath : /etc/kubernetes/ volumes : - name : config secret : secretName : external-dns-config","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/oracle/#verify-externaldns-works-service-example","text":"Create the following sample application to test that ExternalDNS works. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http Apply the manifest above and wait roughly two minutes and check that a corresponding DNS record for your service was created. $ kubectl apply -f nginx.yaml","title":"Verify ExternalDNS works (Service example)"},{"location":"tutorials/ovh/","text":"Setting up ExternalDNS for Services on OVH \u00b6 This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using OVH DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial. Creating a zone with OVH DNS \u00b6 If you are new to OVH, we recommend you first read the following instructions for creating a zone. Creating a zone using the OVH manager Creating a zone using the OVH API Creating OVH Credentials \u00b6 You first need to create an OVH application. Using the OVH documentation you will have your Application key and Application secret And you will need to generate your consumer key, here the permissions needed : - GET on /domain/zone - GET on /domain/zone/*/record - GET on /domain/zone/*/record/* - POST on /domain/zone/*/record - DELETE on /domain/zone/*/record/* - POST on /domain/zone/*/refresh You can use the following curl request to generate & validated your Consumer key curl -XPOST -H \"X-Ovh-Application: <ApplicationKey>\" -H \"Content-type: application/json\" https://eu.api.ovh.com/1.0/auth/credential -d '{ \"accessRules\": [ { \"method\": \"GET\", \"path\": \"/domain/zone\" }, { \"method\": \"GET\", \"path\": \"/domain/zone/*/record\" }, { \"method\": \"GET\", \"path\": \"/domain/zone/*/record/*\" }, { \"method\": \"POST\", \"path\": \"/domain/zone/*/record\" }, { \"method\": \"DELETE\", \"path\": \"/domain/zone/*/record/*\" }, { \"method\": \"POST\", \"path\": \"/domain/zone/*/refresh\" } ], \"redirection\":\"https://github.com/kubernetes-sigs/external-dns/blob/HEAD/docs/tutorials/ovh.md#creating-ovh-credentials\" }' Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment: Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ovh env : - name : OVH_APPLICATION_KEY value : \"YOUR_OVH_APPLICATION_KEY\" - name : OVH_APPLICATION_SECRET value : \"YOUR_OVH_APPLICATION_SECRET\" - name : OVH_CONSUMER_KEY value : \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"endpoints\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ovh env : - name : OVH_APPLICATION_KEY value : \"YOUR_OVH_APPLICATION_KEY\" - name : OVH_APPLICATION_SECRET value : \"YOUR_OVH_APPLICATION_SECRET\" - name : OVH_CONSUMER_KEY value : \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the OVH DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service \u00b6 $ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the OVH DNS records. Verifying OVH DNS records \u00b6 Use the OVH manager or API to verify that the A record for your domain shows the external IP address of the services. Cleanup \u00b6 Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Setting up ExternalDNS for Services on OVH"},{"location":"tutorials/ovh/#setting-up-externaldns-for-services-on-ovh","text":"This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using OVH DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on OVH"},{"location":"tutorials/ovh/#creating-a-zone-with-ovh-dns","text":"If you are new to OVH, we recommend you first read the following instructions for creating a zone. Creating a zone using the OVH manager Creating a zone using the OVH API","title":"Creating a zone with OVH DNS"},{"location":"tutorials/ovh/#creating-ovh-credentials","text":"You first need to create an OVH application. Using the OVH documentation you will have your Application key and Application secret And you will need to generate your consumer key, here the permissions needed : - GET on /domain/zone - GET on /domain/zone/*/record - GET on /domain/zone/*/record/* - POST on /domain/zone/*/record - DELETE on /domain/zone/*/record/* - POST on /domain/zone/*/refresh You can use the following curl request to generate & validated your Consumer key curl -XPOST -H \"X-Ovh-Application: <ApplicationKey>\" -H \"Content-type: application/json\" https://eu.api.ovh.com/1.0/auth/credential -d '{ \"accessRules\": [ { \"method\": \"GET\", \"path\": \"/domain/zone\" }, { \"method\": \"GET\", \"path\": \"/domain/zone/*/record\" }, { \"method\": \"GET\", \"path\": \"/domain/zone/*/record/*\" }, { \"method\": \"POST\", \"path\": \"/domain/zone/*/record\" }, { \"method\": \"DELETE\", \"path\": \"/domain/zone/*/record/*\" }, { \"method\": \"POST\", \"path\": \"/domain/zone/*/refresh\" } ], \"redirection\":\"https://github.com/kubernetes-sigs/external-dns/blob/HEAD/docs/tutorials/ovh.md#creating-ovh-credentials\" }'","title":"Creating OVH Credentials"},{"location":"tutorials/ovh/#deploy-externaldns","text":"Connect your kubectl client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:","title":"Deploy ExternalDNS"},{"location":"tutorials/ovh/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ovh env : - name : OVH_APPLICATION_KEY value : \"YOUR_OVH_APPLICATION_KEY\" - name : OVH_APPLICATION_SECRET value : \"YOUR_OVH_APPLICATION_SECRET\" - name : OVH_CONSUMER_KEY value : \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/ovh/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] - apiGroups : [ \"\" ] resources : [ \"endpoints\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=ovh env : - name : OVH_APPLICATION_KEY value : \"YOUR_OVH_APPLICATION_KEY\" - name : OVH_APPLICATION_SECRET value : \"YOUR_OVH_APPLICATION_SECRET\" - name : OVH_CONSUMER_KEY value : \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/ovh/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 A note about annotations Verify that the annotation on the service uses the same hostname as the OVH DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10. ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/ovh/#create-the-deployment-and-service","text":"$ kubectl create -f nginx.yaml Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the OVH DNS records.","title":"Create the deployment and service"},{"location":"tutorials/ovh/#verifying-ovh-dns-records","text":"Use the OVH manager or API to verify that the A record for your domain shows the external IP address of the services.","title":"Verifying OVH DNS records"},{"location":"tutorials/ovh/#cleanup","text":"Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/pdns/","text":"Setting up ExternalDNS for PowerDNS \u00b6 Prerequisites \u00b6 The provider has been written for and tested against PowerDNS v4.1.x and thus requires PowerDNS Auth Server >= 4.1.x PowerDNS provider support was added via this PR , thus you need to use external-dns version >= v0.5 The PDNS provider expects that your PowerDNS instance is already setup and functional. It expects that zones, you wish to add records to, already exist and are configured correctly. It does not add, remove or configure new zones in anyway. Feature Support \u00b6 The PDNS provider currently does not support: Dry running a configuration is not supported Deployment \u00b6 Deploying external DNS for PowerDNS is actually nearly identical to deploying it for other providers. This is what a sample deployment.yaml looks like: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : # Only use if you're also using RBAC # serviceAccountName: external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=pdns - --pdns-server={{ pdns-api-url }} - --pdns-api-key={{ pdns-http-api-key }} - --txt-owner-id={{ owner-id-for-this-external-dns }} - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the zones matching provided domain; omit to process all available zones in PowerDNS - --log-level=debug - --interval=30s Domain Filter ( --domain-filter ) \u00b6 When the --domain-filter argument is specified, external-dns will only create DNS records for host names (specified in ingress objects and services with the external-dns annotation) related to zones that match the --domain-filter argument in the external-dns deployment manifest. eg. --domain-filter=example.org will allow for zone example.org and any zones in PowerDNS that ends in .example.org , including an.example.org , ie. the subdomains of example.org. eg. --domain-filter=.example.org will allow only zones that end in .example.org , ie. the subdomains of example.org but not the example.org zone itself. The filter can also match parent zones. For example --domain-filter=a.example.com will allow for zone example.com . If you want to match parent zones, you cannot pre-pend your filter with a \u201c.\u201d, eg. --domain-filter=.example.com will not attempt to match parent zones. RBAC \u00b6 If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns: apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default Testing and Verification \u00b6 Important! : Remember to change example.com with your own domain throughout the following text. Spin up a simple \u201cHello World\u201d HTTP server with the following spec ( kubectl apply -f ): apiVersion : apps/v1 kind : Deployment metadata : name : echo spec : selector : matchLabels : app : echo template : metadata : labels : app : echo spec : containers : - image : hashicorp/http-echo name : echo ports : - containerPort : 5678 args : - -text=\"Hello World\" --- apiVersion : v1 kind : Service metadata : name : echo annotations : external-dns.alpha.kubernetes.io/hostname : echo.example.com spec : selector : app : echo type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 5678 Important! : Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching , which is hard to flush. Run the following to make sure everything is in order: $ kubectl get services echo $ kubectl get endpoints echo Make sure everything looks correct, i.e the service is defined and receives a public IP, and that the endpoint also has a pod IP. Once that\u2019s done, wait about 30s-1m (interval for external-dns to kick in), then do: $ curl -H \"X-API-Key: ${ PDNS_API_KEY } \" ${ PDNS_API_URL } /api/v1/servers/localhost/zones/example.com. | jq '.rrsets[] | select(.name | contains(\"echo\"))' Once the API shows the record correctly, you can double check your record using: $ dig @ ${ PDNS_FQDN } echo.example.com.","title":"Setting up ExternalDNS for PowerDNS"},{"location":"tutorials/pdns/#setting-up-externaldns-for-powerdns","text":"","title":"Setting up ExternalDNS for PowerDNS"},{"location":"tutorials/pdns/#prerequisites","text":"The provider has been written for and tested against PowerDNS v4.1.x and thus requires PowerDNS Auth Server >= 4.1.x PowerDNS provider support was added via this PR , thus you need to use external-dns version >= v0.5 The PDNS provider expects that your PowerDNS instance is already setup and functional. It expects that zones, you wish to add records to, already exist and are configured correctly. It does not add, remove or configure new zones in anyway.","title":"Prerequisites"},{"location":"tutorials/pdns/#feature-support","text":"The PDNS provider currently does not support: Dry running a configuration is not supported","title":"Feature Support"},{"location":"tutorials/pdns/#deployment","text":"Deploying external DNS for PowerDNS is actually nearly identical to deploying it for other providers. This is what a sample deployment.yaml looks like: apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : # Only use if you're also using RBAC # serviceAccountName: external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # or ingress or both - --provider=pdns - --pdns-server={{ pdns-api-url }} - --pdns-api-key={{ pdns-http-api-key }} - --txt-owner-id={{ owner-id-for-this-external-dns }} - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the zones matching provided domain; omit to process all available zones in PowerDNS - --log-level=debug - --interval=30s","title":"Deployment"},{"location":"tutorials/pdns/#domain-filter-domain-filter","text":"When the --domain-filter argument is specified, external-dns will only create DNS records for host names (specified in ingress objects and services with the external-dns annotation) related to zones that match the --domain-filter argument in the external-dns deployment manifest. eg. --domain-filter=example.org will allow for zone example.org and any zones in PowerDNS that ends in .example.org , including an.example.org , ie. the subdomains of example.org. eg. --domain-filter=.example.org will allow only zones that end in .example.org , ie. the subdomains of example.org but not the example.org zone itself. The filter can also match parent zones. For example --domain-filter=a.example.com will allow for zone example.com . If you want to match parent zones, you cannot pre-pend your filter with a \u201c.\u201d, eg. --domain-filter=.example.com will not attempt to match parent zones.","title":"Domain Filter (--domain-filter)"},{"location":"tutorials/pdns/#rbac","text":"If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns: apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default","title":"RBAC"},{"location":"tutorials/pdns/#testing-and-verification","text":"Important! : Remember to change example.com with your own domain throughout the following text. Spin up a simple \u201cHello World\u201d HTTP server with the following spec ( kubectl apply -f ): apiVersion : apps/v1 kind : Deployment metadata : name : echo spec : selector : matchLabels : app : echo template : metadata : labels : app : echo spec : containers : - image : hashicorp/http-echo name : echo ports : - containerPort : 5678 args : - -text=\"Hello World\" --- apiVersion : v1 kind : Service metadata : name : echo annotations : external-dns.alpha.kubernetes.io/hostname : echo.example.com spec : selector : app : echo type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 5678 Important! : Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching , which is hard to flush. Run the following to make sure everything is in order: $ kubectl get services echo $ kubectl get endpoints echo Make sure everything looks correct, i.e the service is defined and receives a public IP, and that the endpoint also has a pod IP. Once that\u2019s done, wait about 30s-1m (interval for external-dns to kick in), then do: $ curl -H \"X-API-Key: ${ PDNS_API_KEY } \" ${ PDNS_API_URL } /api/v1/servers/localhost/zones/example.com. | jq '.rrsets[] | select(.name | contains(\"echo\"))' Once the API shows the record correctly, you can double check your record using: $ dig @ ${ PDNS_FQDN } echo.example.com.","title":"Testing and Verification"},{"location":"tutorials/pihole/","text":"Setting up ExternalDNS for Pi-hole \u00b6 This tutorial describes how to setup ExternalDNS to sync records with Pi-hole\u2019s Custom DNS. Pi-hole has an internal list it checks last when resolving requests. This list can contain any number of arbitrary A or CNAME records. There is a pseudo-API exposed that ExternalDNS is able to use to manage these records. Deploy ExternalDNS \u00b6 You can skip to the manifest if authentication is disabled on your Pi-hole instance or you don\u2019t want to use secrets. If your Pi-hole server\u2019s admin dashboard is protected by a password, you\u2019ll likely want to create a secret first containing its value. This is optional since you do retain the option to pass it as a flag with --pihole-password . You can create the secret with: kubectl create secret generic pihole-password \\ --from-literal EXTERNAL_DNS_PIHOLE_PASSWORD = supersecret Replacing \u201csupersecret\u201d with the actual password to your Pi-hole server. ExternalDNS Manifest \u00b6 Apply the following manifest to deploy ExternalDNS, editing values for your environment accordingly. Be sure to change the namespace in the ClusterRoleBinding if you are using a namespace other than default . --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:latest # If authentication is disabled and/or you didn't create # a secret, you can remove this block. envFrom : - secretRef : # Change this if you gave the secret a different name name : pihole-password args : - --source=service - --source=ingress # Pihole only supports A/CNAME records so there is no mechanism to track ownership. # You don't need to set this flag, but if you leave it unset, you will receive warning # logs when ExternalDNS attempts to create TXT records. - --registry=noop # IMPORTANT: If you have records that you manage manually in Pi-hole, set # the policy to upsert-only so they do not get deleted. - --policy=upsert-only - --provider=pihole # Change this to the actual address of your Pi-hole web server - --pihole-server=http://pihole-web.pihole.svc.cluster.local securityContext : fsGroup : 65534 # For ExternalDNS to be able to read Kubernetes token files Arguments \u00b6 --pihole-server (env: EXTERNAL_DNS_PIHOLE_SERVER) - The address of the Pi-hole web server --pihole-password (env: EXTERNAL_DNS_PIHOLE_PASSWORD) - The password to the Pi-hole web server (if enabled) --pihole-tls-skip-verify (env: EXTERNAL_DNS_PIHOLE_TLS_SKIP_VERIFY) - Skip verification of any TLS certificates served by the Pi-hole web server. Verify ExternalDNS Works \u00b6 Ingress Example \u00b6 Create an Ingress resource. ExternalDNS will use the hostname specified in the Ingress object. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : foo spec : ingressClassName : nginx rules : - host : foo.bar.com http : paths : - path : / pathType : Prefix backend : service : name : foo port : number : 80 Service Example \u00b6 The below sample application can be used to verify Services work. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.homelab.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http You can then query your Pi-hole to see if the record was created. Change @192.168.100.2 to the actual address of your DNS server $ dig +short @192.168.100.2 nginx.external-dns-test.homelab.com 192 .168.100.129","title":"Setting up ExternalDNS for Pi-hole"},{"location":"tutorials/pihole/#setting-up-externaldns-for-pi-hole","text":"This tutorial describes how to setup ExternalDNS to sync records with Pi-hole\u2019s Custom DNS. Pi-hole has an internal list it checks last when resolving requests. This list can contain any number of arbitrary A or CNAME records. There is a pseudo-API exposed that ExternalDNS is able to use to manage these records.","title":"Setting up ExternalDNS for Pi-hole"},{"location":"tutorials/pihole/#deploy-externaldns","text":"You can skip to the manifest if authentication is disabled on your Pi-hole instance or you don\u2019t want to use secrets. If your Pi-hole server\u2019s admin dashboard is protected by a password, you\u2019ll likely want to create a secret first containing its value. This is optional since you do retain the option to pass it as a flag with --pihole-password . You can create the secret with: kubectl create secret generic pihole-password \\ --from-literal EXTERNAL_DNS_PIHOLE_PASSWORD = supersecret Replacing \u201csupersecret\u201d with the actual password to your Pi-hole server.","title":"Deploy ExternalDNS"},{"location":"tutorials/pihole/#externaldns-manifest","text":"Apply the following manifest to deploy ExternalDNS, editing values for your environment accordingly. Be sure to change the namespace in the ClusterRoleBinding if you are using a namespace other than default . --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:latest # If authentication is disabled and/or you didn't create # a secret, you can remove this block. envFrom : - secretRef : # Change this if you gave the secret a different name name : pihole-password args : - --source=service - --source=ingress # Pihole only supports A/CNAME records so there is no mechanism to track ownership. # You don't need to set this flag, but if you leave it unset, you will receive warning # logs when ExternalDNS attempts to create TXT records. - --registry=noop # IMPORTANT: If you have records that you manage manually in Pi-hole, set # the policy to upsert-only so they do not get deleted. - --policy=upsert-only - --provider=pihole # Change this to the actual address of your Pi-hole web server - --pihole-server=http://pihole-web.pihole.svc.cluster.local securityContext : fsGroup : 65534 # For ExternalDNS to be able to read Kubernetes token files","title":"ExternalDNS Manifest"},{"location":"tutorials/pihole/#arguments","text":"--pihole-server (env: EXTERNAL_DNS_PIHOLE_SERVER) - The address of the Pi-hole web server --pihole-password (env: EXTERNAL_DNS_PIHOLE_PASSWORD) - The password to the Pi-hole web server (if enabled) --pihole-tls-skip-verify (env: EXTERNAL_DNS_PIHOLE_TLS_SKIP_VERIFY) - Skip verification of any TLS certificates served by the Pi-hole web server.","title":"Arguments"},{"location":"tutorials/pihole/#verify-externaldns-works","text":"","title":"Verify ExternalDNS Works"},{"location":"tutorials/pihole/#ingress-example","text":"Create an Ingress resource. ExternalDNS will use the hostname specified in the Ingress object. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : foo spec : ingressClassName : nginx rules : - host : foo.bar.com http : paths : - path : / pathType : Prefix backend : service : name : foo port : number : 80","title":"Ingress Example"},{"location":"tutorials/pihole/#service-example","text":"The below sample application can be used to verify Services work. For services ExternalDNS will look for the annotation external-dns.alpha.kubernetes.io/hostname on the service and use the corresponding value. --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.homelab.com spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http You can then query your Pi-hole to see if the record was created. Change @192.168.100.2 to the actual address of your DNS server $ dig +short @192.168.100.2 nginx.external-dns-test.homelab.com 192 .168.100.129","title":"Service Example"},{"location":"tutorials/plural/","text":"Setting up ExternalDNS for Services on Plural \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Plural DNS. Make sure to use >=0.12.3 version of ExternalDNS for this tutorial. Creating Plural Credentials \u00b6 A secret containing the a Plural access token is needed for this provider. You can get a token for your user here . To create the secret you can run kubectl create secret generic plural-env --from-literal=PLURAL_ACCESS_TOKEN=<replace-with-your-access-token> . Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=plural - --plural-cluster=example-plural-cluster - --plural-provider=aws # gcp, azure, equinix and kind are also possible env : - name : PLURAL_ACCESS_TOKEN valueFrom : secretKeyRef : key : PLURAL_ACCESS_TOKEN name : plural-env - name : PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh value : https://app.plural.sh Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=plural - --plural-cluster=example-plural-cluster - --plural-provider=aws # gcp, azure, equinix and kind are also possible env : - name : PLURAL_ACCESS_TOKEN valueFrom : secretKeyRef : key : PLURAL_ACCESS_TOKEN name : plural-env - name : PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh value : https://app.plural.sh Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Plural DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Plural DNS records. Verifying Plural DNS records \u00b6 Check your Plural domain overview to view the domains associated with your Plural account. There you can view the records for each domain. The records should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Plural DNS records, we can delete the tutorial\u2019s example: ``` $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Setting up ExternalDNS for Services on Plural"},{"location":"tutorials/plural/#setting-up-externaldns-for-services-on-plural","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Plural DNS. Make sure to use >=0.12.3 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Plural"},{"location":"tutorials/plural/#creating-plural-credentials","text":"A secret containing the a Plural access token is needed for this provider. You can get a token for your user here . To create the secret you can run kubectl create secret generic plural-env --from-literal=PLURAL_ACCESS_TOKEN=<replace-with-your-access-token> .","title":"Creating Plural Credentials"},{"location":"tutorials/plural/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/plural/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=plural - --plural-cluster=example-plural-cluster - --plural-provider=aws # gcp, azure, equinix and kind are also possible env : - name : PLURAL_ACCESS_TOKEN valueFrom : secretKeyRef : key : PLURAL_ACCESS_TOKEN name : plural-env - name : PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh value : https://app.plural.sh","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/plural/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=plural - --plural-cluster=example-plural-cluster - --plural-provider=aws # gcp, azure, equinix and kind are also possible env : - name : PLURAL_ACCESS_TOKEN valueFrom : secretKeyRef : key : PLURAL_ACCESS_TOKEN name : plural-env - name : PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh value : https://app.plural.sh","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/plural/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Plural DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Plural DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/plural/#verifying-plural-dns-records","text":"Check your Plural domain overview to view the domains associated with your Plural account. There you can view the records for each domain. The records should show the external IP address of the service as the A record for your domain.","title":"Verifying Plural DNS records"},{"location":"tutorials/plural/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Plural DNS records, we can delete the tutorial\u2019s example: ``` $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/public-private-route53/","text":"Setting up ExternalDNS using the same domain for public and private Route53 zones \u00b6 This tutorial describes how to setup ExternalDNS using the same domain for public and private Route53 zones and nginx-ingress-controller . It also outlines how to use cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt for both public and private records. Deploy public nginx-ingress-controller \u00b6 Consult External DNS nginx ingress docs for installation guidelines. Specify ingress-class in nginx-ingress-controller container args: apiVersion : apps/v1 kind : Deployment metadata : labels : app : external-ingress name : external-ingress-controller spec : replicas : 1 selector : matchLabels : app : external-ingress template : metadata : labels : app : external-ingress spec : containers : - args : - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/external-ingress-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/external-tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/external-udp-services - --annotations-prefix=nginx.ingress.kubernetes.io - --ingress-class=external-ingress - --publish-service=$(POD_NAMESPACE)/external-ingress env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0 livenessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP initialDelaySeconds : 10 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 name : external-ingress-controller ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP readinessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 Set type: LoadBalancer in your public nginx-ingress-controller Service definition. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : \"3600\" service.beta.kubernetes.io/aws-load-balancer-proxy-protocol : '*' labels : app : external-ingress name : external-ingress spec : externalTrafficPolicy : Cluster ports : - name : http port : 80 protocol : TCP targetPort : http - name : https port : 443 protocol : TCP targetPort : https selector : app : external-ingress sessionAffinity : None type : LoadBalancer Deploy private nginx-ingress-controller \u00b6 Consult External DNS nginx ingress docs for installation guidelines. Make sure to specify ingress-class in nginx-ingress-controller container args: apiVersion : apps/v1 kind : Deployment metadata : labels : app : internal-ingress name : internal-ingress-controller spec : replicas : 1 selector : matchLabels : app : internal-ingress template : metadata : labels : app : internal-ingress spec : containers : - args : - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/internal-ingress-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/internal-tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/internal-udp-services - --annotations-prefix=nginx.ingress.kubernetes.io - --ingress-class=internal-ingress - --publish-service=$(POD_NAMESPACE)/internal-ingress env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0 livenessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP initialDelaySeconds : 10 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 name : internal-ingress-controller ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP readinessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 Set additional annotations in your private nginx-ingress-controller Service definition to create an internal load balancer. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : \"3600\" service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 service.beta.kubernetes.io/aws-load-balancer-proxy-protocol : '*' labels : app : internal-ingress name : internal-ingress spec : externalTrafficPolicy : Cluster ports : - name : http port : 80 protocol : TCP targetPort : http - name : https port : 443 protocol : TCP targetPort : https selector : app : internal-ingress sessionAffinity : None type : LoadBalancer Deploy the public zone ExternalDNS \u00b6 Consult AWS ExternalDNS setup docs for installation guidelines. In ExternalDNS containers args, make sure to specify annotation-filter and aws-zone-type : apiVersion : apps/v1beta2 kind : Deployment metadata : labels : app : external-dns-public name : external-dns-public namespace : kube-system spec : replicas : 1 selector : matchLabels : app : external-dns-public strategy : type : Recreate template : metadata : labels : app : external-dns-public spec : containers : - args : - --source=ingress - --provider=aws - --registry=txt - --txt-owner-id=external-dns - --annotation-filter=kubernetes.io/ingress.class in (external-ingress) - --aws-zone-type=public image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns-public Deploy the private zone ExternalDNS \u00b6 Consult AWS ExternalDNS setup docs for installation guidelines. In ExternalDNS containers args, make sure to specify annotation-filter and aws-zone-type : apiVersion : apps/v1beta2 kind : Deployment metadata : labels : app : external-dns-private name : external-dns-private namespace : kube-system spec : replicas : 1 selector : matchLabels : app : external-dns-private strategy : type : Recreate template : metadata : labels : app : external-dns-private spec : containers : - args : - --source=ingress - --provider=aws - --registry=txt - --txt-owner-id=dev.k8s.nexus - --annotation-filter=kubernetes.io/ingress.class in (internal-ingress) - --aws-zone-type=private image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns-private Create application Service definitions \u00b6 For this setup to work, you\u2019ve to create two Service definitions for your application. At first, create public Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"external-ingress\" labels : app : app name : app-public spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix Then create private Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"internal-ingress\" labels : app : app name : app-private spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix Additionally, you may leverage cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt . To do that, request a certificate in public service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : certmanager.k8s.io/acme-challenge-type : \"dns01\" certmanager.k8s.io/acme-dns01-provider : \"route53\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"external-ingress\" kubernetes.io/tls-acme : \"true\" labels : app : app name : app-public spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix tls : - hosts : - app.domain.com secretName : app-tls And reuse the requested certificate in private Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"internal-ingress\" labels : app : app name : app-private spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix tls : - hosts : - app.domain.com secretName : app-tls","title":"Setting up ExternalDNS using the same domain for public and private Route53 zones"},{"location":"tutorials/public-private-route53/#setting-up-externaldns-using-the-same-domain-for-public-and-private-route53-zones","text":"This tutorial describes how to setup ExternalDNS using the same domain for public and private Route53 zones and nginx-ingress-controller . It also outlines how to use cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt for both public and private records.","title":"Setting up ExternalDNS using the same domain for public and private Route53 zones"},{"location":"tutorials/public-private-route53/#deploy-public-nginx-ingress-controller","text":"Consult External DNS nginx ingress docs for installation guidelines. Specify ingress-class in nginx-ingress-controller container args: apiVersion : apps/v1 kind : Deployment metadata : labels : app : external-ingress name : external-ingress-controller spec : replicas : 1 selector : matchLabels : app : external-ingress template : metadata : labels : app : external-ingress spec : containers : - args : - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/external-ingress-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/external-tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/external-udp-services - --annotations-prefix=nginx.ingress.kubernetes.io - --ingress-class=external-ingress - --publish-service=$(POD_NAMESPACE)/external-ingress env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0 livenessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP initialDelaySeconds : 10 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 name : external-ingress-controller ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP readinessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 Set type: LoadBalancer in your public nginx-ingress-controller Service definition. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : \"3600\" service.beta.kubernetes.io/aws-load-balancer-proxy-protocol : '*' labels : app : external-ingress name : external-ingress spec : externalTrafficPolicy : Cluster ports : - name : http port : 80 protocol : TCP targetPort : http - name : https port : 443 protocol : TCP targetPort : https selector : app : external-ingress sessionAffinity : None type : LoadBalancer","title":"Deploy public nginx-ingress-controller"},{"location":"tutorials/public-private-route53/#deploy-private-nginx-ingress-controller","text":"Consult External DNS nginx ingress docs for installation guidelines. Make sure to specify ingress-class in nginx-ingress-controller container args: apiVersion : apps/v1 kind : Deployment metadata : labels : app : internal-ingress name : internal-ingress-controller spec : replicas : 1 selector : matchLabels : app : internal-ingress template : metadata : labels : app : internal-ingress spec : containers : - args : - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/internal-ingress-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/internal-tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/internal-udp-services - --annotations-prefix=nginx.ingress.kubernetes.io - --ingress-class=internal-ingress - --publish-service=$(POD_NAMESPACE)/internal-ingress env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.namespace image : quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0 livenessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP initialDelaySeconds : 10 periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 name : internal-ingress-controller ports : - containerPort : 80 name : http protocol : TCP - containerPort : 443 name : https protocol : TCP readinessProbe : failureThreshold : 3 httpGet : path : /healthz port : 10254 scheme : HTTP periodSeconds : 10 successThreshold : 1 timeoutSeconds : 1 Set additional annotations in your private nginx-ingress-controller Service definition to create an internal load balancer. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : \"3600\" service.beta.kubernetes.io/aws-load-balancer-internal : 0.0.0.0/0 service.beta.kubernetes.io/aws-load-balancer-proxy-protocol : '*' labels : app : internal-ingress name : internal-ingress spec : externalTrafficPolicy : Cluster ports : - name : http port : 80 protocol : TCP targetPort : http - name : https port : 443 protocol : TCP targetPort : https selector : app : internal-ingress sessionAffinity : None type : LoadBalancer","title":"Deploy private nginx-ingress-controller"},{"location":"tutorials/public-private-route53/#deploy-the-public-zone-externaldns","text":"Consult AWS ExternalDNS setup docs for installation guidelines. In ExternalDNS containers args, make sure to specify annotation-filter and aws-zone-type : apiVersion : apps/v1beta2 kind : Deployment metadata : labels : app : external-dns-public name : external-dns-public namespace : kube-system spec : replicas : 1 selector : matchLabels : app : external-dns-public strategy : type : Recreate template : metadata : labels : app : external-dns-public spec : containers : - args : - --source=ingress - --provider=aws - --registry=txt - --txt-owner-id=external-dns - --annotation-filter=kubernetes.io/ingress.class in (external-ingress) - --aws-zone-type=public image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns-public","title":"Deploy the public zone ExternalDNS"},{"location":"tutorials/public-private-route53/#deploy-the-private-zone-externaldns","text":"Consult AWS ExternalDNS setup docs for installation guidelines. In ExternalDNS containers args, make sure to specify annotation-filter and aws-zone-type : apiVersion : apps/v1beta2 kind : Deployment metadata : labels : app : external-dns-private name : external-dns-private namespace : kube-system spec : replicas : 1 selector : matchLabels : app : external-dns-private strategy : type : Recreate template : metadata : labels : app : external-dns-private spec : containers : - args : - --source=ingress - --provider=aws - --registry=txt - --txt-owner-id=dev.k8s.nexus - --annotation-filter=kubernetes.io/ingress.class in (internal-ingress) - --aws-zone-type=private image : registry.k8s.io/external-dns/external-dns:v0.13.1 name : external-dns-private","title":"Deploy the private zone ExternalDNS"},{"location":"tutorials/public-private-route53/#create-application-service-definitions","text":"For this setup to work, you\u2019ve to create two Service definitions for your application. At first, create public Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"external-ingress\" labels : app : app name : app-public spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix Then create private Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"internal-ingress\" labels : app : app name : app-private spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix Additionally, you may leverage cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt . To do that, request a certificate in public service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : certmanager.k8s.io/acme-challenge-type : \"dns01\" certmanager.k8s.io/acme-dns01-provider : \"route53\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"external-ingress\" kubernetes.io/tls-acme : \"true\" labels : app : app name : app-public spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix tls : - hosts : - app.domain.com secretName : app-tls And reuse the requested certificate in private Service definition: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : \"internal-ingress\" labels : app : app name : app-private spec : rules : - host : app.domain.com http : paths : - backend : service : name : app port : number : 80 pathType : Prefix tls : - hosts : - app.domain.com secretName : app-tls","title":"Create application Service definitions"},{"location":"tutorials/rcodezero/","text":"Setting up ExternalDNS for Services on RcodeZero \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using RcodeZero Anycast DNS . Make sure to use >=0.5.0 version of ExternalDNS for this tutorial. The following steps are required to use RcodeZero with ExternalDNS: Sign up for an RcodeZero account (or use an existing account). Add your zone to the RcodeZero DNS Enable the RcodeZero API, and generate an API key. Deploy ExternalDNS to use the RcodeZero provider. Verify the setup bey deploying a test services (optional) Creating a RcodeZero DNS zone \u00b6 Before records can be added to your domain name automatically, you need to add your domain name to the set of zones managed by RcodeZero. In order to add the zone, perform the following steps: Log in to the RcodeZero Dashboard, and move to the Add Zone page. Select \u201cMASTER\u201d as domain type, and add your domain name there. Use this domain name instead of \u201cexample.com\u201d throughout the rest of this tutorial. Note that \u201cSECONDARY\u201d domains cannot be managed by ExternalDNS, because this would not allow modification of records in the zone. Enable the API, and create Credentials \u00b6 The RcodeZero Anycast-Network is provisioned via web interface or REST-API. Enable the RcodeZero API to generate an API key on RcodeZero API . The API key will be added to the environment variable \u2018RC0_API_KEY\u2019 via one of the Manifest templates (as described below). Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Choose a Manifest from below, depending on whether or not you have RBAC enabled. Before applying it, modify the Manifest as follows: Replace \u201cexample.com\u201d with the domain name you added to RcodeZero. Replace YOUR_RCODEZERO_API_KEY with the API key created above. Replace YOUR_ENCRYPTION_KEY_STRING with a string to encrypt the TXT records Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=rcodezero - --rc0-enc-txt # (optional) encrypt TXT records; encryption key has to be provided with RC0_ENC_KEY env var. env : - name : RC0_API_KEY value : \"YOUR_RCODEZERO_API_KEY\" - name : RC0_ENC_VAR value : \"YOUR_ENCRYPTION_KEY_STRING\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=rcodezero - --rc0-enc-txt # (optional) encrypt TXT records; encryption key has to be provided with RC0_ENC_KEY env var. env : - name : RC0_API_KEY value : \"YOUR_RCODEZERO_API_KEY\" - name : RC0_ENC_VAR value : \"YOUR_ENCRYPTION_KEY_STRING\" Deploying an Nginx Service \u00b6 After you have deployed ExternalDNS with RcodeZero, you can deploy a simple service based on Nginx to test the setup. This is optional, though highly recommended before using ExternalDNS in production. Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Change the file as follows: Replace the annotation of the service; use the same hostname as the RcodeZero DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). Set the TTL annotation of the service. A valid TTL of 120 or above must be given. This annotation is optional, and defaults to \u201c300\u201d if no value is given. These annotations will be used to determine what services should be registered with DNS. Removing these annotations will cause ExternalDNS to remove the corresponding DNS records. Create the Deployment and Service: $ kubectl create -f nginx.yaml Depending on your cloud provider, it might take a while to create an external IP for the service. Once an external IP address is assigned to the service, ExternalDNS will notice the new address and synchronize the RcodeZero DNS records accordingly. Verifying RcodeZero DNS records \u00b6 Check your RcodeZero Configured Zones and select the respective zone name. The zone should now contain the external IP address of the service as an A record. Cleanup \u00b6 Once you have verified that ExternalDNS successfully manages RcodeZero DNS records for external services, you can delete the tutorial example as follows: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Setting up ExternalDNS for Services on RcodeZero"},{"location":"tutorials/rcodezero/#setting-up-externaldns-for-services-on-rcodezero","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using RcodeZero Anycast DNS . Make sure to use >=0.5.0 version of ExternalDNS for this tutorial. The following steps are required to use RcodeZero with ExternalDNS: Sign up for an RcodeZero account (or use an existing account). Add your zone to the RcodeZero DNS Enable the RcodeZero API, and generate an API key. Deploy ExternalDNS to use the RcodeZero provider. Verify the setup bey deploying a test services (optional)","title":"Setting up ExternalDNS for Services on RcodeZero"},{"location":"tutorials/rcodezero/#creating-a-rcodezero-dns-zone","text":"Before records can be added to your domain name automatically, you need to add your domain name to the set of zones managed by RcodeZero. In order to add the zone, perform the following steps: Log in to the RcodeZero Dashboard, and move to the Add Zone page. Select \u201cMASTER\u201d as domain type, and add your domain name there. Use this domain name instead of \u201cexample.com\u201d throughout the rest of this tutorial. Note that \u201cSECONDARY\u201d domains cannot be managed by ExternalDNS, because this would not allow modification of records in the zone.","title":"Creating a RcodeZero DNS zone"},{"location":"tutorials/rcodezero/#enable-the-api-and-create-credentials","text":"The RcodeZero Anycast-Network is provisioned via web interface or REST-API. Enable the RcodeZero API to generate an API key on RcodeZero API . The API key will be added to the environment variable \u2018RC0_API_KEY\u2019 via one of the Manifest templates (as described below).","title":"Enable the API, and create Credentials"},{"location":"tutorials/rcodezero/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Choose a Manifest from below, depending on whether or not you have RBAC enabled. Before applying it, modify the Manifest as follows: Replace \u201cexample.com\u201d with the domain name you added to RcodeZero. Replace YOUR_RCODEZERO_API_KEY with the API key created above. Replace YOUR_ENCRYPTION_KEY_STRING with a string to encrypt the TXT records","title":"Deploy ExternalDNS"},{"location":"tutorials/rcodezero/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=rcodezero - --rc0-enc-txt # (optional) encrypt TXT records; encryption key has to be provided with RC0_ENC_KEY env var. env : - name : RC0_API_KEY value : \"YOUR_RCODEZERO_API_KEY\" - name : RC0_ENC_VAR value : \"YOUR_ENCRYPTION_KEY_STRING\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/rcodezero/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=rcodezero - --rc0-enc-txt # (optional) encrypt TXT records; encryption key has to be provided with RC0_ENC_KEY env var. env : - name : RC0_API_KEY value : \"YOUR_RCODEZERO_API_KEY\" - name : RC0_ENC_VAR value : \"YOUR_ENCRYPTION_KEY_STRING\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/rcodezero/#deploying-an-nginx-service","text":"After you have deployed ExternalDNS with RcodeZero, you can deploy a simple service based on Nginx to test the setup. This is optional, though highly recommended before using ExternalDNS in production. Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : example.com external-dns.alpha.kubernetes.io/ttl : \"120\" #optional spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Change the file as follows: Replace the annotation of the service; use the same hostname as the RcodeZero DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019). Set the TTL annotation of the service. A valid TTL of 120 or above must be given. This annotation is optional, and defaults to \u201c300\u201d if no value is given. These annotations will be used to determine what services should be registered with DNS. Removing these annotations will cause ExternalDNS to remove the corresponding DNS records. Create the Deployment and Service: $ kubectl create -f nginx.yaml Depending on your cloud provider, it might take a while to create an external IP for the service. Once an external IP address is assigned to the service, ExternalDNS will notice the new address and synchronize the RcodeZero DNS records accordingly.","title":"Deploying an Nginx Service"},{"location":"tutorials/rcodezero/#verifying-rcodezero-dns-records","text":"Check your RcodeZero Configured Zones and select the respective zone name. The zone should now contain the external IP address of the service as an A record.","title":"Verifying RcodeZero DNS records"},{"location":"tutorials/rcodezero/#cleanup","text":"Once you have verified that ExternalDNS successfully manages RcodeZero DNS records for external services, you can delete the tutorial example as follows: $ kubectl delete -f nginx.yaml $ kubectl delete -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/rdns/","text":"Setting up ExternalDNS for RancherDNS(RDNS) with kubernetes \u00b6 This tutorial describes how to setup ExternalDNS for usage within a kubernetes cluster that makes use of RDNS and nginx ingress controller . You need to: * install RDNS with etcd enabled * install external-dns with rdns as a provider Installing RDNS with etcdv3 backend \u00b6 Clone RDNS \u00b6 git clone https://github.com/rancher/rdns-server.git Installing ETCD \u00b6 cd rdns-server docker-compose -f deploy/etcdv3/etcd-compose.yaml up -d ETCD was successfully deployed on http://172.31.35.77:2379 Installing RDNS \u00b6 export ETCD_ENDPOINTS=\"http://172.31.35.77:2379\" export DOMAIN=\"lb.rancher.cloud\" ./scripts/start etcdv3 RDNS was successfully deployed on 172.31.35.77 Installing ExternalDNS \u00b6 Install external ExternalDNS \u00b6 ETCD_URLS is configured to etcd client service address. RDNS_ROOT_DOMAIN is configured to the same with RDNS DOMAIN environment. e.g. lb.rancher.cloud. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=rdns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://172.31.35.77:2379 - name : RDNS_ROOT_DOMAIN value : lb.rancher.cloud Manifest (for clusters with RBAC enabled) \u00b6 --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : kube-system --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : kube-system --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=rdns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://172.31.35.77:2379 - name : RDNS_ROOT_DOMAIN value : lb.rancher.cloud Testing ingress example \u00b6 $ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: nginx.lb.rancher.cloud http: paths: - backend: serviceName: nginx servicePort: 80 $ kubectl apply -f ingress.yaml ingress.extensions \"nginx\" created Wait a moment until DNS has the ingress IP. The RDNS IP in this example is \u201c172.31.35.77\u201d. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx nginx.lb.rancher.cloud 172.31.42.211 80 2m $ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools If you don't see a command prompt, try pressing enter. dnstools# dig @172.31.35.77 nginx.lb.rancher.cloud +short 172.31.42.211 dnstools#","title":"Setting up ExternalDNS for RancherDNS(RDNS) with kubernetes"},{"location":"tutorials/rdns/#setting-up-externaldns-for-rancherdnsrdns-with-kubernetes","text":"This tutorial describes how to setup ExternalDNS for usage within a kubernetes cluster that makes use of RDNS and nginx ingress controller . You need to: * install RDNS with etcd enabled * install external-dns with rdns as a provider","title":"Setting up ExternalDNS for RancherDNS(RDNS) with kubernetes"},{"location":"tutorials/rdns/#installing-rdns-with-etcdv3-backend","text":"","title":"Installing RDNS with etcdv3 backend"},{"location":"tutorials/rdns/#clone-rdns","text":"git clone https://github.com/rancher/rdns-server.git","title":"Clone RDNS"},{"location":"tutorials/rdns/#installing-etcd","text":"cd rdns-server docker-compose -f deploy/etcdv3/etcd-compose.yaml up -d ETCD was successfully deployed on http://172.31.35.77:2379","title":"Installing ETCD"},{"location":"tutorials/rdns/#installing-rdns","text":"export ETCD_ENDPOINTS=\"http://172.31.35.77:2379\" export DOMAIN=\"lb.rancher.cloud\" ./scripts/start etcdv3 RDNS was successfully deployed on 172.31.35.77","title":"Installing RDNS"},{"location":"tutorials/rdns/#installing-externaldns","text":"","title":"Installing ExternalDNS"},{"location":"tutorials/rdns/#install-external-externaldns","text":"ETCD_URLS is configured to etcd client service address. RDNS_ROOT_DOMAIN is configured to the same with RDNS DOMAIN environment. e.g. lb.rancher.cloud.","title":"Install external ExternalDNS"},{"location":"tutorials/rdns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=rdns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://172.31.35.77:2379 - name : RDNS_ROOT_DOMAIN value : lb.rancher.cloud","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/rdns/#manifest-for-clusters-with-rbac-enabled","text":"--- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : kube-system --- apiVersion : v1 kind : ServiceAccount metadata : name : external-dns namespace : kube-system --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns namespace : kube-system spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=ingress - --provider=rdns - --log-level=debug # debug only env : - name : ETCD_URLS value : http://172.31.35.77:2379 - name : RDNS_ROOT_DOMAIN value : lb.rancher.cloud","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/rdns/#testing-ingress-example","text":"$ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx annotations: kubernetes.io/ingress.class: \"nginx\" spec: rules: - host: nginx.lb.rancher.cloud http: paths: - backend: serviceName: nginx servicePort: 80 $ kubectl apply -f ingress.yaml ingress.extensions \"nginx\" created Wait a moment until DNS has the ingress IP. The RDNS IP in this example is \u201c172.31.35.77\u201d. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx nginx.lb.rancher.cloud 172.31.42.211 80 2m $ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools If you don't see a command prompt, try pressing enter. dnstools# dig @172.31.35.77 nginx.lb.rancher.cloud +short 172.31.42.211 dnstools#","title":"Testing ingress example"},{"location":"tutorials/rfc2136/","text":"Configuring RFC2136 provider \u00b6 This tutorial describes how to use the RFC2136 with either BIND or Windows DNS. Using with BIND \u00b6 To use external-dns with BIND: generate/procure a key, configure DNS and add a deployment of external-dns. Server credentials: \u00b6 RFC2136 was developed for and tested with BIND DNS server. This documentation assumes that you already have a configured and working server. If you don\u2019t, please check BIND documents or tutorials. If your DNS is provided for you, ask for a TSIG key authorized to update and transfer the zone you wish to update. The key will look something like below. Skip the next steps wrt BIND setup. key \"externaldns-key\" { algorithm hmac-sha256; secret \"96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8=\"; }; If you are your own DNS administrator create a TSIG key. Use tsig-keygen -a hmac-sha256 externaldns or on older distributions dnssec-keygen -a HMAC-SHA256 -b 256 -n HOST externaldns . You will end up with a key printed to standard out like above (or in the case of dnssec-keygen in a file called Kexternaldns......key ). BIND Configuration: \u00b6 If you do not administer your own DNS, skip to RFC provider configuration Edit your named.conf file (or appropriate included file) and add/change the following. Make sure You are listening on the right interfaces. At least whatever interface external-dns will be communicating over and the interface that faces the internet. Add the key that you generated/was given to you above. Copy paste the four lines that you got (not the same as the example key) into your file. Create a zone for kubernetes. If you already have a zone, skip to the next step. (I put the zone in it\u2019s own subdirectory because named, which shouldn\u2019t be running as root, needs to create a journal file and the default zone directory isn\u2019t writeable by named). zone \"k8s.example.org\" { type master; file \"/etc/bind/pri/k8s/k8s.zone\"; }; Add your key to both transfer and update. For instance with our previous zone. zone \"k8s.example.org\" { type master; file \"/etc/bind/pri/k8s/k8s.zone\"; allow-transfer { key \"externaldns-key\"; }; update-policy { grant externaldns-key zonesub ANY; }; }; Create a zone file (k8s.zone): $TTL 60 ; 1 minute k8s.example.org IN SOA k8s.example.org. root.k8s.example.org. ( 16 ; serial 60 ; refresh (1 minute) 60 ; retry (1 minute) 60 ; expire (1 minute) 60 ; minimum (1 minute) ) NS ns.k8s.example.org. ns A 123.456.789.012 Reload (or restart) named Using external-dns \u00b6 To use external-dns add an ingress or a LoadBalancer service with a host that is part of the domain-filter. For example both of the following would produce A records. apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: svc.example.org spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: ingress.example.org http: paths: - path: / backend: serviceName: my-service servicePort: 8000 Custom TTL \u00b6 The default DNS record TTL (Time-To-Live) is 0 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com external-dns.alpha.kubernetes.io/ttl: 60 spec: ... This will set the DNS record\u2019s TTL to 60 seconds. A default TTL for all records can be set using the the flag with a time in seconds, minutes or hours, such as --rfc2136-min-ttl=60s There are other annotation that can affect the generation of DNS records, but these are beyond the scope of this tutorial and are covered in the main documentation. Test with external-dns installed on local machine (optional) \u00b6 You may install external-dns and test on a local machine by running: external-dns --txt-owner-id k8s --provider rfc2136 --rfc2136-host=192.168.0.1 --rfc2136-port=53 --rfc2136-zone=k8s.example.org --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= --rfc2136-tsig-secret-alg=hmac-sha256 --rfc2136-tsig-keyname=externaldns-key --rfc2136-tsig-axfr --source ingress --once --domain-filter=k8s.example.org --dry-run - host should be the IP of your master DNS server. - tsig-secret should be changed to match your secret. - tsig-keyname needs to match the keyname you used (if you changed it). - domain-filter can be used as shown to filter the domains you wish to update. RFC2136 provider configuration: \u00b6 In order to use external-dns with your cluster you need to add a deployment with access to your ingress and service resources. The following are two example manifests with and without RBAC respectively. With RBAC: apiVersion: v1 kind: Namespace metadata: name: external-dns labels: name: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns namespace: external-dns rules: - apiGroups: - \"\" resources: - services - endpoints - pods - nodes verbs: - get - watch - list - apiGroups: - extensions - networking.k8s.io resources: - ingresses verbs: - get - list - watch --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer namespace: external-dns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --registry=txt - --txt-prefix=external-dns- - --txt-owner-id=k8s - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= - --rfc2136-tsig-secret-alg=hmac-sha256 - --rfc2136-tsig-keyname=externaldns-key - --rfc2136-tsig-axfr - --source=ingress - --domain-filter=k8s.example.org Without RBAC: apiVersion: v1 kind: Namespace metadata: name: external-dns labels: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --registry=txt - --txt-prefix=external-dns- - --txt-owner-id=k8s - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= - --rfc2136-tsig-secret-alg=hmac-sha256 - --rfc2136-tsig-keyname=externaldns-key - --rfc2136-tsig-axfr - --source=ingress - --domain-filter=k8s.example.org Microsoft DNS (Insecure Updates) \u00b6 While external-dns was not developed or tested against Microsoft DNS, it can be configured to work against it. YMMV. Insecure Updates \u00b6 DNS-side configuration \u00b6 Create a DNS zone Enable insecure dynamic updates for the zone Enable Zone Transfers to all servers external-dns configuration \u00b6 You\u2019ll want to configure external-dns similarly to the following: ... - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-insecure - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records. ... Secure Updates Using RFC3645 (GSS-TSIG) \u00b6 DNS-side configuration \u00b6 Create a DNS zone Enable secure dynamic updates for the zone Enable Zone Transfers to all servers If you see any error messages which indicate that external-dns was somehow not able to fetch existing DNS records from your DNS server, this could mean that you forgot about step 3. Kerberos Configuration \u00b6 DNS with secure updates relies upon a valid Kerberos configuration running within the external-dns container. At this time, you will need to create a ConfigMap for the external-dns container to use and mount it in your deployment. Below is an example of a working Kerberos configuration inside a ConfigMap definition. This may be different depending on many factors in your environment: apiVersion : v1 kind : ConfigMap metadata : creationTimestamp : null name : krb5.conf data : krb5.conf : | [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt default_ccache_name = KEYRING:persistent:%{uid} default_realm = YOUR-REALM.COM [realms] YOUR-REALM.COM = { kdc = dc1.yourdomain.com admin_server = dc1.yourdomain.com } [domain_realm] yourdomain.com = YOUR-REALM.COM .yourdomain.com = YOUR-REALM.COM In most cases, the realm name will probably be the same as the domain name, so you can simply replace YOUR-REALM.COM with something like YOURDOMAIN.COM . Once the ConfigMap is created, the container external-dns container needs to be told to mount that ConfigMap as a volume at the default Kerberos configuration location. The pod spec should include a similar configuration to the following: ... volumeMounts : - mountPath : /etc/krb5.conf name : kerberos-config-volume subPath : krb5.conf ... volumes : - configMap : defaultMode : 420 name : krb5.conf name : kerberos-config-volume ... external-dns configuration \u00b6 You\u2019ll want to configure external-dns similarly to the following: ... - --provider=rfc2136 - --rfc2136-gss-tsig - --rfc2136-host=dns-host.yourdomain.com - --rfc2136-port=53 - --rfc2136-zone=your-zone.com - --rfc2136-kerberos-username=your-domain-account - --rfc2136-kerberos-password=your-domain-password - --rfc2136-kerberos-realm=your-domain.com - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records. ... As noted above, the --rfc2136-kerberos-realm flag is completely optional and won\u2019t be necessary in many cases. Most likely, you will only need it if you see errors similar to this: KRB Error: (68) KDC_ERR_WRONG_REALM Reserved for future use . The flag --rfc2136-host can be set to the host\u2019s domain name or IP address. However, it also determines the name of the Kerberos principal which is used during authentication. This means that Active Directory might only work if this is set to a specific domain name, possibly leading to errors like this: KDC_ERR_S_PRINCIPAL_UNKNOWN Server not found in Kerberos database . To fix this, try setting --rfc2136-host to the \u201cactual\u201d hostname of your DNS server.","title":"Configuring RFC2136 provider"},{"location":"tutorials/rfc2136/#configuring-rfc2136-provider","text":"This tutorial describes how to use the RFC2136 with either BIND or Windows DNS.","title":"Configuring RFC2136 provider"},{"location":"tutorials/rfc2136/#using-with-bind","text":"To use external-dns with BIND: generate/procure a key, configure DNS and add a deployment of external-dns.","title":"Using with BIND"},{"location":"tutorials/rfc2136/#server-credentials","text":"RFC2136 was developed for and tested with BIND DNS server. This documentation assumes that you already have a configured and working server. If you don\u2019t, please check BIND documents or tutorials. If your DNS is provided for you, ask for a TSIG key authorized to update and transfer the zone you wish to update. The key will look something like below. Skip the next steps wrt BIND setup. key \"externaldns-key\" { algorithm hmac-sha256; secret \"96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8=\"; }; If you are your own DNS administrator create a TSIG key. Use tsig-keygen -a hmac-sha256 externaldns or on older distributions dnssec-keygen -a HMAC-SHA256 -b 256 -n HOST externaldns . You will end up with a key printed to standard out like above (or in the case of dnssec-keygen in a file called Kexternaldns......key ).","title":"Server credentials:"},{"location":"tutorials/rfc2136/#bind-configuration","text":"If you do not administer your own DNS, skip to RFC provider configuration Edit your named.conf file (or appropriate included file) and add/change the following. Make sure You are listening on the right interfaces. At least whatever interface external-dns will be communicating over and the interface that faces the internet. Add the key that you generated/was given to you above. Copy paste the four lines that you got (not the same as the example key) into your file. Create a zone for kubernetes. If you already have a zone, skip to the next step. (I put the zone in it\u2019s own subdirectory because named, which shouldn\u2019t be running as root, needs to create a journal file and the default zone directory isn\u2019t writeable by named). zone \"k8s.example.org\" { type master; file \"/etc/bind/pri/k8s/k8s.zone\"; }; Add your key to both transfer and update. For instance with our previous zone. zone \"k8s.example.org\" { type master; file \"/etc/bind/pri/k8s/k8s.zone\"; allow-transfer { key \"externaldns-key\"; }; update-policy { grant externaldns-key zonesub ANY; }; }; Create a zone file (k8s.zone): $TTL 60 ; 1 minute k8s.example.org IN SOA k8s.example.org. root.k8s.example.org. ( 16 ; serial 60 ; refresh (1 minute) 60 ; retry (1 minute) 60 ; expire (1 minute) 60 ; minimum (1 minute) ) NS ns.k8s.example.org. ns A 123.456.789.012 Reload (or restart) named","title":"BIND Configuration:"},{"location":"tutorials/rfc2136/#using-external-dns","text":"To use external-dns add an ingress or a LoadBalancer service with a host that is part of the domain-filter. For example both of the following would produce A records. apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: svc.example.org spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: ingress.example.org http: paths: - path: / backend: serviceName: my-service servicePort: 8000","title":"Using external-dns"},{"location":"tutorials/rfc2136/#custom-ttl","text":"The default DNS record TTL (Time-To-Live) is 0 seconds. You can customize this value by setting the annotation external-dns.alpha.kubernetes.io/ttl . e.g., modify the service manifest YAML file above: apiVersion: v1 kind: Service metadata: name: nginx annotations: external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com external-dns.alpha.kubernetes.io/ttl: 60 spec: ... This will set the DNS record\u2019s TTL to 60 seconds. A default TTL for all records can be set using the the flag with a time in seconds, minutes or hours, such as --rfc2136-min-ttl=60s There are other annotation that can affect the generation of DNS records, but these are beyond the scope of this tutorial and are covered in the main documentation.","title":"Custom TTL"},{"location":"tutorials/rfc2136/#test-with-external-dns-installed-on-local-machine-optional","text":"You may install external-dns and test on a local machine by running: external-dns --txt-owner-id k8s --provider rfc2136 --rfc2136-host=192.168.0.1 --rfc2136-port=53 --rfc2136-zone=k8s.example.org --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= --rfc2136-tsig-secret-alg=hmac-sha256 --rfc2136-tsig-keyname=externaldns-key --rfc2136-tsig-axfr --source ingress --once --domain-filter=k8s.example.org --dry-run - host should be the IP of your master DNS server. - tsig-secret should be changed to match your secret. - tsig-keyname needs to match the keyname you used (if you changed it). - domain-filter can be used as shown to filter the domains you wish to update.","title":"Test with external-dns installed on local machine (optional)"},{"location":"tutorials/rfc2136/#rfc2136-provider-configuration","text":"In order to use external-dns with your cluster you need to add a deployment with access to your ingress and service resources. The following are two example manifests with and without RBAC respectively. With RBAC: apiVersion: v1 kind: Namespace metadata: name: external-dns labels: name: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns namespace: external-dns rules: - apiGroups: - \"\" resources: - services - endpoints - pods - nodes verbs: - get - watch - list - apiGroups: - extensions - networking.k8s.io resources: - ingresses verbs: - get - list - watch --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer namespace: external-dns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --registry=txt - --txt-prefix=external-dns- - --txt-owner-id=k8s - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= - --rfc2136-tsig-secret-alg=hmac-sha256 - --rfc2136-tsig-keyname=externaldns-key - --rfc2136-tsig-axfr - --source=ingress - --domain-filter=k8s.example.org Without RBAC: apiVersion: v1 kind: Namespace metadata: name: external-dns labels: name: external-dns --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns template: metadata: labels: app: external-dns spec: containers: - name: external-dns image: registry.k8s.io/external-dns/external-dns:v0.13.1 args: - --registry=txt - --txt-prefix=external-dns- - --txt-owner-id=k8s - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= - --rfc2136-tsig-secret-alg=hmac-sha256 - --rfc2136-tsig-keyname=externaldns-key - --rfc2136-tsig-axfr - --source=ingress - --domain-filter=k8s.example.org","title":"RFC2136 provider configuration:"},{"location":"tutorials/rfc2136/#microsoft-dns-insecure-updates","text":"While external-dns was not developed or tested against Microsoft DNS, it can be configured to work against it. YMMV.","title":"Microsoft DNS (Insecure Updates)"},{"location":"tutorials/rfc2136/#insecure-updates","text":"","title":"Insecure Updates"},{"location":"tutorials/rfc2136/#dns-side-configuration","text":"Create a DNS zone Enable insecure dynamic updates for the zone Enable Zone Transfers to all servers","title":"DNS-side configuration"},{"location":"tutorials/rfc2136/#external-dns-configuration","text":"You\u2019ll want to configure external-dns similarly to the following: ... - --provider=rfc2136 - --rfc2136-host=192.168.0.1 - --rfc2136-port=53 - --rfc2136-zone=k8s.example.org - --rfc2136-insecure - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records. ...","title":"external-dns configuration"},{"location":"tutorials/rfc2136/#secure-updates-using-rfc3645-gss-tsig","text":"","title":"Secure Updates Using RFC3645 (GSS-TSIG)"},{"location":"tutorials/rfc2136/#dns-side-configuration_1","text":"Create a DNS zone Enable secure dynamic updates for the zone Enable Zone Transfers to all servers If you see any error messages which indicate that external-dns was somehow not able to fetch existing DNS records from your DNS server, this could mean that you forgot about step 3.","title":"DNS-side configuration"},{"location":"tutorials/rfc2136/#kerberos-configuration","text":"DNS with secure updates relies upon a valid Kerberos configuration running within the external-dns container. At this time, you will need to create a ConfigMap for the external-dns container to use and mount it in your deployment. Below is an example of a working Kerberos configuration inside a ConfigMap definition. This may be different depending on many factors in your environment: apiVersion : v1 kind : ConfigMap metadata : creationTimestamp : null name : krb5.conf data : krb5.conf : | [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt default_ccache_name = KEYRING:persistent:%{uid} default_realm = YOUR-REALM.COM [realms] YOUR-REALM.COM = { kdc = dc1.yourdomain.com admin_server = dc1.yourdomain.com } [domain_realm] yourdomain.com = YOUR-REALM.COM .yourdomain.com = YOUR-REALM.COM In most cases, the realm name will probably be the same as the domain name, so you can simply replace YOUR-REALM.COM with something like YOURDOMAIN.COM . Once the ConfigMap is created, the container external-dns container needs to be told to mount that ConfigMap as a volume at the default Kerberos configuration location. The pod spec should include a similar configuration to the following: ... volumeMounts : - mountPath : /etc/krb5.conf name : kerberos-config-volume subPath : krb5.conf ... volumes : - configMap : defaultMode : 420 name : krb5.conf name : kerberos-config-volume ...","title":"Kerberos Configuration"},{"location":"tutorials/rfc2136/#external-dns-configuration_1","text":"You\u2019ll want to configure external-dns similarly to the following: ... - --provider=rfc2136 - --rfc2136-gss-tsig - --rfc2136-host=dns-host.yourdomain.com - --rfc2136-port=53 - --rfc2136-zone=your-zone.com - --rfc2136-kerberos-username=your-domain-account - --rfc2136-kerberos-password=your-domain-password - --rfc2136-kerberos-realm=your-domain.com - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records. ... As noted above, the --rfc2136-kerberos-realm flag is completely optional and won\u2019t be necessary in many cases. Most likely, you will only need it if you see errors similar to this: KRB Error: (68) KDC_ERR_WRONG_REALM Reserved for future use . The flag --rfc2136-host can be set to the host\u2019s domain name or IP address. However, it also determines the name of the Kerberos principal which is used during authentication. This means that Active Directory might only work if this is set to a specific domain name, possibly leading to errors like this: KDC_ERR_S_PRINCIPAL_UNKNOWN Server not found in Kerberos database . To fix this, try setting --rfc2136-host to the \u201cactual\u201d hostname of your DNS server.","title":"external-dns configuration"},{"location":"tutorials/scaleway/","text":"Setting up ExternalDNS for Services on Scaleway \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Scaleway DNS. Make sure to use >=0.7.4 version of ExternalDNS for this tutorial. Warning : Scaleway DNS is currently in Public Beta and may not be suited for production usage. Importing a Domain into Scaleway DNS \u00b6 In order to use your domain, you need to import it into Scaleway DNS. If it\u2019s not already done, you can follow this documentation Once the domain is imported you can either use the root zone, or create a subzone to use. In this example we will use example.com as an example. Creating Scaleway Credentials \u00b6 To use ExternalDNS with Scaleway DNS, you need to create an API token (composed of the Access Key and the Secret Key). You can either use existing ones or you can create a new token, as explained in How to generate an API token or directly by going to the credentials page . Note that you will also need to the Organization ID, which can be retrieve on the same page. Three environment variables are needed to run ExternalDNS with Scaleway DNS: - SCW_ACCESS_KEY which is the Access Key. - SCW_SECRET_KEY which is the Secret Key. - SCW_DEFAULT_ORGANIZATION_ID which is the Default Organization ID. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. The following example are suited for development. For a production usage, prefer secrets over environment, and use a tagged release . Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=scaleway env : - name : SCW_ACCESS_KEY value : \"<your access key>\" - name : SCW_SECRET_KEY value : \"<your secret key>\" - name : SCW_DEFAULT_ORGANIZATION_ID value : \"<your default organization ID>\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=scaleway env : - name : SCW_ACCESS_KEY value : \"<your access key>\" - name : SCW_SECRET_KEY value : \"<your secret key>\" - name : SCW_DEFAULT_ORGANIZATION_ID value : \"<your default organization ID>\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Scaleway DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Scaleway DNS records. Verifying Scaleway DNS records \u00b6 Check your Scaleway DNS UI to view the records for your Scaleway DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Scaleway DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on Scaleway"},{"location":"tutorials/scaleway/#setting-up-externaldns-for-services-on-scaleway","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Scaleway DNS. Make sure to use >=0.7.4 version of ExternalDNS for this tutorial. Warning : Scaleway DNS is currently in Public Beta and may not be suited for production usage.","title":"Setting up ExternalDNS for Services on Scaleway"},{"location":"tutorials/scaleway/#importing-a-domain-into-scaleway-dns","text":"In order to use your domain, you need to import it into Scaleway DNS. If it\u2019s not already done, you can follow this documentation Once the domain is imported you can either use the root zone, or create a subzone to use. In this example we will use example.com as an example.","title":"Importing a Domain into Scaleway DNS"},{"location":"tutorials/scaleway/#creating-scaleway-credentials","text":"To use ExternalDNS with Scaleway DNS, you need to create an API token (composed of the Access Key and the Secret Key). You can either use existing ones or you can create a new token, as explained in How to generate an API token or directly by going to the credentials page . Note that you will also need to the Organization ID, which can be retrieve on the same page. Three environment variables are needed to run ExternalDNS with Scaleway DNS: - SCW_ACCESS_KEY which is the Access Key. - SCW_SECRET_KEY which is the Secret Key. - SCW_DEFAULT_ORGANIZATION_ID which is the Default Organization ID.","title":"Creating Scaleway Credentials"},{"location":"tutorials/scaleway/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. The following example are suited for development. For a production usage, prefer secrets over environment, and use a tagged release .","title":"Deploy ExternalDNS"},{"location":"tutorials/scaleway/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=scaleway env : - name : SCW_ACCESS_KEY value : \"<your access key>\" - name : SCW_SECRET_KEY value : \"<your secret key>\" - name : SCW_DEFAULT_ORGANIZATION_ID value : \"<your default organization ID>\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/scaleway/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : replicas : 1 selector : matchLabels : app : external-dns strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=scaleway env : - name : SCW_ACCESS_KEY value : \"<your access key>\" - name : SCW_SECRET_KEY value : \"<your secret key>\" - name : SCW_DEFAULT_ORGANIZATION_ID value : \"<your default organization ID>\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/scaleway/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Scaleway DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Scaleway DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/scaleway/#verifying-scaleway-dns-records","text":"Check your Scaleway DNS UI to view the records for your Scaleway DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Scaleway DNS records"},{"location":"tutorials/scaleway/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Scaleway DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"},{"location":"tutorials/security-context/","text":"Running ExternalDNS with limited privileges \u00b6 You can run ExternalDNS with reduced privileges since v0.5.6 using the following SecurityContext . apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - ... # your arguments here securityContext : runAsNonRoot : true runAsUser : 65534 readOnlyRootFilesystem : true capabilities : drop : [ \"ALL\" ]","title":"Running ExternalDNS with limited privileges"},{"location":"tutorials/security-context/#running-externaldns-with-limited-privileges","text":"You can run ExternalDNS with reduced privileges since v0.5.6 using the following SecurityContext . apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - ... # your arguments here securityContext : runAsNonRoot : true runAsUser : 65534 readOnlyRootFilesystem : true capabilities : drop : [ \"ALL\" ]","title":"Running ExternalDNS with limited privileges"},{"location":"tutorials/tencentcloud/","text":"Setting up ExternalDNS for Tencent Cloud \u00b6 External Dns Version \u00b6 Make sure to use >=0.13.1 version of ExternalDNS for this tutorial Set up PrivateDns or DNSPod \u00b6 Tencent Cloud DNSPod Service is the domain name resolution and management service for public access. Tencent Cloud PrivateDNS Service is the domain name resolution and management service for VPC internal access. If you want to use internal dns service in Tencent Cloud. 1. Set up the args --tencent-cloud-zone-type=private 2. Create a DNS domain in PrivateDNS console. DNS domain which will contain the managed DNS records. If you want to use public dns service in Tencent Cloud. 1. Set up the args --tencent-cloud-zone-type=public 2. Create a Domain in DnsPod console. DNS domain which will contain the managed DNS records. Set up CAM for API Key \u00b6 In Tencent CAM Console. you may get the secretId and secretKey pair. make sure the key pair has those Policy. { \"version\" : \"2.0\" , \"statement\" : [ { \"effect\" : \"allow\" , \"action\" : [ \"dnspod:ModifyRecord\" , \"dnspod:DeleteRecord\" , \"dnspod:CreateRecord\" , \"dnspod:DescribeRecordList\" , \"dnspod:DescribeDomainList\" ], \"resource\" : [ \"*\" ] }, { \"effect\" : \"allow\" , \"action\" : [ \"privatedns:DescribePrivateZoneList\" , \"privatedns:DescribePrivateZoneRecordList\" , \"privatedns:CreatePrivateZoneRecord\" , \"privatedns:DeletePrivateZoneRecord\" , \"privatedns:ModifyPrivateZoneRecord\" ], \"resource\" : [ \"*\" ] } ] } Deploy ExternalDNS \u00b6 Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : v1 kind : ConfigMap metadata : name : external-dns data : tencent-cloud.json : | { \"regionId\": \"ap-shanghai\", \"secretId\": \"******\", \"secretKey\": \"******\", \"vpcId\": \"vpc-******\", \"internetEndpoint\": false # Default: false. Access the Tencent API through the intranet. If you need to deploy on the public network, you need to change to true } --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=tencentcloud - --policy=sync # set `upsert-only` would prevent ExternalDNS from deleting any records - --tencent-cloud-zone-type=private # only look at private hosted zones. set `public` to use the public dns service. - --tencent-cloud-config-file=/etc/kubernetes/tencent-cloud.json image : registry.k8s.io/external-dns/external-dns:v0.13.1 imagePullPolicy : Always name : external-dns resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/kubernetes name : config-volume readOnly : true dnsPolicy : ClusterFirst hostAliases : - hostnames : - privatedns.internal.tencentcloudapi.com - dnspod.internal.tencentcloudapi.com ip : 169.254.0.95 restartPolicy : Always schedulerName : default-scheduler securityContext : {} serviceAccount : external-dns serviceAccountName : external-dns terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 items : - key : tencent-cloud.json path : tencent-cloud.json name : external-dns name : config-volume Example \u00b6 Service \u00b6 apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com external-dns.alpha.kubernetes.io/internal-hostname : nginx-internal.external-dns-test.com external-dns.alpha.kubernetes.io/ttl : \"600\" spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http nginx.external-dns-test.com will record to the Loadbalancer VIP. nginx-internal.external-dns-test.com will record to the ClusterIP. all of the DNS Record ttl will be 600. Attention \u00b6 This makes ExternalDNS safe for running in environments where there are other records managed via other means.","title":"Setting up ExternalDNS for Tencent Cloud"},{"location":"tutorials/tencentcloud/#setting-up-externaldns-for-tencent-cloud","text":"","title":"Setting up ExternalDNS for Tencent Cloud"},{"location":"tutorials/tencentcloud/#external-dns-version","text":"Make sure to use >=0.13.1 version of ExternalDNS for this tutorial","title":"External Dns Version"},{"location":"tutorials/tencentcloud/#set-up-privatedns-or-dnspod","text":"Tencent Cloud DNSPod Service is the domain name resolution and management service for public access. Tencent Cloud PrivateDNS Service is the domain name resolution and management service for VPC internal access. If you want to use internal dns service in Tencent Cloud. 1. Set up the args --tencent-cloud-zone-type=private 2. Create a DNS domain in PrivateDNS console. DNS domain which will contain the managed DNS records. If you want to use public dns service in Tencent Cloud. 1. Set up the args --tencent-cloud-zone-type=public 2. Create a Domain in DnsPod console. DNS domain which will contain the managed DNS records.","title":"Set up PrivateDns or DNSPod"},{"location":"tutorials/tencentcloud/#set-up-cam-for-api-key","text":"In Tencent CAM Console. you may get the secretId and secretKey pair. make sure the key pair has those Policy. { \"version\" : \"2.0\" , \"statement\" : [ { \"effect\" : \"allow\" , \"action\" : [ \"dnspod:ModifyRecord\" , \"dnspod:DeleteRecord\" , \"dnspod:CreateRecord\" , \"dnspod:DescribeRecordList\" , \"dnspod:DescribeDomainList\" ], \"resource\" : [ \"*\" ] }, { \"effect\" : \"allow\" , \"action\" : [ \"privatedns:DescribePrivateZoneList\" , \"privatedns:DescribePrivateZoneRecordList\" , \"privatedns:CreatePrivateZoneRecord\" , \"privatedns:DeletePrivateZoneRecord\" , \"privatedns:ModifyPrivateZoneRecord\" ], \"resource\" : [ \"*\" ] } ] }","title":"Set up CAM for API Key"},{"location":"tutorials/tencentcloud/#deploy-externaldns","text":"","title":"Deploy ExternalDNS"},{"location":"tutorials/tencentcloud/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : v1 kind : ConfigMap metadata : name : external-dns data : tencent-cloud.json : | { \"regionId\": \"ap-shanghai\", \"secretId\": \"******\", \"secretKey\": \"******\", \"vpcId\": \"vpc-******\", \"internetEndpoint\": false # Default: false. Access the Tencent API through the intranet. If you need to deploy on the public network, you need to change to true } --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - args : - --source=service - --source=ingress - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=tencentcloud - --policy=sync # set `upsert-only` would prevent ExternalDNS from deleting any records - --tencent-cloud-zone-type=private # only look at private hosted zones. set `public` to use the public dns service. - --tencent-cloud-config-file=/etc/kubernetes/tencent-cloud.json image : registry.k8s.io/external-dns/external-dns:v0.13.1 imagePullPolicy : Always name : external-dns resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/kubernetes name : config-volume readOnly : true dnsPolicy : ClusterFirst hostAliases : - hostnames : - privatedns.internal.tencentcloudapi.com - dnspod.internal.tencentcloudapi.com ip : 169.254.0.95 restartPolicy : Always schedulerName : default-scheduler securityContext : {} serviceAccount : external-dns serviceAccountName : external-dns terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 items : - key : tencent-cloud.json path : tencent-cloud.json name : external-dns name : config-volume","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/tencentcloud/#example","text":"","title":"Example"},{"location":"tutorials/tencentcloud/#service","text":"apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : nginx.external-dns-test.com external-dns.alpha.kubernetes.io/internal-hostname : nginx-internal.external-dns-test.com external-dns.alpha.kubernetes.io/ttl : \"600\" spec : type : LoadBalancer ports : - port : 80 name : http targetPort : 80 selector : app : nginx --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 name : http nginx.external-dns-test.com will record to the Loadbalancer VIP. nginx-internal.external-dns-test.com will record to the ClusterIP. all of the DNS Record ttl will be 600.","title":"Service"},{"location":"tutorials/tencentcloud/#attention","text":"This makes ExternalDNS safe for running in environments where there are other records managed via other means.","title":"Attention"},{"location":"tutorials/transip/","text":"Setting up ExternalDNS for Services on TransIP \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using TransIP. Make sure to use >=0.5.14 version of ExternalDNS for this tutorial, have at least 1 domain registered at TransIP and enabled the API. Enable TransIP API and prepare your API key \u00b6 To use the TransIP API you need an account at TransIP and enable API usage as described in the knowledge base . With the private key generated by the API, we create a kubernetes secret: $ kubectl create secret generic transip-api-key --from-file = transip-api-key = /path/to/private.key Deploy ExternalDNS \u00b6 Below are example manifests, for both cluster without or with RBAC enabled. Don\u2019t forget to replace YOUR_TRANSIP_ACCOUNT_NAME with your TransIP account name. In these examples, an example domain-filter is defined. Such a filter can be used to prevent ExternalDNS from touching any domain not listed in the filter. Refer to the docs for any other command-line parameters you might want to use. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains - --provider=transip - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME - --transip-keyfile=/transip/transip-api-key volumeMounts : - mountPath : /transip name : transip-api-key readOnly : true volumes : - name : transip-api-key secret : secretName : transip-api-key Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains - --provider=transip - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME - --transip-keyfile=/transip/transip-api-key volumeMounts : - mountPath : /transip name : transip-api-key readOnly : true volumes : - name : transip-api-key secret : secretName : transip-api-key Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; this is the name ExternalDNS will create and manage DNS records for. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the TransIP DNS records. Verifying TransIP DNS records \u00b6 Check your TransIP Control Panel to view the records for your TransIP DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Setting up ExternalDNS for Services on TransIP"},{"location":"tutorials/transip/#setting-up-externaldns-for-services-on-transip","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using TransIP. Make sure to use >=0.5.14 version of ExternalDNS for this tutorial, have at least 1 domain registered at TransIP and enabled the API.","title":"Setting up ExternalDNS for Services on TransIP"},{"location":"tutorials/transip/#enable-transip-api-and-prepare-your-api-key","text":"To use the TransIP API you need an account at TransIP and enable API usage as described in the knowledge base . With the private key generated by the API, we create a kubernetes secret: $ kubectl create secret generic transip-api-key --from-file = transip-api-key = /path/to/private.key","title":"Enable TransIP API and prepare your API key"},{"location":"tutorials/transip/#deploy-externaldns","text":"Below are example manifests, for both cluster without or with RBAC enabled. Don\u2019t forget to replace YOUR_TRANSIP_ACCOUNT_NAME with your TransIP account name. In these examples, an example domain-filter is defined. Such a filter can be used to prevent ExternalDNS from touching any domain not listed in the filter. Refer to the docs for any other command-line parameters you might want to use.","title":"Deploy ExternalDNS"},{"location":"tutorials/transip/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains - --provider=transip - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME - --transip-keyfile=/transip/transip-api-key volumeMounts : - mountPath : /transip name : transip-api-key readOnly : true volumes : - name : transip-api-key secret : secretName : transip-api-key","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/transip/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"watch\" , \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains - --provider=transip - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME - --transip-keyfile=/transip/transip-api-key volumeMounts : - mountPath : /transip name : transip-api-key readOnly : true volumes : - name : transip-api-key secret : secretName : transip-api-key","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/transip/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; this is the name ExternalDNS will create and manage DNS records for. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the TransIP DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/transip/#verifying-transip-dns-records","text":"Check your TransIP Control Panel to view the records for your TransIP DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying TransIP DNS records"},{"location":"tutorials/ultradns/","text":"Setting up ExternalDNS for Services on UltraDNS \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using UltraDNS. For this tutorial, please make sure that you are using a version > 0.7.2 of ExternalDNS. Managing DNS with UltraDNS \u00b6 If you would like to read-up on the UltraDNS service, you can find additional details here: Introduction to UltraDNS Before proceeding, please create a new DNS Zone that you will create your records in for this tutorial process. For the examples in this tutorial, we will be using example.com as our Zone. Setting Up UltraDNS Credentials \u00b6 The following environment variables will be needed to run ExternalDNS with UltraDNS. ULTRADNS_USERNAME , ULTRADNS_PASSWORD , & ULTRADNS_BASEURL ULTRADNS_ACCOUNTNAME (optional variable). Deploying ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then, apply one of the following manifests file to deploy ExternalDNS. Note: We are assuming the zone is already present within UltraDNS. Note: While creating CNAMES as target endpoints, the --txt-prefix option is mandatory. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress # ingress is also possible - --domain-filter=example.com # (Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into.. - --provider=ultradns - --txt-prefix=txt- env : - name : ULTRADNS_USERNAME value : \"\" - name : ULTRADNS_PASSWORD # The password is required to be BASE64 encrypted. value : \"\" - name : ULTRADNS_BASEURL value : \"https://api.ultradns.com/\" - name : ULTRADNS_ACCOUNTNAME value : \"\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com #(Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into.. - --provider=ultradns - --txt-prefix=txt- env : - name : ULTRADNS_USERNAME value : \"\" - name : ULTRADNS_PASSWORD # The password is required to be BASE64 encrypted. value : \"\" - name : ULTRADNS_BASEURL value : \"https://api.ultradns.com/\" - name : ULTRADNS_ACCOUNTNAME value : \"\" Deploying an Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Please note the annotation on the service. Use the same hostname as the UltraDNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Creating the Deployment and Service: \u00b6 $ kubectl create -f nginx.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and will synchronize the UltraDNS records. Verifying UltraDNS Records \u00b6 Please verify on the UltraDNS UI that the records are created under the zone \u201cexample.com\u201d. For more information on UltraDNS UI, refer to (https://docs.ultradns.neustar/mspuserguide.html). Select the zone that was created above (or select the appropriate zone if a different zone was used.) The external IP address will be displayed as a CNAME record for your zone. Cleaning Up the Deployment and Service \u00b6 Now that we have verified that ExternalDNS will automatically manage your UltraDNS records, you can delete example zones that you created in this tutorial: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml Examples to Manage your Records \u00b6 Creating Multiple A Records Target \u00b6 First, you want to create a service file called \u2018apple-banana-echo.yaml\u2019 --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image Then, create service file called \u2018expose-apple-banana-app.yaml\u2019 to expose the services. For more information to deploy ingress controller, refer to (https://kubernetes.github.io/ingress-nginx/deploy/) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.1,10.10.10.23 spec : rules : - http : paths : - path : /apple pathType : Prefix backend : service : name : example-service port : number : 5678 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f expose-apple-banana-app.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f expose-apple-banana-app.yaml $ kubectl delete -f external-dns.yaml Creating CNAME Record \u00b6 Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. If this not provided, your records will not be created. First, create a service file called \u2018apple-banana-echo.yaml\u2019 Config File Example \u2013 kubernetes cluster is on-premise not on cloud --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : apple.cname.com. spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 Config File Example \u2013 Kubernetes cluster service from different cloud vendors --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : apple type : LoadBalancer ports : - protocol : TCP port : 5678 targetPort : 5678 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI , that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f external-dns.yaml Creating Multiple Types Of Records \u00b6 Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. Since you will also be created a CNAME record, If this not provided, your records will not be created. First, create a service file called \u2018apple-banana-echo.yaml\u2019 Config File Example \u2013 kubernetes cluster is on-premise not on cloud --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app1 labels : app : apple1 spec : containers : - name : example-app1 image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service1 spec : selector : app : apple1 ports : - port : 5679 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app2 labels : app : apple2 spec : containers : - name : example-app2 image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service2 spec : selector : app : apple2 ports : - port : 5680 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : apple.cname.com. spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress1 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple-banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3 spec : rules : - http : paths : - path : /apple backend : service : name : example-service1 port : number : 5679 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress2 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3,10.10.10.20 spec : rules : - http : paths : - path : /apple backend : service : name : example-service2 port : number : 5680 Config File Example \u2013 Kubernetes cluster service from different cloud vendors --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app1 labels : app : apple1 spec : containers : - name : example-app1 image : hashicorp/http-echo args : - \"-text=apple\" --- apiVersion : extensions/v1beta1 kind : Service apiVersion : v1 metadata : name : example-service1 spec : selector : app : apple1 ports : - port : 5679 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3,10.10.10.25 spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress1 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple-banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3 spec : rules : - http : paths : - path : /apple backend : service : name : example-service1 port : number : 5679 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI , that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: console $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f external-dns.yaml","title":"Setting up ExternalDNS for Services on UltraDNS"},{"location":"tutorials/ultradns/#setting-up-externaldns-for-services-on-ultradns","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using UltraDNS. For this tutorial, please make sure that you are using a version > 0.7.2 of ExternalDNS.","title":"Setting up ExternalDNS for Services on UltraDNS"},{"location":"tutorials/ultradns/#managing-dns-with-ultradns","text":"If you would like to read-up on the UltraDNS service, you can find additional details here: Introduction to UltraDNS Before proceeding, please create a new DNS Zone that you will create your records in for this tutorial process. For the examples in this tutorial, we will be using example.com as our Zone.","title":"Managing DNS with UltraDNS"},{"location":"tutorials/ultradns/#setting-up-ultradns-credentials","text":"The following environment variables will be needed to run ExternalDNS with UltraDNS. ULTRADNS_USERNAME , ULTRADNS_PASSWORD , & ULTRADNS_BASEURL ULTRADNS_ACCOUNTNAME (optional variable).","title":"Setting Up UltraDNS Credentials"},{"location":"tutorials/ultradns/#deploying-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then, apply one of the following manifests file to deploy ExternalDNS. Note: We are assuming the zone is already present within UltraDNS. Note: While creating CNAMES as target endpoints, the --txt-prefix option is mandatory.","title":"Deploying ExternalDNS"},{"location":"tutorials/ultradns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress # ingress is also possible - --domain-filter=example.com # (Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into.. - --provider=ultradns - --txt-prefix=txt- env : - name : ULTRADNS_USERNAME value : \"\" - name : ULTRADNS_PASSWORD # The password is required to be BASE64 encrypted. value : \"\" - name : ULTRADNS_BASEURL value : \"https://api.ultradns.com/\" - name : ULTRADNS_ACCOUNTNAME value : \"\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/ultradns/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" , \"watch\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service - --source=ingress - --domain-filter=example.com #(Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into.. - --provider=ultradns - --txt-prefix=txt- env : - name : ULTRADNS_USERNAME value : \"\" - name : ULTRADNS_PASSWORD # The password is required to be BASE64 encrypted. value : \"\" - name : ULTRADNS_BASEURL value : \"https://api.ultradns.com/\" - name : ULTRADNS_ACCOUNTNAME value : \"\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/ultradns/#deploying-an-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Please note the annotation on the service. Use the same hostname as the UltraDNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.","title":"Deploying an Nginx Service"},{"location":"tutorials/ultradns/#creating-the-deployment-and-service","text":"$ kubectl create -f nginx.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and will synchronize the UltraDNS records.","title":"Creating the Deployment and Service:"},{"location":"tutorials/ultradns/#verifying-ultradns-records","text":"Please verify on the UltraDNS UI that the records are created under the zone \u201cexample.com\u201d. For more information on UltraDNS UI, refer to (https://docs.ultradns.neustar/mspuserguide.html). Select the zone that was created above (or select the appropriate zone if a different zone was used.) The external IP address will be displayed as a CNAME record for your zone.","title":"Verifying UltraDNS Records"},{"location":"tutorials/ultradns/#cleaning-up-the-deployment-and-service","text":"Now that we have verified that ExternalDNS will automatically manage your UltraDNS records, you can delete example zones that you created in this tutorial: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleaning Up the Deployment and Service"},{"location":"tutorials/ultradns/#examples-to-manage-your-records","text":"","title":"Examples to Manage your Records"},{"location":"tutorials/ultradns/#creating-multiple-a-records-target","text":"First, you want to create a service file called \u2018apple-banana-echo.yaml\u2019 --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image Then, create service file called \u2018expose-apple-banana-app.yaml\u2019 to expose the services. For more information to deploy ingress controller, refer to (https://kubernetes.github.io/ingress-nginx/deploy/) apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.1,10.10.10.23 spec : rules : - http : paths : - path : /apple pathType : Prefix backend : service : name : example-service port : number : 5678 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f expose-apple-banana-app.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f expose-apple-banana-app.yaml $ kubectl delete -f external-dns.yaml","title":"Creating Multiple A Records Target"},{"location":"tutorials/ultradns/#creating-cname-record","text":"Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. If this not provided, your records will not be created. First, create a service file called \u2018apple-banana-echo.yaml\u2019 Config File Example \u2013 kubernetes cluster is on-premise not on cloud --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : apple.cname.com. spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 Config File Example \u2013 Kubernetes cluster service from different cloud vendors --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : apple type : LoadBalancer ports : - protocol : TCP port : 5678 targetPort : 5678 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI , that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f external-dns.yaml","title":"Creating CNAME Record"},{"location":"tutorials/ultradns/#creating-multiple-types-of-records","text":"Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. Since you will also be created a CNAME record, If this not provided, your records will not be created. First, create a service file called \u2018apple-banana-echo.yaml\u2019 Config File Example \u2013 kubernetes cluster is on-premise not on cloud --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app1 labels : app : apple1 spec : containers : - name : example-app1 image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service1 spec : selector : app : apple1 ports : - port : 5679 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app2 labels : app : apple2 spec : containers : - name : example-app2 image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service2 spec : selector : app : apple2 ports : - port : 5680 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : apple.cname.com. spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress1 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple-banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3 spec : rules : - http : paths : - path : /apple backend : service : name : example-service1 port : number : 5679 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress2 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3,10.10.10.20 spec : rules : - http : paths : - path : /apple backend : service : name : example-service2 port : number : 5680 Config File Example \u2013 Kubernetes cluster service from different cloud vendors --- apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com. spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 --- kind : Pod apiVersion : v1 metadata : name : example-app labels : app : apple spec : containers : - name : example-app image : hashicorp/http-echo args : - \"-text=apple\" --- kind : Service apiVersion : v1 metadata : name : example-service spec : selector : app : apple ports : - port : 5678 # Default port for image --- kind : Pod apiVersion : v1 metadata : name : example-app1 labels : app : apple1 spec : containers : - name : example-app1 image : hashicorp/http-echo args : - \"-text=apple\" --- apiVersion : extensions/v1beta1 kind : Service apiVersion : v1 metadata : name : example-service1 spec : selector : app : apple1 ports : - port : 5679 # Default port for image --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3,10.10.10.25 spec : rules : - http : paths : - path : /apple backend : service : name : example-service port : number : 5678 --- apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : example-ingress1 annotations : ingress.kubernetes.io/rewrite-target : / ingress.kubernetes.io/scheme : internet-facing external-dns.alpha.kubernetes.io/hostname : apple-banana.example.com. external-dns.alpha.kubernetes.io/target : 10.10.10.3 spec : rules : - http : paths : - path : /apple backend : service : name : example-service1 port : number : 5679 Then, create the deployment and service: $ kubectl create -f apple-banana-echo.yaml $ kubectl create -f external-dns.yaml Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service. Please verify on the UltraDNS UI , that the records have been created under the zone \u201cexample.com\u201d. Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d: console $ kubectl delete -f apple-banana-echo.yaml $ kubectl delete -f external-dns.yaml","title":"Creating Multiple Types Of Records"},{"location":"tutorials/vinyldns/","text":"Setting up ExternalDNS for VinylDNS \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using VinylDNS. The environment vars VINYLDNS_ACCESS_KEY , VINYLDNS_SECRET_KEY , and VINYLDNS_HOST will be needed to run ExternalDNS with VinylDNS. Create a sample deployment and service for external-dns to use \u00b6 Run an application and expose it via a Kubernetes Service: $ kubectl run nginx --image = nginx --replicas = 1 --port = 80 $ kubectl expose deployment nginx --port = 80 --target-port = 80 --type = LoadBalancer Annotate the Service with your desired external DNS name. Make sure to change example.org to your domain. $ kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.example.org.\" After the service is up and running, it should get an EXTERNAL-IP. At first this may showing as <pending> $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 <none> 443/TCP 1h nginx 10.0.0.115 <pending> 80:30543/TCP 10s Once it\u2019s available % kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 <none> 443/TCP 1h nginx 10.0.0.115 34.x.x.x 80:30543/TCP 2m Deploy ExternalDNS to Kubernetes \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Note for examples below When using registry=txt option, make sure to also use the txt-prefix and txt-owner-id options as well. If you try to create a TXT record in VinylDNS without a prefix, it will try to create a TXT record with the same name as your actual DNS record and fail (creating a stranded record external-dns cannot manage). Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --provider=vinyldns - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --registry=txt - --txt-owner-id=grizz - --txt-prefix=txt- env : - name : VINYLDNS_HOST value : \"YOUR_VINYLDNS_HOST\" - name : VINYLDNS_ACCESS_KEY value : \"YOUR_VINYLDNS_ACCESS_KEY\" - name : VINYLDNS_SECRET_KEY value : \"YOUR_VINYLDNS_SECRET_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --provider=vinyldns - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --registry=txt - --txt-owner-id=grizz - --txt-prefix=txt- env : env : - name : VINYLDNS_HOST value : \"YOUR_VINYLDNS_HOST\" - name : VINYLDNS_ACCESS_KEY value : \"YOUR_VINYLDNS_ACCESS_KEY\" - name : VINYLDNS_SECRET_KEY value : \"YOUR_VINYLDNS_SECRET_KEYY Running a locally built version pointed to the above nginx service \u00b6 Make sure your kubectl is configured correctly. Assuming you have the sources, build and run it like below. The vinyl access details needs to exported to the environment before running. make # output skipped export VINYLDNS_HOST = <fqdn of vinyl dns api> export VINYLDNS_ACCESS_KEY = <access key> export VINYLDNS_SECRET_KEY = <secret key> ./build/external-dns \\ --provider = vinyldns \\ --source = service \\ --domain-filter = elements.capsps.comcast.net. \\ --zone-id-filter = 20e8bfd2-3a70-4e1b-8e11-c9c1948528d3 \\ --registry = txt \\ --txt-owner-id = grizz \\ --txt-prefix = txt- \\ --namespace = default \\ --once \\ --dry-run \\ --log-level debug INFO [ 0000 ] running in dry-run mode. No changes to DNS records will be made. INFO [ 0000 ] Created Kubernetes client https://some-k8s-cluster.example.com INFO [ 0001 ] Zone: [ nginx.example.org. ] # output skipped Having --dry-run=true and --log-level=debug is a great way to see exactly what VinylDNS is doing or is about to do.","title":"Setting up ExternalDNS for VinylDNS"},{"location":"tutorials/vinyldns/#setting-up-externaldns-for-vinyldns","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using VinylDNS. The environment vars VINYLDNS_ACCESS_KEY , VINYLDNS_SECRET_KEY , and VINYLDNS_HOST will be needed to run ExternalDNS with VinylDNS.","title":"Setting up ExternalDNS for VinylDNS"},{"location":"tutorials/vinyldns/#create-a-sample-deployment-and-service-for-external-dns-to-use","text":"Run an application and expose it via a Kubernetes Service: $ kubectl run nginx --image = nginx --replicas = 1 --port = 80 $ kubectl expose deployment nginx --port = 80 --target-port = 80 --type = LoadBalancer Annotate the Service with your desired external DNS name. Make sure to change example.org to your domain. $ kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.example.org.\" After the service is up and running, it should get an EXTERNAL-IP. At first this may showing as <pending> $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 <none> 443/TCP 1h nginx 10.0.0.115 <pending> 80:30543/TCP 10s Once it\u2019s available % kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.0.0.1 <none> 443/TCP 1h nginx 10.0.0.115 34.x.x.x 80:30543/TCP 2m","title":"Create a sample deployment and service for external-dns to use"},{"location":"tutorials/vinyldns/#deploy-externaldns-to-kubernetes","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Note for examples below When using registry=txt option, make sure to also use the txt-prefix and txt-owner-id options as well. If you try to create a TXT record in VinylDNS without a prefix, it will try to create a TXT record with the same name as your actual DNS record and fail (creating a stranded record external-dns cannot manage).","title":"Deploy ExternalDNS to Kubernetes"},{"location":"tutorials/vinyldns/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --provider=vinyldns - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --registry=txt - --txt-owner-id=grizz - --txt-prefix=txt- env : - name : VINYLDNS_HOST value : \"YOUR_VINYLDNS_HOST\" - name : VINYLDNS_ACCESS_KEY value : \"YOUR_VINYLDNS_ACCESS_KEY\" - name : VINYLDNS_SECRET_KEY value : \"YOUR_VINYLDNS_SECRET_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/vinyldns/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --provider=vinyldns - --source=service - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --registry=txt - --txt-owner-id=grizz - --txt-prefix=txt- env : env : - name : VINYLDNS_HOST value : \"YOUR_VINYLDNS_HOST\" - name : VINYLDNS_ACCESS_KEY value : \"YOUR_VINYLDNS_ACCESS_KEY\" - name : VINYLDNS_SECRET_KEY value : \"YOUR_VINYLDNS_SECRET_KEYY","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/vinyldns/#running-a-locally-built-version-pointed-to-the-above-nginx-service","text":"Make sure your kubectl is configured correctly. Assuming you have the sources, build and run it like below. The vinyl access details needs to exported to the environment before running. make # output skipped export VINYLDNS_HOST = <fqdn of vinyl dns api> export VINYLDNS_ACCESS_KEY = <access key> export VINYLDNS_SECRET_KEY = <secret key> ./build/external-dns \\ --provider = vinyldns \\ --source = service \\ --domain-filter = elements.capsps.comcast.net. \\ --zone-id-filter = 20e8bfd2-3a70-4e1b-8e11-c9c1948528d3 \\ --registry = txt \\ --txt-owner-id = grizz \\ --txt-prefix = txt- \\ --namespace = default \\ --once \\ --dry-run \\ --log-level debug INFO [ 0000 ] running in dry-run mode. No changes to DNS records will be made. INFO [ 0000 ] Created Kubernetes client https://some-k8s-cluster.example.com INFO [ 0001 ] Zone: [ nginx.example.org. ] # output skipped Having --dry-run=true and --log-level=debug is a great way to see exactly what VinylDNS is doing or is about to do.","title":"Running a locally built version pointed to the above nginx service"},{"location":"tutorials/vultr/","text":"Setting up ExternalDNS for Services on Vultr \u00b6 This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Vultr DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial. Managing DNS with Vultr \u00b6 If you want to read up on vultr DNS service you can read the following tutorial: Introduction to Vultr DNS Create a new DNS Zone where you want to create your records in. For the examples we will be using example.com Creating Vultr Credentials \u00b6 You will need to create a new API Key which can be found on the Vultr Dashboard . The environment variable VULTR_API_KEY will be needed to run ExternalDNS with Vultr. Deploy ExternalDNS \u00b6 Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. Manifest (for clusters without RBAC enabled) \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=vultr env : - name : VULTR_API_KEY value : \"YOU_VULTR_API_KEY\" Manifest (for clusters with RBAC enabled) \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=vultr env : - name : VULTR_API_KEY value : \"YOU_VULTR_API_KEY\" Deploying a Nginx Service \u00b6 Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Vultr DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Vultr DNS records. Verifying Vultr DNS records \u00b6 Check your Vultr UI to view the records for your Vultr DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain. Cleanup \u00b6 Now that we have verified that ExternalDNS will automatically manage Vultr DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Setting up ExternalDNS for Services on Vultr"},{"location":"tutorials/vultr/#setting-up-externaldns-for-services-on-vultr","text":"This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Vultr DNS. Make sure to use >=0.6 version of ExternalDNS for this tutorial.","title":"Setting up ExternalDNS for Services on Vultr"},{"location":"tutorials/vultr/#managing-dns-with-vultr","text":"If you want to read up on vultr DNS service you can read the following tutorial: Introduction to Vultr DNS Create a new DNS Zone where you want to create your records in. For the examples we will be using example.com","title":"Managing DNS with Vultr"},{"location":"tutorials/vultr/#creating-vultr-credentials","text":"You will need to create a new API Key which can be found on the Vultr Dashboard . The environment variable VULTR_API_KEY will be needed to run ExternalDNS with Vultr.","title":"Creating Vultr Credentials"},{"location":"tutorials/vultr/#deploy-externaldns","text":"Connect your kubectl client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.","title":"Deploy ExternalDNS"},{"location":"tutorials/vultr/#manifest-for-clusters-without-rbac-enabled","text":"apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=vultr env : - name : VULTR_API_KEY value : \"YOU_VULTR_API_KEY\"","title":"Manifest (for clusters without RBAC enabled)"},{"location":"tutorials/vultr/#manifest-for-clusters-with-rbac-enabled","text":"apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" , \"networking.k8s.io\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : apps/v1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate selector : matchLabels : app : external-dns template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.k8s.io/external-dns/external-dns:v0.13.1 args : - --source=service # ingress is also possible - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above. - --provider=vultr env : - name : VULTR_API_KEY value : \"YOU_VULTR_API_KEY\"","title":"Manifest (for clusters with RBAC enabled)"},{"location":"tutorials/vultr/#deploying-a-nginx-service","text":"Create a service file called \u2018nginx.yaml\u2019 with the following contents: apiVersion : apps/v1 kind : Deployment metadata : name : nginx spec : selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - image : nginx name : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx annotations : external-dns.alpha.kubernetes.io/hostname : my-app.example.com spec : selector : app : nginx type : LoadBalancer ports : - protocol : TCP port : 80 targetPort : 80 Note the annotation on the service; use the same hostname as the Vultr DNS zone created above. ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records. Create the deployment and service: $ kubectl create -f nginx.yaml Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Vultr DNS records.","title":"Deploying a Nginx Service"},{"location":"tutorials/vultr/#verifying-vultr-dns-records","text":"Check your Vultr UI to view the records for your Vultr DNS zone. Click on the zone for the one created above if a different domain was used. This should show the external IP address of the service as the A record for your domain.","title":"Verifying Vultr DNS records"},{"location":"tutorials/vultr/#cleanup","text":"Now that we have verified that ExternalDNS will automatically manage Vultr DNS records, we can delete the tutorial\u2019s example: $ kubectl delete service -f nginx.yaml $ kubectl delete service -f externaldns.yaml","title":"Cleanup"}]}